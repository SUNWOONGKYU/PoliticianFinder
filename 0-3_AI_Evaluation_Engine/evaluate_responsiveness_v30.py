#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Responsiveness Category Evaluation for V30
ë¯¼ì›ê³¼ ì‚¬íšŒ ë¬¸ì œì— ì‹ ì†í•˜ê²Œ ëŒ€ì‘í•˜ëŠ” ëŠ¥ë ¥ í‰ê°€
"""

import os
import sys
import io
from datetime import datetime
import json

# Fix encoding on Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Load environment variables manually
env_path = ".env"
with open(env_path, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if line and not line.startswith('#'):
            if '=' in line:
                key, value = line.split('=', 1)
                os.environ[key.strip()] = value.strip()

from supabase import create_client, Client
import anthropic

# Initialize clients
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
supabase: Client = create_client(supabase_url, supabase_key)

client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Constants
POLITICIAN_ID = "f9e00370"
CATEGORY = "responsiveness"
EVALUATOR_AI = "Claude"

# Rating scale definition
RATING_SCALE = {
    4: "íƒì›”",  # ë²•ë¥  ì œì •, êµ­ê°€ ì¸ì •
    3: "ìš°ìˆ˜",  # ì •ì±… ì„±ê³¼ ìˆ˜ì¹˜í™”
    2: "ì–‘í˜¸",  # ì¼ë°˜ ê¸ì • í™œë™
    1: "ê²½ë¯¸í•œ ê¸ì •",
    0: "ì¤‘ë¦½",
    -1: "ê²½ë¯¸í•œ ë¶€ì •",
    -2: "ë¯¸í¡",  # ì‹¤ì±…, ì •ì±… ì‹¤íŒ¨
    -3: "ë¶ˆëŸ‰",  # ì¤‘ëŒ€ ì‹¤ì±…, ê°•í•œ ë¹„íŒ
    -4: "ë§¤ìš° ë¶ˆëŸ‰",  # ë²”ì£„, ê¸°ì†Œ, ì‚¬í‡´
}

RESPONSIVENESS_GUIDANCE = """
ë¯¼ì›ê³¼ ì‚¬íšŒ ë¬¸ì œì— ì‹ ì†í•˜ê²Œ ëŒ€ì‘í•˜ëŠ” ëŠ¥ë ¥ í‰ê°€:
- ë¯¼ì› ì²˜ë¦¬: êµ­ë¯¼ ë¯¼ì›ì— ì–¼ë§ˆë‚˜ ì‹ ì†í•˜ê³  ì ì ˆí•˜ê²Œ ëŒ€ì‘í–ˆëŠ”ê°€
- í˜„ìž¥ ë°©ë¬¸: ë¬¸ì œê°€ ë°œìƒí•œ í˜„ìž¥ì„ ì§ì ‘ ë°©ë¬¸í•˜ì—¬ íŒŒì•…í–ˆëŠ”ê°€
- ì‹ ì† ëŒ€ì‘: ê¸´ê¸‰í•œ ìƒí™©ì— ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ëŒ€ì‘í–ˆëŠ”ê°€
- ì‚¬í›„ ì¡°ì¹˜: ë¬¸ì œ í•´ê²° í›„ ìž¬ë°œ ë°©ì§€ë¥¼ ìœ„í•œ ì¡°ì¹˜ë¥¼ ì·¨í–ˆëŠ”ê°€

í‰ê°€ ê¸°ì¤€:
+4 íƒì›”: ë²•ë¥  ì œì •, êµ­ê°€/ì–¸ë¡  ì¸ì •ì„ ë°›ì€ ì‹ ì† ëŒ€ì‘ ë° ë¬¸ì œ í•´ê²°
+3 ìš°ìˆ˜: ì •ëŸ‰ì  ì„±ê³¼ ìˆ˜ì¹˜í™” (ë¯¼ì› ì²˜ë¦¬ ìˆ˜ ì¦ëŒ€, ì‘ë‹µ ì‹œê°„ ë‹¨ì¶• ë“±)
+2 ì–‘í˜¸: ì¼ë°˜ì ì¸ ê¸ì •ì  ë¯¼ì› ì²˜ë¦¬, í˜„ìž¥ ë°©ë¬¸, ì‹ ì† ëŒ€ì‘ ì‚¬ë¡€
+1 ê²½ë¯¸í•œ ê¸ì •: ìž‘ì€ ê·œëª¨ì˜ ë¯¼ì› ëŒ€ì‘
0 ì¤‘ë¦½: ì±…ìž„ ë¶ˆë¶„ëª…, íŒë‹¨ ì–´ë ¤ì›€
-1 ê²½ë¯¸í•œ ë¶€ì •: ë¯¸í¡í•œ ì‘ë‹µ, ëŠ¦ì€ ëŒ€ì‘
-2 ë¯¸í¡: ì¤‘ìš”í•œ ë¯¼ì› ë¯¸ì²˜ë¦¬, ì‹¤ì±…, ì •ì±… ì‹¤íŒ¨
-3 ë¶ˆëŸ‰: ì¤‘ëŒ€í•œ ë¯¸í¡, ê°•í•œ ë¹„íŒ, ìˆ˜ì‚¬ ë“±
-4 ë§¤ìš° ë¶ˆëŸ‰: ë²”ì£„, ê¸°ì†Œ, ì‚¬í‡´ ë“±
"""


def get_collected_items():
    """Fetch all responsiveness category items for the politician"""
    print(f"\nðŸ“¥ Fetching collected data for {POLITICIAN_ID} in {CATEGORY} category...")

    try:
        response = supabase.table("collected_data_v30").select(
            "id, politician_id, category, title, content"
        ).eq("politician_id", POLITICIAN_ID).eq("category", CATEGORY).execute()

        items = response.data
        print(f"âœ“ Found {len(items)} items")
        return items
    except Exception as e:
        print(f"âœ— Error fetching collected data: {e}")
        return []


def get_evaluated_items():
    """Get items already evaluated by Claude"""
    print(f"ðŸ“Š Checking evaluated items for {POLITICIAN_ID} in {CATEGORY}...")

    try:
        response = supabase.table("evaluations_v30").select(
            "collected_data_id"
        ).eq("politician_id", POLITICIAN_ID).eq("category", CATEGORY).eq("evaluator_ai", EVALUATOR_AI).execute()

        evaluated_ids = set(row["collected_data_id"] for row in response.data)
        print(f"âœ“ Found {len(evaluated_ids)} already evaluated items")
        return evaluated_ids
    except Exception as e:
        print(f"âœ— Error fetching evaluations: {e}")
        return set()


def evaluate_item_with_claude(item):
    """Use Claude to evaluate a single item"""
    title = item.get("title", "")
    content = item.get("content", "")

    prompt = f"""ë‹¹ì‹ ì€ ì •ì¹˜ì¸ ì •ì±… í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

ë‹¤ìŒ ë‚´ìš©ì„ \"ë¯¼ì›ê³¼ ì‚¬íšŒ ë¬¸ì œì— ì‹ ì†í•˜ê²Œ ëŒ€ì‘í•˜ëŠ” ëŠ¥ë ¥(responsiveness)\" ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

[í‰ê°€ ê¸°ì¤€]
{RESPONSIVENESS_GUIDANCE}

[í‰ê°€ ëŒ€ìƒ í•­ëª©]
ì œëª©: {title}
ë‚´ìš©: {content}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
{{
  "rating": -4ë¶€í„° 4ê¹Œì§€ì˜ ì •ìˆ˜ (ì˜ˆ: +2, 0, -1),
  "score": -4ë¶€í„° 4ê¹Œì§€ì˜ ì •ìˆ˜,
  "rating_text": "íƒì›”/ìš°ìˆ˜/ì–‘í˜¸/ê²½ë¯¸í•œ ê¸ì •/ì¤‘ë¦½/ê²½ë¯¸í•œ ë¶€ì •/ë¯¸í¡/ë¶ˆëŸ‰/ë§¤ìš° ë¶ˆëŸ‰ ì¤‘ í•˜ë‚˜",
  "reasoning": "í‰ê°€ ì´ìœ  (2-3ë¬¸ìž¥, í•œê¸€)"
}}

JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

    try:
        message = client.messages.create(
            model="claude-opus-4-5-20251101",
            max_tokens=500,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        response_text = message.content[0].text

        # Parse JSON response
        import json
        result = json.loads(response_text)
        return result
    except Exception as e:
        print(f"  âœ— Claude evaluation error: {e}")
        return None


def save_evaluation(item_id, rating, score, reasoning):
    """Save evaluation to evaluations_v30 table"""
    try:
        evaluation_data = {
            "politician_id": POLITICIAN_ID,
            "category": CATEGORY,
            "collected_data_id": item_id,
            "evaluator_ai": EVALUATOR_AI,
            "rating": f"{rating:+d}",  # Format: "+2" or "-1"
            "score": score,
            "reasoning": reasoning,
            "evaluated_at": datetime.now().isoformat()
        }

        supabase.table("evaluations_v30").insert(evaluation_data).execute()
        return True
    except Exception as e:
        print(f"  âœ— Database save error: {e}")
        return False


def main():
    """Main evaluation workflow"""
    print("=" * 70)
    print("ðŸš€ Responsiveness Category Evaluation (V30)")
    print(f"   Politician ID: {POLITICIAN_ID}")
    print(f"   Category: {CATEGORY}")
    print(f"   Evaluator AI: {EVALUATOR_AI}")
    print("=" * 70)

    # Step 1: Get all collected items
    collected_items = get_collected_items()
    if not collected_items:
        print("âœ— No collected items found")
        return 0

    # Step 2: Get already evaluated items
    evaluated_ids = get_evaluated_items()

    # Step 3: Filter items to evaluate
    items_to_evaluate = [
        item for item in collected_items
        if item["id"] not in evaluated_ids
    ]

    print(f"\nðŸ“‹ Items to evaluate: {len(items_to_evaluate)} / {len(collected_items)}")

    if not items_to_evaluate:
        print("âœ“ All items already evaluated!")
        return 0

    # Step 4: Evaluate each item
    successful_evaluations = 0
    failed_evaluations = 0

    for idx, item in enumerate(items_to_evaluate, 1):
        print(f"\n[{idx}/{len(items_to_evaluate)}] Evaluating: {item['title'][:60]}...")

        # Get Claude evaluation
        evaluation = evaluate_item_with_claude(item)

        if not evaluation:
            print(f"  âœ— Evaluation failed")
            failed_evaluations += 1
            continue

        rating = evaluation.get("rating")
        score = evaluation.get("score")
        reasoning = evaluation.get("reasoning", "")

        print(f"  Rating: {rating:+d} ({evaluation.get('rating_text', '')})")
        print(f"  Reasoning: {reasoning[:80]}...")

        # Save to database
        if save_evaluation(item["id"], rating, score, reasoning):
            print(f"  âœ“ Saved successfully")
            successful_evaluations += 1
        else:
            print(f"  âœ— Failed to save")
            failed_evaluations += 1

    # Summary
    print("\n" + "=" * 70)
    print("ðŸ“Š EVALUATION SUMMARY")
    print(f"   Total items: {len(collected_items)}")
    print(f"   Already evaluated: {len(evaluated_ids)}")
    print(f"   Successfully evaluated: {successful_evaluations}")
    print(f"   Failed: {failed_evaluations}")
    print(f"   Total evaluated now: {successful_evaluations + len(evaluated_ids)}")
    print("=" * 70)

    return successful_evaluations


if __name__ == "__main__":
    completed = main()
    sys.exit(0 if completed > 0 else 1)

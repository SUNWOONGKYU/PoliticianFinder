#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ì¡°ì€í¬ V30 ë°ì´í„° í’ˆì§ˆ ìƒì„¸ ê²€ì¦"""

import sys
import os
from supabase import create_client
from collections import Counter
from datetime import datetime, timedelta
from urllib.parse import urlparse
from dotenv import load_dotenv

# UTF-8 ì¶œë ¥
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    except AttributeError:
        pass

load_dotenv(override=True)

supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

politician_id = 'd0a5d6e1'
politician_name = 'ì¡°ì€í¬'

# OFFICIAL ë„ë©”ì¸
OFFICIAL_DOMAINS = [
    "assembly.go.kr", "likms.assembly.go.kr", "mois.go.kr", "korea.kr",
    "nec.go.kr", "bai.go.kr", "pec.go.kr", "scourt.go.kr", "nesdc.go.kr",
    "manifesto.or.kr", "peoplepower21.org", "theminjoo.kr",
    "seoul.go.kr", "gg.go.kr", "busan.go.kr", "open.go.kr"
]

print("="*80)
print(f"ì¡°ì€í¬ ({politician_id}) ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
print("="*80)
print()

# ì „ì²´ ë°ì´í„° ì¡°íšŒ
result = supabase.table('collected_data_v30')\
    .select('*')\
    .eq('politician_id', politician_id)\
    .execute()

data = result.data
total = len(data)

print(f"ğŸ“Š ì´ ë°ì´í„°: {total}ê°œ")
print()

if total == 0:
    print("âŒ ë°ì´í„° ì—†ìŒ!")
    sys.exit(1)

# 1. AI ë¶„í¬ ë¶„ì„
print("="*80)
print("1ï¸âƒ£ AI ë¶„í¬ (V30 ëª©í‘œ: Gemini 90% + Grok 10%)")
print("="*80)
ai_counts = Counter([d['collector_ai'] for d in data])
for ai in sorted(ai_counts.keys()):
    count = ai_counts[ai]
    pct = (count / total) * 100
    target_pct = 90 if ai == 'Gemini' else 10 if ai == 'Grok' else 0
    diff = pct - target_pct
    status = "âœ…" if abs(diff) < 5 else "âš ï¸" if abs(diff) < 15 else "âŒ"
    marker = f"(ëª©í‘œ {target_pct}%, ì°¨ì´ {diff:+.1f}%)" if target_pct > 0 else "ğŸš¨ V30ì—ì„œ ì œê±°ë¨!"
    print(f"  {status} {ai:12s}: {count:3d}ê°œ ({pct:5.1f}%) {marker}")
print()

# 2. í•„ìˆ˜ í•„ë“œ í’ˆì§ˆ
print("="*80)
print("2ï¸âƒ£ í•„ìˆ˜ í•„ë“œ í’ˆì§ˆ")
print("="*80)

missing_title = [d for d in data if not d.get('title') or len(d.get('title', '').strip()) == 0]
missing_content = [d for d in data if not d.get('content') or len(d.get('content', '').strip()) == 0]
missing_url = [d for d in data if not d.get('source_url') or len(d.get('source_url', '').strip()) == 0]
missing_source_name = [d for d in data if not d.get('source_name')]
missing_date = [d for d in data if not d.get('published_date')]
missing_category = [d for d in data if not d.get('category')]

print(f"  title ëˆ„ë½: {len(missing_title)}ê°œ {'âœ…' if len(missing_title) == 0 else 'âŒ'}")
if len(missing_title) > 0:
    print(f"    ìƒ˜í”Œ: {missing_title[0].get('id')}")

print(f"  content ëˆ„ë½: {len(missing_content)}ê°œ {'âœ…' if len(missing_content) == 0 else 'âŒ'}")
if len(missing_content) > 0:
    print(f"    ìƒ˜í”Œ: {missing_content[0].get('id')}")

print(f"  source_url ëˆ„ë½: {len(missing_url)}ê°œ {'âœ…' if len(missing_url) == 0 else 'âŒ'}")
if len(missing_url) > 0:
    print(f"    ìƒ˜í”Œ: {missing_url[0].get('id')}")

print(f"  source_name ëˆ„ë½: {len(missing_source_name)}ê°œ {'âœ…' if len(missing_source_name) == 0 else 'âŒ'}")
print(f"  published_date ëˆ„ë½: {len(missing_date)}ê°œ {'âœ…' if len(missing_date) == 0 else 'âŒ'}")
print(f"  category ëˆ„ë½: {len(missing_category)}ê°œ {'âœ…' if len(missing_category) == 0 else 'ğŸš¨ V30 í•„ìˆ˜!'}")
print()

# 3. URL í’ˆì§ˆ ë¶„ì„
print("="*80)
print("3ï¸âƒ£ URL í’ˆì§ˆ")
print("="*80)

fake_patterns = ['example.com', 'placeholder', 'sample', 'test.com', 'dummy']
fake_urls = [d for d in data if any(p in d.get('source_url', '').lower() for p in fake_patterns)]
invalid_urls = [d for d in data if d.get('source_url') and not d['source_url'].startswith('http')]

print(f"  ê°€ì§œ URL íŒ¨í„´: {len(fake_urls)}ê°œ {'âœ…' if len(fake_urls) == 0 else 'âŒ'}")
if len(fake_urls) > 0:
    for i, d in enumerate(fake_urls[:3], 1):
        print(f"    {i}. {d['source_url'][:60]}")

print(f"  ìœ íš¨í•˜ì§€ ì•Šì€ URL: {len(invalid_urls)}ê°œ {'âœ…' if len(invalid_urls) == 0 else 'âŒ'}")
if len(invalid_urls) > 0:
    for i, d in enumerate(invalid_urls[:3], 1):
        print(f"    {i}. {d['source_url'][:60]}")
print()

# 4. source_type ê²€ì¦
print("="*80)
print("4ï¸âƒ£ source_type ê²€ì¦ (OFFICIAL vs PUBLIC)")
print("="*80)

# source_type í•„ë“œ ì¡´ì¬ ì—¬ë¶€
has_source_type = [d for d in data if 'source_type' in d and d['source_type']]
print(f"  source_type í•„ë“œ ì¡´ì¬: {len(has_source_type)}ê°œ ({len(has_source_type)/total*100:.1f}%)")

if len(has_source_type) > 0:
    type_counts = Counter([d['source_type'] for d in has_source_type])
    for st in sorted(type_counts.keys()):
        count = type_counts[st]
        pct = (count / len(has_source_type)) * 100
        target = "50%"
        print(f"    {st}: {count}ê°œ ({pct:.1f}%) - ëª©í‘œ: {target}")

    # source_typeê³¼ URL ë„ë©”ì¸ ì¼ì¹˜ ê²€ì¦
    mismatches = []
    for d in has_source_type:
        url = d.get('source_url', '')
        source_type = d.get('source_type')
        is_official_domain = any(domain in url for domain in OFFICIAL_DOMAINS)

        if source_type == 'OFFICIAL' and not is_official_domain:
            mismatches.append((d['id'], url, 'OFFICIALë¡œ í‘œê¸°ë˜ì—ˆìœ¼ë‚˜ PUBLIC ë„ë©”ì¸'))
        elif source_type == 'PUBLIC' and is_official_domain:
            mismatches.append((d['id'], url, 'PUBLICë¡œ í‘œê¸°ë˜ì—ˆìœ¼ë‚˜ OFFICIAL ë„ë©”ì¸'))

    print(f"\n  source_type ë¶ˆì¼ì¹˜: {len(mismatches)}ê°œ {'âœ…' if len(mismatches) == 0 else 'âŒ'}")
    if len(mismatches) > 0:
        for i, (id, url, reason) in enumerate(mismatches[:3], 1):
            print(f"    {i}. {reason}")
            print(f"       URL: {url[:70]}")
else:
    print(f"  ğŸš¨ source_type í•„ë“œ ì—†ìŒ - V30 ìŠ¤í‚¤ë§ˆ ì•„ë‹˜!")
print()

# 5. ê¸°ê°„ ì œí•œ ê²€ì¦
print("="*80)
print("5ï¸âƒ£ ê¸°ê°„ ì œí•œ (OFFICIAL 4ë…„, PUBLIC 2ë…„)")
print("="*80)

now = datetime.now()
official_limit = now - timedelta(days=365*4)
public_limit = now - timedelta(days=365*2)

out_of_range = []
for d in data:
    if not d.get('published_date'):
        continue
    try:
        pub_date = datetime.fromisoformat(d['published_date'].replace('Z', '+00:00'))
        source_type = d.get('source_type', 'UNKNOWN')

        if source_type == 'OFFICIAL' and pub_date < official_limit:
            out_of_range.append((d['id'], pub_date, source_type, '4ë…„ ì´ˆê³¼'))
        elif source_type == 'PUBLIC' and pub_date < public_limit:
            out_of_range.append((d['id'], pub_date, source_type, '2ë…„ ì´ˆê³¼'))
    except:
        pass

print(f"  ê¸°ê°„ ì´ˆê³¼: {len(out_of_range)}ê°œ {'âœ…' if len(out_of_range) == 0 else 'âŒ'}")
if len(out_of_range) > 0:
    for i, (id, date, stype, reason) in enumerate(out_of_range[:3], 1):
        print(f"    {i}. [{stype}] {date.strftime('%Y-%m-%d')} - {reason}")
print()

# 6. ì¤‘ë³µ ê²€ì‚¬
print("="*80)
print("6ï¸âƒ£ ì¤‘ë³µ ê²€ì‚¬ (ê°™ì€ AI + ê°™ì€ URL)")
print("="*80)

seen = {}
duplicates = []
for d in data:
    key = (d['collector_ai'], d.get('source_url', ''))
    if key in seen:
        duplicates.append((seen[key], d['id'], d['source_url']))
    else:
        seen[key] = d['id']

print(f"  ì¤‘ë³µ: {len(duplicates)}ê°œ {'âœ…' if len(duplicates) == 0 else 'âŒ'}")
if len(duplicates) > 0:
    print(f"  ì²« 3ê°œ ìƒ˜í”Œ:")
    for i, (id1, id2, url) in enumerate(duplicates[:3], 1):
        print(f"    {i}. ID {id1} = ID {id2}")
        print(f"       URL: {url[:70]}")
print()

# 7. Grok X ì „ë‹´ ê²€ì¦
print("="*80)
print("7ï¸âƒ£ Grok X/íŠ¸ìœ„í„° ì „ë‹´ ê²€ì¦")
print("="*80)

grok_data = [d for d in data if d['collector_ai'] == 'Grok']
grok_x = [d for d in grok_data if 'twitter.com' in d.get('source_url', '') or 'x.com' in d.get('source_url', '')]
grok_non_x = [d for d in grok_data if 'twitter.com' not in d.get('source_url', '') and 'x.com' not in d.get('source_url', '')]

print(f"  Grok ì´: {len(grok_data)}ê°œ")
print(f"  Grok X/Twitter: {len(grok_x)}ê°œ ({len(grok_x)/len(grok_data)*100 if grok_data else 0:.1f}%)")
print(f"  Grok X ì™¸: {len(grok_non_x)}ê°œ {'âœ…' if len(grok_non_x) == 0 else 'âŒ'}")

if len(grok_non_x) > 0:
    print(f"\n  ğŸš¨ Grokì´ X ì™¸ ì¶œì²˜ ìˆ˜ì§‘ (V30 ìœ„ë°˜!):")
    for i, d in enumerate(grok_non_x[:5], 1):
        print(f"    {i}. {d.get('source_name', 'Unknown')}: {d['source_url'][:60]}")
print()

# ìµœì¢… ìš”ì•½
print("="*80)
print("ğŸ“‹ í’ˆì§ˆ ê²€ì¦ ìš”ì•½")
print("="*80)

issues = []
if 'Perplexity' in ai_counts:
    issues.append(f"ğŸš¨ Perplexity ë°ì´í„° {ai_counts['Perplexity']}ê°œ (V30ì€ 0ê°œì—¬ì•¼ í•¨)")
if len(missing_category) > 0:
    issues.append(f"ğŸš¨ category í•„ë“œ ì—†ìŒ {len(missing_category)}ê°œ (V30 í•„ìˆ˜ í•„ë“œ)")
if len(has_source_type) < total:
    issues.append(f"ğŸš¨ source_type í•„ë“œ ì—†ìŒ {total - len(has_source_type)}ê°œ (V30 í•„ìˆ˜ í•„ë“œ)")
if len(missing_title) > 0:
    issues.append(f"âŒ title ëˆ„ë½ {len(missing_title)}ê°œ")
if len(missing_content) > 0:
    issues.append(f"âŒ content ëˆ„ë½ {len(missing_content)}ê°œ")
if len(missing_url) > 0:
    issues.append(f"âŒ source_url ëˆ„ë½ {len(missing_url)}ê°œ")
if len(fake_urls) > 0:
    issues.append(f"âŒ ê°€ì§œ URL {len(fake_urls)}ê°œ")
if len(out_of_range) > 0:
    issues.append(f"âŒ ê¸°ê°„ ì´ˆê³¼ {len(out_of_range)}ê°œ")
if len(duplicates) > 0:
    issues.append(f"âš ï¸ ì¤‘ë³µ {len(duplicates)}ê°œ")
if len(grok_non_x) > 0:
    issues.append(f"âŒ Grok X ì™¸ ìˆ˜ì§‘ {len(grok_non_x)}ê°œ")

if issues:
    print("ğŸš¨ ë°œê²¬ëœ í’ˆì§ˆ ë¬¸ì œ:")
    for i, issue in enumerate(issues, 1):
        print(f"  {i}. {issue}")
    print()
    print("ğŸ’¡ ê²°ë¡ : ì´ ë°ì´í„°ëŠ” V30 ìŠ¤í‚¤ë§ˆê°€ ì•„ë‹ˆë¼ ì´ì „ ë²„ì „(V28?) ë°ì´í„°ì…ë‹ˆë‹¤.")
    print("          collected_data_v30 í…Œì´ë¸”ì„ ë¹„ìš°ê³  V30 ìŠ¤í¬ë¦½íŠ¸ë¡œ ì¬ìˆ˜ì§‘ í•„ìš”!")
else:
    print("âœ… ëª¨ë“  í’ˆì§ˆ ê²€ì¦ í†µê³¼!")

print("="*80)

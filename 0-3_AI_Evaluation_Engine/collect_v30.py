# -*- coding: utf-8 -*-
#
# V30 Data Collection Script (40-20-40 ë°°ë¶„ í™•ì • - 2026-01-31)
#
# ============================================================
# ğŸš¨ ì ˆëŒ€ ê·œì¹™ (40-20-40 ë°°ë¶„, ì¹´í…Œê³ ë¦¬ë‹¹ 50ê°œ+20%ë²„í¼)
# ============================================================
# | êµ¬ë¶„              | ê¸°ë³¸ | ìµœëŒ€(120%) | ì—­í•                          |
# |-------------------|------|------------|------------------------------|
# | Gemini OFFICIAL   | 20ê°œ | 24ê°œ       | êµ­íšŒ, ì •ë¶€, ì§€ë°©ì •ë¶€, ê³µê³µê¸°ê´€|
# | Gemini PUBLIC     | 10ê°œ | 12ê°œ       | YouTube, ë¸”ë¡œê·¸, ìœ„í‚¤ (ë¹„ì–¸ë¡ )|
# | Perplexity PUBLIC | 20ê°œ | 24ê°œ       | ë‰´ìŠ¤/ì–¸ë¡ ë§Œ                   |
# | ì´ê³„              | 50ê°œ | 60ê°œ       |                              |
# ============================================================
#
# ê²€ì¦ í›„ ì²˜ë¦¬:
#   - 50ê°œ ì´ìƒ â†’ íŒ¨ìŠ¤ âœ…
#   - 50ê°œ ë¯¸ë§Œ â†’ ì¶”ê°€ ìˆ˜ì§‘ ğŸ”„ (ë²„í¼ ë²”ìœ„ ë‚´ ìµœëŒ€ 60ê°œ)
#
# ì—­í•  ë¶„ë‹´:
#   - Gemini: OFFICIAL (ì •ë¶€/ê³µê³µ) + PUBLIC (ë¹„ì–¸ë¡ : YouTube, ë¸”ë¡œê·¸, ìœ„í‚¤)
#   - Perplexity: PUBLICë§Œ (ë‰´ìŠ¤/ì–¸ë¡  55ê°œ+ ì–¸ë¡ ì‚¬)
#
# [WARN] Claude/ChatGPT/Grok = ìˆ˜ì§‘ ì œì™¸ (í‰ê°€ë§Œ)
#
# Usage:
#     # Full Collection (Gemini + Perplexity)
#     python collect_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ"
#
#     # Run specific AI only
#     python collect_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --ai=Gemini
#     python collect_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --ai=Perplexity
#
#     # Specific Category only
#     python collect_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --category=1
#
#     # Parallel Execution (Faster, Recommended)
#     python collect_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --parallel
#
#     # Mini Test (10 items per category)
#     python collect_v30.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --parallel --test
# 

import os
import sys
import json
import re
from duplicate_check_utils import normalize_url, normalize_title, is_duplicate_by_url, is_duplicate_by_title
import argparse
import time
import random
import requests
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from supabase import create_client
from dotenv import load_dotenv
import uuid # Import uuid for unique ID generation

# UTF-8 Output Setting
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except AttributeError:
        # Ignore if already set or buffer is missing
        pass

# ============================================================
# URL ê²€ì¦ í•¨ìˆ˜ (ë“€ì–¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜ì˜ - 2026-01-29)
# ============================================================

def validate_url(url: str, timeout: float = 5.0) -> bool:
    """URL ê²€ì¦ (GET stream=True + User-Agent) - 90%+ ì„±ê³µë¥ 

    ê¸°ì¡´ HEAD ìš”ì²­ ë°©ì‹ì˜ ë¬¸ì œì  í•´ê²°:
    - ì¼ë¶€ ì„œë²„ê°€ HEAD ìš”ì²­ ì°¨ë‹¨
    - User-Agent ì—†ìœ¼ë©´ 403/406 ì‘ë‹µ

    í•´ê²°ì±…:
    - GET + stream=True (ì „ì²´ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
    - User-Agent í—¤ë” ì¶”ê°€
    """
    if not url or 'dummy' in url.lower():
        return False

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    try:
        response = requests.get(url, timeout=timeout, allow_redirects=True,
                               headers=headers, stream=True)
        response.close()  # ë°”ë¡œ ë‹«ê¸° (ì „ì²´ ë‹¤ìš´ë¡œë“œ ë°©ì§€)
        return response.status_code < 400
    except:
        return False


def resolve_redirect_url(redirect_url: str, timeout: float = 10.0) -> str:
    """Gemini redirect URLì„ ì‹¤ì œ URLë¡œ ë³€í™˜

    Geminiê°€ ë°˜í™˜í•˜ëŠ” URL í˜•íƒœ:
    https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQ...

    í•´ê²°ì±…:
    - allow_redirects=Falseë¡œ ìš”ì²­
    - 302 ì‘ë‹µì˜ Location í—¤ë”ì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ
    """
    if not redirect_url or 'grounding-api-redirect' not in redirect_url:
        return redirect_url

    try:
        response = requests.head(redirect_url, timeout=timeout, allow_redirects=False)
        if response.status_code in [301, 302, 303, 307, 308] and 'Location' in response.headers:
            return response.headers['Location']
    except:
        pass

    return redirect_url  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜


def extract_json_from_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ë°°ì—´ì„ ì¶”ì¶œ (robust)

    AI ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•˜ëŠ” ì—¬ëŸ¬ ë°©ë²• ì‹œë„:
    1. ```json ë¸”ë¡
    2. ``` ë¸”ë¡
    3. [ ... ] ë°°ì—´ ì§ì ‘ ì¶”ì¶œ (greedy + lazy)
    4. í…ìŠ¤íŠ¸ ìì²´ê°€ JSONì¸ ê²½ìš°
    5. ê°œë³„ {} ê°ì²´ ìˆ˜ì§‘
    """
    if not text:
        return "[]"

    # ë°©ë²• 1: ```json ë¸”ë¡ ì¶”ì¶œ
    if '```json' in text:
        match = re.search(r'```json\s*([\s\S]*?)\s*```', text)
        if match:
            return match.group(1).strip()

    # ë°©ë²• 2: ``` ë¸”ë¡ ì¶”ì¶œ
    if '```' in text:
        match = re.search(r'```\s*([\s\S]*?)\s*```', text)
        if match:
            content = match.group(1).strip()
            if content.startswith('['):
                return content

    # ë°©ë²• 3: [ ... ] ë°°ì—´ ì§ì ‘ ì¶”ì¶œ
    match = re.search(r'\[\s*\{[\s\S]*\}\s*\]', text)
    if match:
        return match.group(0)

    # ë°©ë²• 4: í…ìŠ¤íŠ¸ ìì²´ê°€ JSONì¼ ê²½ìš°
    stripped = text.strip()
    if stripped.startswith('[') and stripped.endswith(']'):
        return stripped

    # ë°©ë²• 5: ê°œë³„ {} ê°ì²´ ìˆ˜ì§‘í•˜ì—¬ ë°°ì—´ë¡œ ë§Œë“¤ê¸°
    objects = re.findall(r'\{[^{}]+\}', text)
    if objects:
        valid_objects = []
        for obj in objects:
            try:
                parsed = json.loads(obj)
                if isinstance(parsed, dict) and ('title' in parsed or 'source_url' in parsed or 'url' in parsed):
                    valid_objects.append(obj)
            except:
                pass
        if valid_objects:
            return '[' + ','.join(valid_objects) + ']'

    return "[]"


# ============================================================
# topic_mode -> DB sentiment mapping
# ============================================================ 
def topic_mode_to_sentiment(topic_mode):
    """Converts topic_mode to DB sentiment value

    Args:
        topic_mode: 'negative', 'positive', 'free'

    Returns:
        sentiment: 'negative', 'positive', 'free'

    Notes:
        - DB sentiment CHECK constraint: ('positive', 'negative', 'neutral', 'free')
        - topic_mode 'free' is saved as 'free' in DB
        - 'free' = freely collected (includes positive/negative/neutral)
    """
    mapping = {
        'negative': 'negative',
        'positive': 'positive',
        'free': 'free'  # [OK] Modified: 'free' saved as is
    }
    return mapping.get(topic_mode, 'free')


# Load environment variables
load_dotenv(override=True)

# Supabase client
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

# V30 Table Name
TABLE_COLLECTED_DATA = "collected_data_v30"

# AI Client Cache
ai_clients = {}

# Category Definitions (Based on V30 - V28.3)
CATEGORIES = [
    ("expertise", "Expertise"),
    ("leadership", "Leadership"),
    ("vision", "Vision"),
    ("integrity", "Integrity"),
    ("ethics", "Ethics"),
    ("accountability", "Accountability"),
    ("transparency", "Transparency"),
    ("communication", "Communication"),
    ("responsiveness", "Responsiveness"),
    ("publicinterest", "PublicInterest")
]

# ============================================================
# ğŸš¨ V30 ì ˆëŒ€ ê·œì¹™ - ìˆ˜ì§‘ ë°°ë¶„ (40-20-40, 20% ë²„í¼)
# ============================================================
# | êµ¬ë¶„              | ê¸°ë³¸ | ìµœëŒ€(120%) | ì—­í•                          |
# |-------------------|------|------------|------------------------------|
# | Gemini OFFICIAL   | 20   | 24ê°œ       | êµ­íšŒ, ì •ë¶€, ì§€ë°©ì •ë¶€, ê³µê³µê¸°ê´€|
# | Gemini PUBLIC     | 10   | 12ê°œ       | YouTube, ë¸”ë¡œê·¸, ìœ„í‚¤ (ë¹„ì–¸ë¡ )|
# | Perplexity PUBLIC | 20   | 24ê°œ       | ë‰´ìŠ¤/ì–¸ë¡ ë§Œ                   |
# | ì´ê³„              | 50   | 60ê°œ       |                              |
# ============================================================
COLLECT_DISTRIBUTION = {
    "Gemini": {
        "official": 24,  # OFFICIAL 100% (20 + 20%ë²„í¼) - êµ­íšŒ, ì •ë¶€, ì§€ë°©ì •ë¶€
        "public": 12,    # PUBLIC ë¹„ì–¸ë¡  (10 + 20%ë²„í¼) - YouTube, ë¸”ë¡œê·¸, ìœ„í‚¤
        "total": 36      # 36ê°œ (ë²„í¼ í¬í•¨)
    },
    "Perplexity": {
        "official": 0,   # OFFICIAL 0% - ìˆ˜ì§‘ ì•ˆ í•¨!
        "public": 24,    # PUBLIC ì–¸ë¡ ë§Œ (20 + 20%ë²„í¼) - ë‰´ìŠ¤/ì–¸ë¡  55ê°œ+
        "total": 24      # 24ê°œ (ë²„í¼ í¬í•¨)
    }
    # ì´ ìˆ˜ì§‘: ìµœëŒ€ 60ê°œ â†’ ê²€ì¦ â†’ 50ê°œ í™•ë³´
    #
    # [WARN] Claude: ìˆ˜ì§‘ ì œì™¸ (í‰ê°€ë§Œ) - web_search ë¹„ìš© ë¬¸ì œ
    # [WARN] ChatGPT: ìˆ˜ì§‘ ì œì™¸ (í‰ê°€ë§Œ) - Bing ê²€ìƒ‰ ë¹„ìš© ë¬¸ì œ
    # [WARN] Grok: ìˆ˜ì§‘ ì œì™¸ (í‰ê°€ë§Œ) - X ë°ì´í„° ìˆ˜ì§‘ ë¶ˆì•ˆì •
}

# ì¶”ê°€ ë²„í¼ ì—†ìŒ (120% ë²„í¼ê°€ COLLECT_DISTRIBUTIONì— ì´ë¯¸ í¬í•¨)
EXTRA_BUFFER = 0

# Mini Test Allocation (1/5 scale) - ì ˆëŒ€ ê·œì¹™ ë¹„ìœ¨ ìœ ì§€
TEST_DISTRIBUTION = {
    "Gemini": {
        "official": 4,   # OFFICIAL (4ê°œ)
        "public": 2,     # PUBLIC ë¹„ì–¸ë¡  (2ê°œ)
        "total": 6       # 6ê°œ
    },
    "Perplexity": {
        "official": 0,   # OFFICIAL 0% - ìˆ˜ì§‘ ì•ˆ í•¨!
        "public": 4,     # PUBLIC ì–¸ë¡  (4ê°œ)
        "total": 4       # 4ê°œ
    }
    # í…ŒìŠ¤íŠ¸ ì´: 10ê°œ
}

# 20-20-60 Balance Allocation (ê¸°ë³¸ 50ê°œ ê¸°ì¤€, 40-20-40 ë°°ë¶„)
# Gemini OFFICIAL 20ê°œ: negative 4, positive 4, free 12
# Gemini PUBLIC 10ê°œ: negative 2, positive 2, free 6
# Perplexity PUBLIC 20ê°œ: negative 4, positive 4, free 12
SENTIMENT_DISTRIBUTION = {
    "Gemini": {
        "official": {"negative": 4, "positive": 4, "free": 12},    # 20ê°œ
        "public": {"negative": 2, "positive": 2, "free": 6}        # 10ê°œ
    },
    "Perplexity": {
        "official": {"negative": 0, "positive": 0, "free": 0},     # 0ê°œ - ìˆ˜ì§‘ ì•ˆ í•¨!
        "public": {"negative": 4, "positive": 4, "free": 12}       # 20ê°œ
    }
}
# ì´ê³„: Gemini 30 (OFFICIAL 20 + PUBLIC 10) + Perplexity 20 (PUBLIC) = 50ê°œ

# Test Mode 20-20-60 Allocation (1/5 scale)
TEST_SENTIMENT_DISTRIBUTION = {
    "Gemini": {
        "official": {"negative": 1, "positive": 1, "free": 2},     # 4ê°œ
        "public": {"negative": 1, "positive": 1, "free": 0}        # 2ê°œ
    },
    "Perplexity": {
        "official": {"negative": 0, "positive": 0, "free": 0},     # 0ê°œ
        "public": {"negative": 1, "positive": 1, "free": 1}        # 3ê°œ
    }
}

# AI Model Configuration
AI_CONFIGS = {
    "Claude": {
        "model": "claude-3-5-haiku-20241022",
        "env_key": "ANTHROPIC_API_KEY"
    },
    "ChatGPT": {
        "model": "gpt-4o-mini",
        "env_key": "OPENAI_API_KEY"
    },
    "Grok": {
        "model": "grok-3",
        "env_key": "XAI_API_KEY",
        "base_url": "https://api.x.ai/v1"
    },
    "Gemini": {
        "model": "gemini-2.0-flash",
        "env_key": "GEMINI_API_KEY"
    },
    "Perplexity": {
        "model": "sonar",
        "env_key": "PERPLEXITY_API_KEY",
        "base_url": "https://api.perplexity.ai"
    }
}

# Official Data Domains (Gemini OFFICIAL Source)
OFFICIAL_DOMAINS = [
    "assembly.go.kr",
    "likms.assembly.go.kr",
    "mois.go.kr",
    "korea.kr",
    "nec.go.kr",
    "bai.go.kr",
    "pec.go.kr",
    "scourt.go.kr",
    "nesdc.go.kr",
    "manifesto.or.kr",
    "peoplepower21.org",
    "theminjoo.kr",
    "seoul.go.kr",
    "gg.go.kr",
    "busan.go.kr",
    "incheon.go.kr",
    "daegu.go.kr",
    "daejeon.go.kr",
    "gwangju.go.kr",
    "ulsan.go.kr",
    "sejong.go.kr",
    "open.go.kr",
    "acrc.go.kr",
    "humanrights.go.kr"
]

# ============================================================
# Gemini ë„ë©”ì¸ ìˆœí™˜ ë¦¬ìŠ¤íŠ¸ (URL ë‹¤ì–‘ì„± í™•ë³´)
# ============================================================
GEMINI_OFFICIAL_DOMAIN_HINTS = [
    "site:assembly.go.kr",          # 0: êµ­íšŒ
    "site:likms.assembly.go.kr",    # 1: ì˜ì•ˆì •ë³´ì‹œìŠ¤í…œ
    "site:korea.kr",                # 2: ì •ë¶€ ëŒ€í‘œ
    "site:open.go.kr",              # 3: ì •ë³´ê³µê°œ
    "site:seoul.go.kr",             # 4: ì„œìš¸ì‹œ
    "site:manifesto.or.kr OR site:nec.go.kr",  # 5: ì„ ê±°/ê³µì•½
    "site:mois.go.kr OR site:acrc.go.kr",      # 6: í–‰ì•ˆë¶€/ê¶Œìµìœ„
    "site:bai.go.kr OR site:peoplepower21.org", # 7: ê°ì‚¬ì›/ì°¸ì—¬ì—°ëŒ€
    "",                             # 8: ììœ  ê²€ìƒ‰
    "",                             # 9: ììœ  ê²€ìƒ‰
]

# Gemini PUBLIC í”Œë«í¼ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ì§€ì‹œë¬¸
# í…ŒìŠ¤íŠ¸ ê²°ê³¼: í”Œë«í¼ ì „ìš© ì¿¼ë¦¬ ì‹œ YouTube 100%, ë‚˜ë¬´ìœ„í‚¤ 100% ì ì¤‘ í™•ì¸
GEMINI_PUBLIC_PLATFORM_HINTS = [
    "ìœ íŠœë¸Œ\në°˜ë“œì‹œ youtube.com URLë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì¸í„°ë·°, í† ë¡ , ì—°ì„¤, ì •ì±… ì„¤ëª… ì˜ìƒ.",
    "ìœ íŠœë¸Œ\në°˜ë“œì‹œ youtube.com URLë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì‹œë¯¼/ê°œì¸ ì±„ë„ì˜ ë¶„ì„ ì˜ìƒ.",
    "ìœ íŠœë¸Œ\në°˜ë“œì‹œ youtube.com URLë§Œ ë°˜í™˜í•˜ì„¸ìš”. í† ë¡ /ëŒ€ë‹´/íŒŸìºìŠ¤íŠ¸ ì˜ìƒ.",
    "ë‚˜ë¬´ìœ„í‚¤\në°˜ë“œì‹œ namu.wiki URLë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì •ì¹˜ì¸ ë¬¸ì„œ, ë…¼ë€/ë¹„íŒ, í‰ê°€ ë‚´ìš©.",
    "ë‚˜ë¬´ìœ„í‚¤\në°˜ë“œì‹œ namu.wiki URLë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì„ ê±° ì´ë ¥, ê³µì•½ ì´í–‰, ì˜ì •í™œë™.",
    "ë¸”ë¡œê·¸\në°˜ë“œì‹œ blog.naver.com ë˜ëŠ” tistory.com URLë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì‹œë¯¼ ë¶„ì„/í‰ê°€ ê¸€.",
    "ë¸”ë¡œê·¸\në°˜ë“œì‹œ brunch.co.kr ë˜ëŠ” medium.com URLë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì¹¼ëŸ¼/ë¶„ì„ ê¸€.",
    "ì»¤ë®¤ë‹ˆí‹°\në°˜ë“œì‹œ clien.net, theqoo.net, fmkorea.com, dcinside.com ì¤‘ í•˜ë‚˜ì˜ URLë§Œ ë°˜í™˜í•˜ì„¸ìš”.",
    "ì»¤ë®¤ë‹ˆí‹°\në°˜ë“œì‹œ ppomppu.co.kr, 82cook.com, ruliweb.com, todayhumor.co.kr ì¤‘ í•˜ë‚˜ì˜ URLë§Œ ë°˜í™˜í•˜ì„¸ìš”.",
    "ì‹œë¯¼ë‹¨ì²´ í•™ìˆ \nì°¸ì—¬ì—°ëŒ€(peoplepower21.org), ê²½ì‹¤ë ¨(ccej.or.kr), Google Scholar, RISSì—ì„œ ê²€ìƒ‰í•˜ì„¸ìš”.",
]

# ============================================================
# Gemini PUBLIC ë‰´ìŠ¤ ë„ë©”ì¸ ì°¨ë‹¨ ë¦¬ìŠ¤íŠ¸
# ============================================================
# Gemini PUBLICì€ ë¹„ì–¸ë¡ ë§Œ ìˆ˜ì§‘ (ë‰´ìŠ¤/ì–¸ë¡ ì€ Perplexity ë‹´ë‹¹)
GEMINI_PUBLIC_NEWS_BLOCKED = [
    # ì¢…í•©ì¼ê°„ì§€
    'chosun.com', 'joongang.co.kr', 'donga.com', 'hani.co.kr', 'khan.co.kr',
    'munhwa.com', 'kmib.co.kr', 'segye.com', 'naeil.com', 'hankookilbo.com',
    # ë°©ì†¡ì‚¬
    'kbs.co.kr', 'mbc.co.kr', 'sbs.co.kr', 'jtbc.co.kr', 'tvchosun.com',
    'channela.com', 'mbn.co.kr', 'ytn.co.kr',
    # í†µì‹ ì‚¬
    'yna.co.kr', 'yonhapnews.co.kr', 'newsis.com', 'news1.kr',
    # ê²½ì œì§€
    'hankyung.com', 'mk.co.kr', 'sedaily.com', 'edaily.co.kr', 'mt.co.kr',
    'fnnews.com', 'etnews.com', 'businesspost.co.kr', 'asiae.co.kr',
    # ì¸í„°ë„·ë§¤ì²´
    'ohmynews.com', 'pressian.com', 'mediatoday.co.kr', 'newstapa.org',
    'sisajournal.com', 'sisain.co.kr', 'huffpost.kr',
    # ì§€ì—­ì–¸ë¡  íŒ¨í„´ (ê³µí†µ suffix)
    'kyeongin.com', 'kyeonggi.com', 'joongboo.com', 'kgnews.co.kr',
    'idomin.com', 'jnilbo.com', 'kwangju.co.kr',
    # í•´ì™¸ì–¸ë¡ 
    'reuters.com', 'bbc.com', 'bbc.co.uk', 'cnn.com', 'nytimes.com',
    'washingtonpost.com', 'theguardian.com', 'apnews.com',
    # ë‰´ìŠ¤ í¬í„¸/ì§‘í•©
    'v.daum.net', 'news.naver.com', 'news.nate.com',
    # ê¸°íƒ€ ë‰´ìŠ¤ íŒ¨í„´
    'newspim.com', 'newdaily.co.kr', 'nocutnews.co.kr', 'polinews.co.kr',
    'pennmike.com', 'thefact.co.kr', 'wikitree.co.kr',
    # í…ŒìŠ¤íŠ¸ì—ì„œ ëˆ„ì¶œëœ ì†Œê·œëª¨ ì–¸ë¡ 
    'joongang.tv', 'ntoday.co.kr', 'economytribune.co.kr',
    'labortoday.co.kr', 'ngonews.kr', 'snilbo.co.kr',
    'jeonmae.co.kr', 'ikbn.news', 'fieldnews.kr',
    'sigryang.com', 'koreasisailbo.com', 'senews.kr',
    'hyundaiilbo.com', 'jeongpil.com', 'seoulcity.co.kr',
    'k-health.com', 'fntoday.co.kr', 'asn24.com',
    'kspnews.com', 'snfocus.net', '5donews.co.kr',
    'kihoilbo.co.kr', 'newspak.co.kr', 'g-enews.com',
    'anseongnews.com', 'lghellovision.net', 'newsprime.co.kr',
]

# Politician Details Cache
_politician_info_cache = {}

def get_politician_info(politician_id):
    """Retrieve detailed politician information (for distinguishing identical names)"""
    if politician_id in _politician_info_cache:
        return _politician_info_cache[politician_id]

    try:
        result = supabase.table('politicians').select('*').eq('id', politician_id).execute()
        if result.data:
            p = result.data[0]
            name = p.get('name', '')
            party = p.get('party', '')
            position = p.get('position', 'National Assembly Member')
            district = p.get('district', '')
            previous_position = p.get('previous_position', '')
            gender = p.get('gender', '')
            birth_year = ""
            if p.get('birth_date'):
                birth_year = str(p.get('birth_date'))[:4] + "ë…„ìƒ"

            # search_string êµ¬ì„±: í•µì‹¬ì •ë³´ + êµ¬ë¶„ì •ë³´
            search_string = f"{party} {name} {position}"
            if district:
                search_string += f" {district}"
            if previous_position:
                search_string += f" (ì „ì§: {previous_position})"
            if gender:
                search_string += f" {gender}"
            if birth_year:
                search_string += f" {birth_year}"

            info = {
                'name': name,
                'party': party,
                'position': position,
                'district': district,
                'birth_year': birth_year,
                'search_string': search_string
            }
            _politician_info_cache[politician_id] = info
            return info
    except Exception as e:
        print(f"  [WARN] Failed to retrieve politician info: {e}")

    return {'name': '', 'party': '', 'position': 'National Assembly Member', 'district': '', 'birth_year': '', 'search_string': ''}

# Category Items (V28.3 Neutralized)
def load_category_items_from_instructions():
    """instructions íŒŒì¼ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ 10ê°œ í•­ëª©ì„ ë™ì ìœ¼ë¡œ ë¡œë“œ.

    íŒŒì¼ ìœ„ì¹˜: ì„¤ê³„ë¬¸ì„œ_V7.0/V30/instructions/2_collect/cat{01~10}_{category}.md
    í…Œì´ë¸” í˜•ì‹: | # | **í•­ëª©ëª…** | ì„¤ëª… |

    Returns:
        dict: { "expertise": [("í•­ëª©ëª…", "ì„¤ëª…"), ...], ... }
    """
    import glob

    base_dir = os.path.join(os.path.dirname(__file__), "ì„¤ê³„ë¬¸ì„œ_V7.0", "V30", "instructions", "2_collect")
    category_file_map = {
        "expertise": "cat01_expertise.md",
        "leadership": "cat02_leadership.md",
        "vision": "cat03_vision.md",
        "integrity": "cat04_integrity.md",
        "ethics": "cat05_ethics.md",
        "accountability": "cat06_accountability.md",
        "transparency": "cat07_transparency.md",
        "communication": "cat08_communication.md",
        "responsiveness": "cat09_responsiveness.md",
        "publicinterest": "cat10_publicinterest.md",
    }

    result = {}
    for category_name, filename in category_file_map.items():
        filepath = os.path.join(base_dir, filename)
        items = []
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            # í…Œì´ë¸”ì—ì„œ í•­ëª© ì¶”ì¶œ: | N-N | **í•­ëª©ëª…** | ì„¤ëª… |
            import re as _re
            pattern = r'\|\s*\d+-\d+\s*\|\s*\*\*(.+?)\*\*\s*\|\s*(.+?)\s*\|'
            matches = _re.findall(pattern, content)

            for item_name, description in matches:
                # (í•­ëª©ëª…, ì„¤ëª…) - ì„¤ëª…ì„ ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ í™œìš©
                items.append((item_name.strip(), description.strip()))

            if len(items) != 10:
                print(f"  [WARN] {category_name}: {len(items)}ê°œ í•­ëª© ë¡œë“œ (10ê°œ ì˜ˆìƒ)")

        except FileNotFoundError:
            print(f"  [ERROR] íŒŒì¼ ì—†ìŒ: {filepath}")
        except Exception as e:
            print(f"  [ERROR] {category_name} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ê¸°ë³¸ê°’
        if not items:
            items = [("", "")]

        result[category_name] = items

    return result

# instructions íŒŒì¼ì—ì„œ ë™ì  ë¡œë“œ (í•˜ë“œì½”ë”© ì œê±°)
CATEGORY_ITEMS = load_category_items_from_instructions()

# Translate CATEGORIES to a dictionary for easier lookup
CATEGORIES_DICT = {item[0]: item[1] for item in CATEGORIES}

# ============================================================
# SafeFormatDict: .format_map()ì—ì„œ ëˆ„ë½ í‚¤ë¥¼ ì›ë³¸ ìœ ì§€
# ============================================================
class SafeFormatDict(dict):
    """str.format_map()ì—ì„œ ëˆ„ë½ í‚¤ë¥¼ {key} í˜•íƒœë¡œ ìœ ì§€"""
    def __missing__(self, key):
        return '{' + key + '}'

# ============================================================
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë” (Single Source of Truth)
# ============================================================
_prompt_cache = {}

def load_prompt_template(ai_name, data_type):
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ì—ì„œ search_instruction + prompt_body ë¡œë“œ

    íŒŒì¼ ìœ„ì¹˜: ì„¤ê³„ë¬¸ì„œ_V7.0/V30/instructions/2_collect/prompts/
    êµ¬ë¶„ì: ---SEARCH_INSTRUCTION_START/END---, ---PROMPT_BODY_START/END---

    Returns:
        (search_instruction_template, prompt_body_template) íŠœí”Œ
        ì‹¤íŒ¨ ì‹œ (None, None)
    """
    cache_key = f"{ai_name}_{data_type}"
    if cache_key in _prompt_cache:
        return _prompt_cache[cache_key]

    file_map = {
        ("Gemini", "official"): "gemini_official.md",
        ("Gemini", "public"): "gemini_public.md",
        ("Perplexity", "public"): "perplexity_public.md",
    }
    filename = file_map.get((ai_name, data_type))
    if not filename:
        print(f"  [ERROR] No prompt template for {ai_name}/{data_type}")
        return None, None

    filepath = os.path.join(
        os.path.dirname(__file__),
        "ì„¤ê³„ë¬¸ì„œ_V7.0", "V30", "instructions", "2_collect", "prompts", filename
    )

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()

        # search_instruction ì¶”ì¶œ
        si_match = re.search(
            r'---SEARCH_INSTRUCTION_START---\s*([\s\S]*?)\s*---SEARCH_INSTRUCTION_END---',
            content
        )
        search_instruction = si_match.group(1).strip() if si_match else ""

        # prompt_body ì¶”ì¶œ
        pb_match = re.search(
            r'---PROMPT_BODY_START---\s*([\s\S]*?)\s*---PROMPT_BODY_END---',
            content
        )
        prompt_body = pb_match.group(1).strip() if pb_match else ""

        if not search_instruction and not prompt_body:
            print(f"  [ERROR] Empty template: {filepath}")
            return None, None

        _prompt_cache[cache_key] = (search_instruction, prompt_body)
        return search_instruction, prompt_body

    except FileNotFoundError:
        print(f"  [ERROR] Template file not found: {filepath}")
        return None, None
    except Exception as e:
        print(f"  [ERROR] Failed to load template {filepath}: {e}")
        return None, None

def get_exact_count(table_name, filters=None):
    """Retrieve exact count"""
    try:
        query = supabase.table(table_name).select('*', count='exact')
        if filters:
            for key, value in filters.items():
                if value is not None:
                    query = query.eq(key, value)
        response = query.limit(1).execute()
        return response.count if response.count else 0
    except Exception as e:
        print(f"  [WARN] Failed to retrieve count: {e}")
        return 0


def normalize_date(date_str):
    """Normalize date string to YYYY-MM-DD format, return None on failure"""
    if not date_str or not isinstance(date_str, str):
        return None

    date_str = date_str.strip()

    # Clearly invalid values
    if date_str.upper() in ['N/A', 'NA', 'UNKNOWN', 'UNCLEAR', 'NONE', '-', '']:
        return None
    if 'Unknown' in date_str or 'Unclear' in date_str or 'None' in date_str:
        return None

    import re

    # Check YYYY-MM-DD format
    match = re.match(r'^(\d{4})-(\d{2})-(\d{2})$', date_str)
    if match:
        year, month, day = match.groups()
        # Correct invalid month/day (00 -> 01)
        month = int(month) if int(month) > 0 else 1
        day = int(day) if int(day) > 0 else 1
        # Validate month/day range
        month = min(month, 12)
        day = min(day, 28)  # Safely limit to 28 days
        return f"{year}-{month:02d}-{day:02d}"

    # YYYY-MM (only month available) -> Convert to YYYY-MM-01
    match = re.match(r'^(\d{4})-(\d{1,2})$', date_str)
    if match:
        year, month = match.groups()
        month = int(month) if int(month) > 0 else 1
        month = min(month, 12)
        return f"{year}-{month:02d}-01"

    # YYYY (only year) -> Convert to YYYY-01-01
    if re.match(r'^\d{4}$', date_str):
        return f"{date_str}-01-01"

    # Ignore other formats
    return None


def check_politician_exists(politician_id):
    """Check politician ID"""
    try:
        result = supabase.table('politicians').select('*').eq('id', politician_id).execute()
        if result.data and len(result.data) > 0:
            return True, result.data[0].get('name', '')
        return False, None
    except Exception as e:
        print(f"  [FAIL] Politician check error: {e}")
        return False, None


def init_ai_client(ai_name):
    """Initialize AI client"""
    global ai_clients

    if ai_name in ai_clients:
        return ai_clients[ai_name]

    config = AI_CONFIGS.get(ai_name)
    if not config:
        raise ValueError(f"Unknown AI: {ai_name}")

    api_key = os.getenv(config['env_key'])
    if not api_key:
        raise ValueError(f"{config['env_key']} environment variable is not set.")

    if ai_name == "Perplexity":
        from openai import OpenAI
        ai_clients[ai_name] = OpenAI(
            api_key=api_key,
            base_url=config['base_url']
        )
    elif ai_name == "Claude":
        import anthropic
        ai_clients[ai_name] = anthropic.Anthropic(api_key=api_key)
    elif ai_name == "ChatGPT":
        from openai import OpenAI
        ai_clients[ai_name] = OpenAI(api_key=api_key)
    elif ai_name == "Grok":
        from openai import OpenAI
        ai_clients[ai_name] = OpenAI(
            api_key=api_key,
            base_url=config['base_url']
        )
    elif ai_name == "Perplexity":
        from openai import OpenAI
        ai_clients[ai_name] = OpenAI(
            api_key=api_key,
            base_url=config['base_url']
        )
    elif ai_name == "Gemini":
        from google import genai
        client = genai.Client(api_key=api_key)
        ai_clients[ai_name] = client

    return ai_clients[ai_name]


# --- Perplexity ì‹¤ì œ API í˜¸ì¶œ (ë“€ì–¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜ì˜) ---
def call_perplexity(client, prompt, data_type):
    """Perplexity API í˜¸ì¶œ (Sonar + URL ê²€ì¦)

    ë“€ì–¼ í…ŒìŠ¤íŠ¸ í•´ê²°ì±… ì ìš©:
    1. sonar ëª¨ë¸ ì‚¬ìš© (ë¹„ìš© ìµœì í™”)
    2. GET stream=Trueë¡œ URL ê²€ì¦
    3. ê¸ˆì§€ ë„ë©”ì¸ í•„í„°ë§ (ë‰´ìŠ¤/ì–¸ë¡ ë§Œ)
    """
    print(f"  [Perplexity] API í˜¸ì¶œ ì¤‘... (data_type: {data_type})")

    # Perplexity ê¸ˆì§€ ë„ë©”ì¸ (ë‰´ìŠ¤/ì–¸ë¡ ë§Œ ìˆ˜ì§‘)
    PERPLEXITY_BLOCKED_DOMAINS = [
        'youtube.com', 'youtu.be',
        'wikipedia.org', 'namu.wiki',
        'blog.naver.com', 'brunch.co.kr', 'tistory.com',
        'dcinside.com', 'clien.net', 'fmkorea.com',
        'peoplepowerparty.kr', 'theminjoo.kr',
        'assembly.go.kr', 'nanet.go.kr'
    ]

    try:
        # API í˜¸ì¶œ (sonar ëª¨ë¸ - ë¹„ìš© ìµœì í™”)
        response = client.chat.completions.create(
            model="sonar",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        content = response.choices[0].message.content
        print(f"  [Perplexity] ì‘ë‹µ ìˆ˜ì‹ ")

        # JSON ì¶”ì¶œ
        json_text = extract_json_from_text(content)

        try:
            raw_data = json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"  [Perplexity] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            # ê¸´ê¸‰ ë³µêµ¬: ê°œë³„ ê°ì²´ íŒŒì‹± ì‹œë„
            items = re.findall(r'\{[^{}]*\}', json_text)
            raw_data = []
            for item in items[:10]:
                try:
                    raw_data.append(json.loads(item))
                except:
                    pass
            if not raw_data:
                return None

        print(f"  [Perplexity] ì›ë³¸ ë°ì´í„°: {len(raw_data)}ê°œ")

        # URL ê²€ì¦ ë° í•„í„°ë§
        verified_data = []
        blocked_count = 0
        invalid_count = 0

        for item in raw_data:
            url = item.get('source_url') or item.get('url') or ''

            # dummy URL ì²´í¬
            if 'dummy' in url.lower() or not url:
                invalid_count += 1
                continue

            # ê¸ˆì§€ ë„ë©”ì¸ ì²´í¬ (PerplexityëŠ” ë‰´ìŠ¤/ì–¸ë¡ ë§Œ)
            is_blocked = any(domain in url.lower() for domain in PERPLEXITY_BLOCKED_DOMAINS)
            if is_blocked:
                blocked_count += 1
                continue

            # URL ì ‘ì† ê²€ì¦
            if not validate_url(url):
                invalid_count += 1
                continue

            # í•„ë“œëª… í†µì¼
            verified_item = {
                'title': item.get('title') or item.get('data_title') or '',
                'content': item.get('content') or item.get('data_content') or '',
                'source': item.get('source') or item.get('data_source') or '',
                'source_url': url,
                'date': item.get('date') or item.get('data_date') or ''
            }
            verified_data.append(verified_item)

        if blocked_count > 0:
            print(f"  [Perplexity] ê¸ˆì§€ ë„ë©”ì¸ ì œì™¸: {blocked_count}ê°œ")
        if invalid_count > 0:
            print(f"  [Perplexity] ë¬´íš¨ URL ì œì™¸: {invalid_count}ê°œ")
        print(f"  [Perplexity] ìµœì¢… í†µê³¼: {len(verified_data)}ê°œ")

        return json.dumps(verified_data, ensure_ascii=False, indent=2) if verified_data else None

    except Exception as e:
        print(f"  âŒ Perplexity API ì—ëŸ¬: {e}")
        return None
# --- End Perplexity ---

# --- Gemini ì‹¤ì œ API í˜¸ì¶œ (ë“€ì–¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜ì˜) ---
def call_gemini_with_search(client, prompt, data_type="public"):
    """Gemini API í˜¸ì¶œ (Google Search Grounding + URL ê²€ì¦)

    ë“€ì–¼ í…ŒìŠ¤íŠ¸ í•´ê²°ì±… ì ìš©:
    1. grounding_metadataì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ
    2. redirect URL â†’ ì‹¤ì œ URL ë³€í™˜
    3. GET stream=Trueë¡œ URL ê²€ì¦
    """
    from google.genai import types

    print(f"  [Gemini] API í˜¸ì¶œ ì¤‘...")

    try:
        # Google Search Tool ì‚¬ìš©
        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())]
            )
        )

        response_text = response.text if response.text else ""

        # JSON ì¶”ì¶œ
        json_text = extract_json_from_text(response_text)

        try:
            raw_data = json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"  [Gemini] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            # ê¸´ê¸‰ ë³µêµ¬: ê°œë³„ ê°ì²´ íŒŒì‹± ì‹œë„
            items = re.findall(r'\{[^{}]*\}', json_text)
            raw_data = []
            for item in items[:10]:
                try:
                    raw_data.append(json.loads(item))
                except:
                    pass
            if not raw_data:
                return None

        print(f"  [Gemini] ì›ë³¸ ë°ì´í„°: {len(raw_data)}ê°œ")

        # grounding_metadataì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ
        grounding_urls = []
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                gm = candidate.grounding_metadata
                if hasattr(gm, 'grounding_chunks') and gm.grounding_chunks:
                    for chunk in gm.grounding_chunks:
                        if hasattr(chunk, 'web') and chunk.web:
                            grounding_urls.append(chunk.web.uri)
                    print(f"  [Gemini] grounding_metadata URL: {len(grounding_urls)}ê°œ")

        # URL ê²€ì¦ ë° í•„í„°ë§
        verified_data = []
        redirect_resolved = 0
        invalid_count = 0
        news_blocked_count = 0

        for item in raw_data:
            url = item.get('source_url') or item.get('url') or ''

            # dummy URL ì²´í¬
            if 'dummy' in url.lower() or not url:
                invalid_count += 1
                continue

            # redirect URL â†’ ì‹¤ì œ URL ë³€í™˜
            if 'grounding-api-redirect' in url:
                real_url = resolve_redirect_url(url)
                if real_url != url:
                    redirect_resolved += 1
                    print(f"  [Gemini] redirect URL í•´ê²°: {real_url[:60]}...")
                    item['source_url'] = real_url
                    url = real_url
                else:
                    invalid_count += 1
                    continue

            # PUBLICì¼ ë•Œ ë‰´ìŠ¤ ë„ë©”ì¸ ì°¨ë‹¨
            if data_type == "public":
                url_lower = url.lower()
                is_news = any(domain in url_lower for domain in GEMINI_PUBLIC_NEWS_BLOCKED)
                # ì¶”ê°€: ~ì¼ë³´, ~ì‹ ë¬¸, ~ë‰´ìŠ¤ ë“± íŒ¨í„´ ë§¤ì¹­ (ì§€ì—­ì–¸ë¡  í¬ê´„ ì°¨ë‹¨)
                if not is_news:
                    from urllib.parse import urlparse
                    try:
                        hostname = urlparse(url).hostname or ''
                        # ë‰´ìŠ¤ì„± ë„ë©”ì¸ íŒ¨í„´: ilbo, sinmun, news, times, daily
                        news_patterns = ['ilbo', 'sinmun', 'news', 'times', 'daily', 'journal', 'herald', 'press']
                        is_news = any(p in hostname.lower() for p in news_patterns)
                    except:
                        pass
                if is_news:
                    news_blocked_count += 1
                    continue

            # URL ì ‘ì† ê²€ì¦
            if not validate_url(url):
                invalid_count += 1
                continue

            # í•„ë“œëª… í†µì¼
            verified_item = {
                'title': item.get('title') or item.get('data_title') or '',
                'content': item.get('content') or item.get('data_content') or '',
                'source': item.get('source') or item.get('data_source') or '',
                'source_url': url,
                'date': item.get('date') or item.get('data_date') or ''
            }
            verified_data.append(verified_item)

        print(f"  [Gemini] redirect URL í•´ê²°: {redirect_resolved}ê°œ")
        if news_blocked_count > 0:
            print(f"  [Gemini] ë‰´ìŠ¤ ë„ë©”ì¸ ì°¨ë‹¨: {news_blocked_count}ê°œ")
        if invalid_count > 0:
            print(f"  [Gemini] ë¬´íš¨ URL ì œì™¸: {invalid_count}ê°œ")
        print(f"  [Gemini] ìµœì¢… í†µê³¼: {len(verified_data)}ê°œ")

        return json.dumps(verified_data, ensure_ascii=False, indent=2) if verified_data else None

    except Exception as e:
        error_str = str(e)
        if '429' in error_str or 'RESOURCE_EXHAUSTED' in error_str:
            # 429 ì—ëŸ¬ëŠ” ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ retry í—ˆìš©
            raise
        print(f"  âŒ Gemini API ì—ëŸ¬: {e}")
        return None
# --- End Gemini ---

# --- Placeholder for call_claude_with_websearch ---
def call_claude_with_websearch(client, prompt):
    """Claude API Call (Placeholder) - Returns multiple unique items"""
    print(f"  [CALL] Claude API Call Attempt")
    count_match = re.search(r'Count: (\d+) items', prompt)
    count = int(count_match.group(1)) if count_match else 1 # Default 1

    items = []
    for i in range(count):
        item_id = random.randint(1000, 9999) # Generate unique ID
        title = f"Claude Dummy Title {item_id}"
        content = f"Claude Dummy Content {item_id} - {prompt[:50]}..."
        source = f"Claude_Dummy_Source_{item_id}"
        url = f"https://dummy.claude.com/article/{item_id}"
        date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d") # Unique date

        items.append({
            "title": title,
            "content": content,
            "source": source,
            "source_url": url,
            "date": date
        })

    return json.dumps(items, ensure_ascii=False, indent=2) # Return as JSON string
# --- End Placeholder ---

# --- Placeholder for call_grok ---
def call_grok(client, prompt):
    """Grok API Call (Placeholder) - Returns multiple unique items"""
    print(f"  [CALL] Grok API Call Attempt")
    count_match = re.search(r'Count: (\d+) items', prompt)
    count = int(count_match.group(1)) if count_match else 1 # Default 1

    items = []
    for i in range(count):
        item_id = random.randint(1000, 9999) # Generate unique ID
        title = f"Grok Dummy Title {item_id}"
        content = f"Grok Dummy Content {item_id} - {prompt[:50]}..."
        source = f"Grok_Dummy_Source_{item_id}"
        url = f"https://dummy.grok.com/article/{item_id}"
        date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d") # Unique date

        items.append({
            "title": title,
            "content": content,
            "source": source,
            "source_url": url,
            "date": date
        })

    return json.dumps(items, ensure_ascii=False, indent=2) # Return as JSON string
# --- End Placeholder ---


def call_ai(ai_name, client, prompt, data_type="public"):
    """Unified AI Call Function"""
    if ai_name == "Perplexity":
        return call_perplexity(client, prompt, data_type)
    elif ai_name == "Claude":
        print(f"  [WARN] Claude web_search is not yet implemented. Skipping call.")
        return None
    elif ai_name == "Gemini":
        return call_gemini_with_search(client, prompt, data_type)
    elif ai_name == "Grok":
        print(f"  [WARN] Grok X search is not yet implemented. Skipping call.")
        return None
    return None


def get_date_range():
    """Calculate V30 Time Limit"""
    evaluation_date = datetime.now()
    official_start = evaluation_date - timedelta(days=365*4)  # 4 years
    public_start = evaluation_date - timedelta(days=365*2)    # 2 years

    return {
        'evaluation_date': evaluation_date.strftime('%Y-%m-%d'),
        'official_start': official_start.strftime('%Y-%m-%d'),
        'official_end': evaluation_date.strftime('%Y-%m-%d'),
        'public_start': public_start.strftime('%Y-%m-%d'),
        'public_end': evaluation_date.strftime('%Y-%m-%d'),
    }


def build_search_prompt(ai_name, data_type, topic_mode, politician_full, item_keywords, remaining, year_hint, extra_keyword, exclude_urls=None, domain_hint=""):
    """Generate V30 Collection Prompt (ë¦¬íŒ©í† ë§: í…œí”Œë¦¿ íŒŒì¼ ë™ì  ë¡œë“œ)

    ì‹¤ì œ ìˆ˜ì§‘ ì¡°í•© 3ê°€ì§€ë§Œ:
    - Gemini OFFICIAL (24ê°œ) - êµ­íšŒ, ì •ë¶€, ê³µê³µê¸°ê´€
    - Gemini PUBLIC (12ê°œ) - ë¹„ì–¸ë¡  (YouTube, ë¸”ë¡œê·¸, ìœ„í‚¤)
    - Perplexity PUBLIC (24ê°œ) - ë‰´ìŠ¤/ì–¸ë¡ ë§Œ

    [WARN] Perplexity OFFICIAL = dead code ì‚­ì œë¨ (official: 0)
    [WARN] Claude/ChatGPT/Grok = ìˆ˜ì§‘ ì œì™¸ (í‰ê°€ë§Œ ë‹´ë‹¹)
    """
    # ì§€ì› ì¡°í•© ê²€ì¦ (dead code ë°©ì§€)
    valid_combos = {("Gemini", "official"), ("Gemini", "public"), ("Perplexity", "public")}
    if (ai_name, data_type) not in valid_combos:
        return None

    # í…œí”Œë¦¿ ë¡œë“œ
    search_instruction_tpl, prompt_body_tpl = load_prompt_template(ai_name, data_type)
    if not search_instruction_tpl and not prompt_body_tpl:
        print(f"  [ERROR] Failed to load prompt template for {ai_name}/{data_type}")
        return None

    # topic_instruction êµ¬ì„± (AIë³„ ì°¨ë“±)
    # Perplexity: "ë¶€ì •ì  ë‰´ìŠ¤ ì°¾ì•„ë¼" â†’ ì•ˆì „ í•„í„° ì¶©ëŒ. ê²€ìƒ‰ í‚¤ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ìš°íšŒ
    # Gemini: Google Search Groundingì´ë¯€ë¡œ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
    if ai_name == "Perplexity":
        if topic_mode == "negative":
            topic_instruction = (
                f"ê²€ìƒ‰ì–´ì— ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì—¬ {politician_full} ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ìœ¼ì„¸ìš”: "
                "ë…¼ë€ OR ì˜í˜¹ OR ë¹„íŒ OR ë¬¸ì œì œê¸° OR ì§€ì  OR ë°˜ë°œ OR ê°ˆë“± OR ê³ ë°œ OR ìˆ˜ì‚¬ OR ì‚¬í‡´ OR íƒ„í•µ OR íŒŒë©´"
            )
        elif topic_mode == "positive":
            topic_instruction = (
                f"ê²€ìƒ‰ì–´ì— ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì—¬ {politician_full} ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ìœ¼ì„¸ìš”: "
                "ì„±ê³¼ OR ì—…ì  OR ì¹­ì°¬ OR í˜¸í‰ OR ìˆ˜ìƒ OR ì„ ì • OR ê³µë¡œ OR ê¸°ì—¬ OR í˜ì‹  OR ê°œì„  OR í•´ê²°"
            )
        else:
            topic_instruction = f"{politician_full} ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ììœ ë¡­ê²Œ ê²€ìƒ‰í•˜ì„¸ìš”. ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¬´ê´€."
    else:  # Gemini
        if topic_mode == "negative":
            topic_instruction = f"Verify if this content is negative for {politician_full}."
        elif topic_mode == "positive":
            topic_instruction = f"Verify if this content is positive for {politician_full}."
        else:
            topic_instruction = f"This content is related to {politician_full}, collect freely regardless of positive/negative/neutral."

    # exclude_instruction êµ¬ì„± (ì´ë¯¸ ìˆ˜ì§‘í•œ URL ì œì™¸)
    exclude_instruction = ""
    if exclude_urls and len(exclude_urls) > 0:
        max_urls = 30 if ai_name == "Gemini" else 20
        exclude_list = "\n".join([f"- {u}" for u in list(exclude_urls)[:max_urls]])
        if ai_name == "Gemini":
            exclude_instruction = f"""
âš ï¸ IMPORTANT: The following URLs have already been collected. Do NOT include them:
{exclude_list}

You MUST find DIFFERENT URLs not listed above!
"""
        else:
            exclude_instruction = f"""
âš ï¸ ì•„ë˜ URLì€ ì´ë¯¸ ìˆ˜ì§‘í–ˆìœ¼ë¯€ë¡œ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:
{exclude_list}

ë°˜ë“œì‹œ ìœ„ URLê³¼ ë‹¤ë¥¸ ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”!
"""

    # ë³€ìˆ˜ ì¹˜í™˜
    fmt_vars = {
        'politician_full': politician_full,
        'item_keywords': item_keywords,
        'extra_keyword': extra_keyword,
        'year_hint': year_hint,
        'remaining': min(remaining, 10),
        'topic_instruction': topic_instruction,
        'exclude_instruction': exclude_instruction,
        'domain_hint': domain_hint,
    }

    search_instruction = search_instruction_tpl.format_map(SafeFormatDict(fmt_vars))
    prompt_body = prompt_body_tpl.format_map(SafeFormatDict(fmt_vars))

    return search_instruction + "\n" + prompt_body


def extract_url(item):
    """Extract URL from item (type safe)"""
    url = item.get('source_url') or item.get('url') or ''
    if isinstance(url, list):
        return url[0] if url else ''
    elif not isinstance(url, str):
        return str(url) if url else ''
    return url


def validate_collected_data(items, expected_data_type, ai_name):
    """Validate collected data (per-item filtering)

    Returns:
        tuple: (valid_items list, error_messages list)
        - valid_items: ê²€ì¦ í†µê³¼í•œ í•­ëª© ë¦¬ìŠ¤íŠ¸
        - errors: ê²½ê³ /ì—ëŸ¬ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸

    Note: ê°œë³„ í•­ëª©ë³„ í•„í„°ë§ (all-or-nothingì´ ì•„ë‹˜!)
    """
    errors = []
    valid_items = []

    if not isinstance(items, list):
        errors.append("Response is not a list format.")
        return [], errors

    for i, item in enumerate(items):
        if not isinstance(item, dict):
            errors.append(f"Item {i} is not a dictionary format.")
            continue

        # í•„ìˆ˜ í•„ë“œ ì²´í¬ (title, content, source_urlë§Œ í•„ìˆ˜, date/sourceëŠ” ì„ íƒ)
        title = item.get('title') or item.get('data_title') or ''
        content = item.get('content') or item.get('data_content') or ''
        source_url = extract_url(item)

        if not title and not content:
            errors.append(f"Item {i}: titleê³¼ content ëª¨ë‘ ë¹„ì–´ìˆìŒ")
            continue

        if not source_url or 'dummy' in source_url.lower():
            errors.append(f"Item {i}: source_url ì—†ê±°ë‚˜ dummy URL")
            continue

        # OFFICIAL ë„ë©”ì¸ ê²€ì¦ (Geminië§Œ - ê²½ê³ ë§Œ, ê±°ë¶€ ì•ˆ í•¨)
        if expected_data_type == "official" and ai_name == "Gemini":
            is_official_domain = any(domain in source_url for domain in OFFICIAL_DOMAINS)
            if not is_official_domain:
                # ë‰´ìŠ¤ê°€ ê³µì‹ í™œë™ì„ ë³´ë„í•˜ëŠ” ê²½ìš°ë„ í—ˆìš© (ê²½ê³ ë§Œ)
                errors.append(f"Item {i}: OFFICIALì´ì§€ë§Œ ë¹„ê³µì‹ ë„ë©”ì¸ (í—ˆìš©): {source_url[:60]}")

        # Gemini prohibits X data collection
        if ai_name == "Gemini" and ("twitter.com" in source_url or "x.com" in source_url):
            errors.append(f"Item {i}: Gemini X/Twitter ê¸ˆì§€ - ì œì™¸: {source_url[:60]}")
            continue

        # Date ì •ê·œí™” (ë¹ˆ ë‚ ì§œ, N/A í—ˆìš© â†’ Noneìœ¼ë¡œ ì²˜ë¦¬)
        raw_date = item.get('date') or item.get('data_date') or ''
        if raw_date and raw_date.strip().upper() in ['N/A', 'NA', 'UNKNOWN', 'NONE', '']:
            item['date'] = ''  # ë¹ˆ ë¬¸ìì—´ë¡œ í†µì¼

        valid_items.append(item)

    return valid_items, errors


def collect_data_type(ai_name, client, politician_id, politician_full, category_name, data_type, topic_mode, count, pre_seen_urls=None):
    """Collects data for a specific AI, category, data type, and sentiment.

    ì´ì „ ì„±ê³µ íŒ¨í„´ ë³µì›:
    - ë¼ìš´ë“œë§ˆë‹¤ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  10ê°œ ì„¸ë¶€í•­ëª©ì„ ê°ê° ê°œë³„ API í˜¸ì¶œ
    - ê° í‚¤ì›Œë“œê°€ ë‹¤ë¥¸ ê²€ìƒ‰ ê²°ê³¼ â†’ URL ë‹¤ì–‘ì„± í™•ë³´ â†’ ì¤‘ë³µ ìµœì†Œí™”
    - 429 ì—ëŸ¬ ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
    - ìµœëŒ€ MAX_ROUNDS ë¼ìš´ë“œ ë°˜ë³µ

    Args:
        pre_seen_urls: ì´ì „ sentimentì—ì„œ ìˆ˜ì§‘ëœ URL set (êµì°¨ ì¤‘ë³µ ë°©ì§€)
    """
    # AIë³„ íŒŒë¼ë¯¸í„° ì°¨ë“± ì ìš© (PerplexityëŠ” URL ë¶€ì¡±ìœ¼ë¡œ ë” ë§ì€ ê¸°íšŒ ë¶€ì—¬)
    if ai_name == "Perplexity":
        MAX_ROUNDS = 9       # PerplexityëŠ” ë¼ìš´ë“œ ë” ë§ì´
        MAX_PER_CALL = 10    # í•œë²ˆì— 10ê°œ ìš”ì²­
        MAX_EMPTY_KW = 10    # í‚¤ì›Œë“œ 10ê°œ ì „ë¶€ ì‹œë„ (ì¡°ê¸° ì¢…ë£Œ ë°©ì§€)
        MAX_EMPTY_ROUNDS = 3 # 3ë¼ìš´ë“œ ì—°ì† ë¹ˆ ë¼ìš´ë“œ í—ˆìš©
    else:  # Gemini
        MAX_ROUNDS = 7
        MAX_PER_CALL = 5
        MAX_EMPTY_KW = 7
        MAX_EMPTY_ROUNDS = 2
    MAX_429_RETRIES = 3  # 429 ì—ëŸ¬ ì¬ì‹œë„ íšŸìˆ˜

    print(f"    -> {topic_mode.upper()} ({count}ê°œ) ìˆ˜ì§‘ ì‹œì‘...")
    all_items = []  # ìœ ë‹ˆí¬ ì•„ì´í…œë§Œ ì €ì¥
    # DB ê¸°ì¡´ URLë§Œ í¬í•¨ (sentiment ê°„ ê³µìœ  ì•ˆ í•¨)
    seen_urls = set(pre_seen_urls) if pre_seen_urls else set()

    category_items = CATEGORY_ITEMS.get(category_name, [("", "")])
    year_hint = "(2025, 2024)"

    # ì •ì²´ ê°ì§€: ë¼ìš´ë“œ ì „ì²´ì—ì„œ ìƒˆ ì•„ì´í…œ 0ê°œë©´ ì¡°ê¸° ì¢…ë£Œ
    consecutive_empty_rounds = 0

    for round_num in range(MAX_ROUNDS):
        unique_count = len(all_items)
        remaining = count - unique_count
        if remaining <= 0:
            break

        if round_num > 0:
            print(f"    [ë¼ìš´ë“œ {round_num+1}] í˜„ì¬ {unique_count}ê°œ / ëª©í‘œ {count}ê°œ")

        round_added_total = 0  # ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ì¶”ê°€ëœ ì´ ìˆ˜

        # ê° ë¼ìš´ë“œì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì•„ì´í…œì„ ìˆœíšŒí•˜ë©° ê°œë³„ API í˜¸ì¶œ
        consecutive_empty_keywords = 0  # ì—°ì† ë¹ˆ í‚¤ì›Œë“œ ìˆ˜
        for kw_idx, (item_name, item_keywords) in enumerate(category_items):
            unique_count = len(all_items)
            remaining = count - unique_count
            if remaining <= 0:
                break

            # ì •ì²´ ê°ì§€: ì—°ì† Nê°œ í‚¤ì›Œë“œì—ì„œ ìƒˆ ì•„ì´í…œ 0ê°œë©´ ì´ë²ˆ ë¼ìš´ë“œ ì¢…ë£Œ
            if consecutive_empty_keywords >= MAX_EMPTY_KW:
                print(f"    âš ï¸ ì—°ì† {consecutive_empty_keywords}ê°œ í‚¤ì›Œë“œ ê²°ê³¼ ì—†ìŒ â†’ ë¼ìš´ë“œ ì¡°ê¸° ì¢…ë£Œ")
                break

            request_count = min(remaining, MAX_PER_CALL)

            # 429 ì—ëŸ¬ ì¬ì‹œë„ ë£¨í”„
            for retry in range(MAX_429_RETRIES):
                try:
                    # Gemini ë„ë©”ì¸ ìˆœí™˜: í‚¤ì›Œë“œ ì¸ë±ìŠ¤ì— ë”°ë¼ ë‹¤ë¥¸ ë„ë©”ì¸ ê²€ìƒ‰
                    domain_hint = ""
                    if ai_name == "Gemini":
                        if data_type == "official":
                            hints = GEMINI_OFFICIAL_DOMAIN_HINTS
                        else:
                            hints = GEMINI_PUBLIC_PLATFORM_HINTS
                        domain_hint = hints[kw_idx % len(hints)]

                    prompt = build_search_prompt(
                        ai_name, data_type, topic_mode, politician_full,
                        item_keywords, request_count, year_hint, item_name,
                        exclude_urls=seen_urls,  # ëª¨ë“  AIì— exclude_urls ì „ë‹¬
                        domain_hint=domain_hint
                    )

                    if not prompt:
                        break

                    # AI Call
                    response_text = call_ai(ai_name, client, prompt, data_type)

                    if not response_text:
                        break  # ì´ í‚¤ì›Œë“œ ìŠ¤í‚µ, ë‹¤ìŒ í‚¤ì›Œë“œë¡œ

                    # JSON Parsing
                    json_match = re.search(r'```json\s*([\s\S]+?)\s*```', response_text)
                    if json_match:
                        response_json_str = json_match.group(1)
                    else:
                        response_json_str = response_text

                    data = json.loads(response_json_str)

                    # Data validation (per-item filtering)
                    valid_data, errors = validate_collected_data(data, data_type, ai_name)

                    if not valid_data:
                        break  # ì´ í‚¤ì›Œë“œ ìŠ¤í‚µ

                    # ë©”ëª¨ë¦¬ ë‚´ ì¤‘ë³µ ì œê±° + ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    added = 0
                    for item in valid_data:
                        if len(all_items) >= count:
                            break

                        # URL ì •ê·œí™”ë¡œ ì¤‘ë³µ ì²´í¬
                        url = extract_url(item)
                        url_normalized = normalize_url(url) if url else ''

                        if url_normalized and url_normalized in seen_urls:
                            continue  # ì´ë¯¸ ìˆ˜ì§‘ëœ URL â†’ ìŠ¤í‚µ

                        # ìœ ë‹ˆí¬ ì•„ì´í…œ ì¶”ê°€
                        item['sentiment'] = topic_mode
                        item['data_type'] = data_type
                        item['collector_ai'] = ai_name
                        all_items.append(item)
                        if url_normalized:
                            seen_urls.add(url_normalized)
                        added += 1

                    if added > 0:
                        # í‚¤ì›Œë“œëª… 5ê¸€ìê¹Œì§€ë§Œ í‘œì‹œ
                        short_name = item_name[:5] if len(item_name) > 5 else item_name
                        print(f"      [{short_name}] +{added}ê°œ â†’ ëˆ„ì  {len(all_items)}ê°œ")
                        consecutive_empty_keywords = 0  # ë¦¬ì…‹
                        round_added_total += added
                    else:
                        consecutive_empty_keywords += 1

                    break  # ì„±ê³µ ì‹œ retry ë£¨í”„ íƒˆì¶œ

                except json.JSONDecodeError as e:
                    print(f"    [{ai_name}] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                    break  # JSON ì—ëŸ¬ëŠ” ì¬ì‹œë„ ì˜ë¯¸ ì—†ìŒ

                except Exception as e:
                    error_str = str(e)
                    if '429' in error_str or 'RESOURCE_EXHAUSTED' in error_str:
                        wait_time = 2 ** (retry + 1)  # 2, 4, 8ì´ˆ
                        print(f"  âŒ {ai_name} API ì—ëŸ¬: 429 RESOURCE_EXHAUSTED. {error_str[:100]}")
                        time.sleep(wait_time)
                        continue  # ì¬ì‹œë„
                    else:
                        print(f"  âŒ {ai_name} API ì—ëŸ¬: {e}")
                        break  # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¬ì‹œë„ ì•ˆ í•¨

            # í‚¤ì›Œë“œ ê°„ ê°„ê²© (rate limit ë°©ì§€)
            if len(all_items) < count:
                time.sleep(0.5)

        # ë¼ìš´ë“œ ì¢…ë£Œ í›„ ì •ì²´ ê°ì§€
        if round_added_total == 0:
            consecutive_empty_rounds += 1
            if consecutive_empty_rounds >= MAX_EMPTY_ROUNDS:
                print(f"    âš ï¸ {consecutive_empty_rounds}ë¼ìš´ë“œ ì—°ì† ìƒˆ ì•„ì´í…œ ì—†ìŒ â†’ ìˆ˜ì§‘ ì¡°ê¸° ì¢…ë£Œ")
                break
        else:
            consecutive_empty_rounds = 0

    print(f"    âœ… {topic_mode.upper()} {len(all_items)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ëª©í‘œ {count}ê°œ)")
    return all_items[:count]


def collect_with_ai(ai_name, politician_id, politician_name, category_idx, category_name, category_korean, test_mode=False, data_type=None):
    """Collects category data with a specific AI (refactored version)

    Args:
        test_mode: True for mini test mode (collects 10 items)
        data_type: "official" or "public" (added)
    """
    mode_label = "[Mini Test]" if test_mode else ""
    print(f"\n{'='*60}")
    print(f"{mode_label}[{ai_name}] {category_korean} ({category_name}) Collection Start (Type: {data_type})")
    print(f"{ '='*60}")

    # [OK] V30 120% Limit Rule: AIë³„ ë…ë¦½ ì œí•œ (Gemini/Perplexity ê°ê° ë”°ë¡œ)
    # Gemini ëª©í‘œ 30 * 1.2 = 36, Perplexity ëª©í‘œ 20 * 1.2 = 24
    AI_MAX = {"Gemini": 36, "Perplexity": 24}
    AI_TARGET = {"Gemini": 30, "Perplexity": 20}
    max_for_ai = AI_MAX.get(ai_name, 50)
    target_for_ai = AI_TARGET.get(ai_name, 45)

    existing_ai_total = get_exact_count(TABLE_COLLECTED_DATA, {
        'politician_id': politician_id,
        'category': category_name,
        'collector_ai': ai_name
    })

    if existing_ai_total >= max_for_ai:
        print(f"  [STOP] {ai_name} 110% limit reached: {existing_ai_total} items (Max {max_for_ai} items)")
        print(f"  [INFO] Skipping collection")
        return 0

    if existing_ai_total >= target_for_ai:
        remaining_allowed = max_for_ai - existing_ai_total
        print(f"  [WARN] {ai_name} target reached: {existing_ai_total} items / {target_for_ai} items")
        print(f"  [INFO] Can collect up to {remaining_allowed} more items")

    # Retrieve detailed politician information (for distinguishing identical names)
    pol_info = get_politician_info(politician_id)
    politician_full = pol_info['search_string'] if pol_info['search_string'] else politician_name

    # Use TEST_SENTIMENT_DISTRIBUTION if in test mode
    sentiment_dist_for_ai = TEST_SENTIMENT_DISTRIBUTION[ai_name] if test_mode else SENTIMENT_DISTRIBUTION[ai_name]
    client = init_ai_client(ai_name)

    all_items = []

    # DB ê¸°ì¡´ URL ë¯¸ë¦¬ ë¡œë“œ â†’ ë©”ëª¨ë¦¬ ì¤‘ë³µ ì œê±°ì— í™œìš© (DB ì €ì¥ ì‹œ íƒˆë½ ë°©ì§€)
    cross_sentiment_urls = set()
    try:
        existing_result = (
            supabase.table(TABLE_COLLECTED_DATA)
            .select('source_url')
            .eq('politician_id', politician_id)
            .eq('collector_ai', ai_name)
            .eq('category', category_name)
            .eq('data_type', data_type)
            .execute()
        )
        if existing_result.data:
            for row in existing_result.data:
                url = row.get('source_url', '')
                if url:
                    normalized = normalize_url(url)
                    if normalized:
                        cross_sentiment_urls.add(normalized)
            if cross_sentiment_urls:
                print(f"    ğŸ“Œ DB ê¸°ì¡´ URL {len(cross_sentiment_urls)}ê°œ ë¡œë“œ â†’ ì¤‘ë³µ ë°©ì§€")
    except Exception as e:
        print(f"    [WARN] DB URL ë¡œë“œ ì‹¤íŒ¨: {e}")

    # Get sentiment distribution corresponding to data_type
    sentiment_dist_for_type = sentiment_dist_for_ai.get(data_type, {}) # Return empty dictionary if data_type not found

    if sentiment_dist_for_type and (sentiment_dist_for_type.get("negative", 0) + sentiment_dist_for_type.get("positive", 0) + sentiment_dist_for_type.get("free", 0)) > 0:

        # ê°ì •ë³„ ìˆ˜ì§‘ (ê¸°ì¡´ ë°ì´í„° í™•ì¸ í›„ ì¶”ê°€ ìˆ˜ì§‘ë¶„ë§Œ ìš”ì²­)
        for sentiment_key, emoji, label in [
            ("negative", "ğŸš¨", "ë¶€ì •"),
            ("positive", "âœ¨", "ê¸ì •"),
            ("free", "ğŸ²", "ììœ ")
        ]:
            target_count = sentiment_dist_for_type.get(sentiment_key, 0)
            if target_count <= 0:
                continue

            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_count = get_exact_count(TABLE_COLLECTED_DATA, {
                'politician_id': politician_id,
                'category': category_name,
                'data_type': data_type,
                'collector_ai': ai_name,
                'sentiment': sentiment_key
            })

            actual_need = max(0, target_count - existing_count)

            print(f"    {emoji} {label} {target_count}ê°œ...")
            print(f"    ğŸ“Š ê¸°ì¡´ {existing_count}ê°œ / ëª©í‘œ {target_count}ê°œ â†’ {actual_need}ê°œ ì¶”ê°€ ìˆ˜ì§‘")

            if actual_need <= 0:
                print(f"    âœ… {label} ì´ë¯¸ ëª©í‘œ ë‹¬ì„±")
                continue

            collected = collect_data_type(
                ai_name, client, politician_id, politician_full, category_name, data_type, sentiment_key, actual_need,
                pre_seen_urls=cross_sentiment_urls  # DB ê¸°ì¡´ URLë§Œ ì „ë‹¬ (sentiment ê°„ ê³µìœ  ì•ˆ í•¨)
            )
            # NOTE: sentiment ê°„ URL ëˆ„ì  ì œê±° (FREE ìˆ˜ëŸ‰ ë³´í˜¸)
            # DB ì¤‘ë³µ ì²´í¬ì—ì„œ êµì°¨ ì¤‘ë³µ ì²˜ë¦¬
            all_items.extend(collected)
            print(f"    âœ… {label} {len(collected)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        print(f"  [INFO] Skipping {ai_name}'s {data_type.upper()} data collection as target is 0.")

    # Save to DB
    saved_count = 0
    skipped_count = 0  # Duplicate skip count

    for item in all_items:
        try:
            # Support various field names (AI responses may vary)
            title = item.get('data_title') or item.get('title') or item.get('item') or ''
            content = item.get('data_content') or item.get('description') or item.get('content') or item.get('item') or ''
            source_url = item.get('source_url') or item.get('url') or item.get('link') or ''
            # URL type check (use first element if list)
            if isinstance(source_url, list):
                source_url = source_url[0] if source_url else ''
            elif not isinstance(source_url, str):
                source_url = str(source_url) if source_url else ''
            source_name = item.get('data_source') or item.get('source') or item.get('source_type') or ''
            raw_date = item.get('data_date') or item.get('date') or item.get('published_date')
            pub_date = normalize_date(raw_date)  # Normalize date format
            sentiment = item.get('sentiment') or 'free'  # sentiment added by collect_data_type()
            # [OK] Keep 'free' value as is (allowed in DB)

            # Use content beginning if title is missing
            if not title and content:
                title = content[:50]

            collector = item.get('collector_ai', ai_name)

            # [OK] V30 Advanced Duplicate Removal: URL Normalization + Title Normalization
            # URL Normalization (remove parameters, anchors)
            source_url_normalized = normalize_url(source_url) if source_url else ''

            # Title Normalization (remove special characters, spaces)
            title_normalized = normalize_title(title) if title else ''

            # Duplicate check: Check by URL or Title
            is_dup = False

            if source_url_normalized or title_normalized:
                try:
                    # ê°™ì€ AI + ì¹´í…Œê³ ë¦¬ ë‚´ ì „ì²´ì—ì„œ ì¤‘ë³µ ì²´í¬ (sentiment ë¬´ê´€)
                    existing_result = (
                        supabase.table(TABLE_COLLECTED_DATA)
                        .select('source_url, title')
                        .eq('politician_id', politician_id)
                        .eq('collector_ai', collector)
                        .eq('category', category_name)
                        .execute()
                    )

                    if existing_result.data:
                        for existing_item in existing_result.data:
                            # URL Duplicate Check
                            if source_url_normalized:
                                existing_url = normalize_url(existing_item.get('source_url', ''))
                                if existing_url and is_duplicate_by_url(source_url, existing_item.get('source_url', '')):
                                    is_dup = True
                                    break

                            # Title Duplicate Check (80% ìœ ì‚¬ë„)
                            if title_normalized and not is_dup:
                                existing_title = existing_item.get('title', '')
                                if is_duplicate_by_title(title, existing_title, threshold=0.80):
                                    is_dup = True
                                    break

                    if is_dup:
                        # Duplicate found - skip
                        skipped_count += 1
                        continue

                except Exception as e:
                    # If duplicate check fails, continue trying to save
                    print(f"  [WARN] Duplicate check error: {e}")
                    pass

            # summary: contentì˜ 30% ìš”ì•½ (ê¸€ì ìˆ˜ ê¸°ì¤€)
            content_str = str(content)[:2000]
            summary_len = max(30, int(len(content_str) * 0.3))
            summary = content_str[:summary_len]

            record = {
                'politician_id': politician_id,
                'politician_name': politician_name,
                'category': category_name.lower(),
                'data_type': item.get('data_type', 'public'),
                'collector_ai': collector,
                'title': str(title)[:200],
                'content': content_str,
                'summary': summary,
                'source_url': source_url,  # ì›ë³¸ URL ì €ì¥ (ì •ê·œí™”ëŠ” ì¤‘ë³µ ë¹„êµìš©ìœ¼ë¡œë§Œ)
                'source_name': str(source_name),
                'published_date': pub_date,
                'sentiment': sentiment,
                'is_verified': False
            }

            supabase.table(TABLE_COLLECTED_DATA).insert(record).execute()
            saved_count += 1
        except Exception as e:
            print(f"  [WARN] Save failed: {e}")

    print(f"  [SAVE] [{ai_name}] {category_korean}: {saved_count} items saved, {skipped_count} duplicates skipped")
    return saved_count


def collect_all_for_politician(politician_id, politician_name, target_ai=None, target_category=None, parallel=False, test_mode=False):
    """Collect all data for a politician

    Args:
        test_mode: True for mini test mode (collects 10 items)
    """
    mode_str = "[Mini Test]" if test_mode else ""
    
    # ì ˆëŒ€ ê·œì¹™: Gemini 30 (OFFICIAL 20 + PUBLIC 10) + Perplexity 20 (PUBLIC) = 50ê°œ
    if test_mode:
        target_per_cat_gemini = TEST_DISTRIBUTION["Gemini"]["total"]
        target_per_cat_perplexity = TEST_DISTRIBUTION["Perplexity"]["total"]
        total_target_per_cat = target_per_cat_gemini + target_per_cat_perplexity  # 6 + 2 = 8 (+ ì¶”ê°€ë²„í¼ 1 = 9)
    elif target_ai == "Gemini":
        total_target_per_cat = COLLECT_DISTRIBUTION["Gemini"]["total"]  # 54
    elif target_ai == "Perplexity":
        total_target_per_cat = COLLECT_DISTRIBUTION["Perplexity"]["total"]  # 18
    else:  # ì „ì²´ ìˆ˜ì§‘ (ì–‘ìª½ AI)
        total_target_per_cat = COLLECT_DISTRIBUTION["Gemini"]["total"] + COLLECT_DISTRIBUTION["Perplexity"]["total"]  # 54 + 18 = 72 (+ ì¶”ê°€ë²„í¼ 6)


    print(f"\n{'#'*60}")
    print(f"# V30 {mode_str} Collection Start: {politician_name} ({politician_id})")
    print(f"# ê¸°ë³¸ 50ê°œ: Gemini 30 (OFFICIAL 20 + PUBLIC 10) + Perplexity 20 (PUBLIC) | ë²„í¼ í¬í•¨ ìµœëŒ€ 60ê°œ")
    print(f"# ê²€ì¦ í›„: 60ê°œ+ â†’ íŒ¨ìŠ¤ | 60ê°œ ë¯¸ë§Œ â†’ ì¶”ê°€ ìˆ˜ì§‘")
    print(f"{'#'*60}")

    # Collection AI List
    collect_ais = ["Gemini", "Perplexity"] # Include both Gemini, Perplexity
    if target_ai:
        collect_ais = [target_ai]

    # Category List
    categories = CATEGORIES
    if target_category:
        categories = [CATEGORIES[target_category - 1]]

    total_saved = 0
    start_time = time.time()

    # If parallel or test mode, run sequentially (ThreadPoolExecutor used in internal collect_data_type)
    if parallel or test_mode:
        print(f"\n[Sequential Run] (Total {len(collect_ais)} AIs x {len(categories)} Categories)")

        failed_tasks = []

        for ai_name in collect_ais:
            print(f"\n{'='*60}")
            print(f"[{ai_name}] Collection Start")
            print(f"{ '='*60}")

            # For each AI, iterate through official and public data types
            for cat_idx, (cat_name, cat_korean) in enumerate(categories):
                for data_type_key in ["official", "public"]:
                    # Check if the data_type is relevant for the current mode (test or normal)
                    if (test_mode and TEST_DISTRIBUTION[ai_name].get(data_type_key, 0) > 0) or \
                       (not test_mode and COLLECT_DISTRIBUTION[ai_name].get(data_type_key, 0) > 0): # Only try to collect if target > 0
                        print(f"  -> {data_type_key.upper()} Data Collection Start for {ai_name} - {cat_korean}")
                        try:
                            count = collect_with_ai(
                                ai_name, politician_id, politician_name,
                                cat_idx,
                                cat_name,
                                cat_korean,
                                test_mode,
                                data_type=data_type_key # Pass data_type explicitly
                            )
                            total_saved += count
                        except Exception as e:
                            print(f"  [FAIL] [{ai_name}] {cat_korean} ({data_type_key}) 1st attempt failed: {e}")
                            failed_tasks.append({
                                'ai_name': ai_name,
                                'cat_idx': cat_idx,
                                'cat_name': cat_name,
                                'cat_korean': cat_korean,
                                'data_type': data_type_key
                            })
            print(f"\n[OK] [{ai_name}] Collection Complete")

            # Interval between AIs
            if ai_name != collect_ais[-1]:
                time.sleep(2)

        # ============================================================ 
        # 2nd Attempt: Retry failed tasks sequentially (safe)
        # ============================================================ 
        if failed_tasks:
            print(f"\n{'='*60}")
            print(f"Retrying failed tasks: {len(failed_tasks)} items")
            print(f"{ '='*60}")

            for attempt in range(1, 4):  # Max 3 retries
                if not failed_tasks:
                    break

                print(f"\n[Retry {attempt}/3] {len(failed_tasks)} tasks")
                retry_success = []

                for task in failed_tasks:
                    try:
                        # Exponential backoff
                        backoff_time = 2 ** (attempt - 1)
                        if backoff_time > 1:
                            time.sleep(backoff_time)

                        count = collect_with_ai(
                            task['ai_name'], politician_id, politician_name,
                            task['cat_idx'],
                            task['cat_name'],
                            task['cat_korean'],
                            test_mode,
                            data_type=task['data_type']
                        )
                        total_saved += count
                        retry_success.append(task)
                        print(f"  [OK] [{task['ai_name']}] {task['cat_korean']} ({task['data_type']}) Retry successful (+{count} items)")
                    except Exception as e:
                        print(f"  [WARN] [{task['ai_name']}] {task['cat_korean']} ({task['data_type']}) Retry {attempt} failed: {e}")

                # Remove successful tasks
                failed_tasks = [t for t in failed_tasks if t not in retry_success]

            # ============================================================ 
            # 3rd Attempt: Final failed task logging
            # ============================================================ 
            if failed_tasks:
                print(f"\n{'='*60}")
                print(f"[FAIL] Final Failed Tasks: {len(failed_tasks)} items")
                print(f"{ '='*60}")
                for task in failed_tasks:
                    print(f"  - [{task['ai_name']}] {task['cat_korean']} ({task['cat_name']}) [{task['data_type']}]")
                print(f"\n[WARN] Some category collections failed. Recallection may be needed.")
    else:
        # Sequential Collection
        for cat_idx, (cat_name, cat_korean) in enumerate(categories):
            for ai_name in collect_ais:
                for data_type_key in ["official", "public"]:
                    if (test_mode and TEST_DISTRIBUTION[ai_name].get(data_type_key, 0) > 0) or \
                       (not test_mode and COLLECT_DISTRIBUTION[ai_name].get(data_type_key, 0) > 0):
                        count = collect_with_ai(
                            ai_name, politician_id, politician_name,
                            cat_idx, cat_name, cat_korean, test_mode,
                            data_type=data_type_key
                        )
                        total_saved += count
                        time.sleep(1)  # API Rate Limit prevention

    elapsed = time.time() - start_time

    print(f"\n{'='*60}")
    print(f"[OK] V30 {mode_str}Collection Complete: {politician_name}")
    print(f"   Total Saved: {total_saved} items")
    print(f"   Time Taken: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
    print(f"{ '='*60}")

    # Output verification results if in test mode
    if test_mode:
        verify_test_results(politician_id, politician_name, categories)

    # # Automatic validation trigger (Option 1: Main agent approach)
    # if not args.skip_validation:
    #     print(f"\n{'='*60}")
    #     print("[SEARCH] Starting automatic validation...")
    #     print("Validating collected data.")
    #     print(f"{ '='*60}\n")
    # 
    #     try:
    #         # Import run_validation_pipeline function from validate_v30.py
    #         import sys
    #         import os
    #         script_dir = os.path.dirname(os.path.abspath(__file__))
    #         sys.path.insert(0, script_dir)
    # 
    #         from validate_v30 import run_validation_pipeline
    # 
    #         # Run validation
    #         result = run_validation_pipeline(
    #             politician_id=args.politician_id,
    #             politician_name=args.politician_name,
    #             mode='all',
    #             ai_name=args.ai
    #         )
    # 
    #         print(f"\n{'='*60}")
    #         print("[OK] Automatic validation complete")
    #         print(f"   - Valid: {result.get('valid', 0)} items")
    #         print(f"   - Invalid: {result.get('invalid', 0)} items")
    #         print(f"{ '='*60}\n")
    # 
    #     except Exception as e:
    #         print(f"\n[WARN] Automatic validation failed: {e}")
    #         print("Please run the validation script manually:")
    #         print(f"python validate_v30.py --politician_id={args.politician_id} --politician_name=\"{args.politician_name}\"\n")
    # 
    return total_saved

def verify_test_results(politician_id, politician_name, categories):
    """Test result verification"""
    print(f"\n{'='*60}")
    print(f"[GRAPH] Test Result Verification: {politician_name}")
    print(f"{ '='*60}")

    total_by_ai = {"Gemini": 0, "Perplexity": 0} # Perplexity included now
    total_by_type = {"official": 0, "public": 0}

    for cat_name, cat_korean in categories:
        # Retrieve count per AI
        for ai_name in ["Gemini", "Perplexity"]: # Perplexity included now
            count = get_exact_count(TABLE_COLLECTED_DATA, {
                'politician_id': politician_id,
                'category': cat_name,
                'collector_ai': ai_name
            })
            total_by_ai[ai_name] += count
        
        # Retrieve count per data type
        for dtype in ["official", "public"]:
            count = get_exact_count(TABLE_COLLECTED_DATA, {
                'politician_id': politician_id,
                'category': cat_name,
                'data_type': dtype
            })
            total_by_type[dtype] += count

    # Output results
    print(f"\n[GRAPH] AI-specific Collection Results:")
    total = sum(total_by_ai.values())
    for ai_name, count in total_by_ai.items():
        pct = (count / total * 100) if total > 0 else 0
        expected_total_per_ai_category = TEST_DISTRIBUTION[ai_name]['total']
        expected = expected_total_per_ai_category * len(categories)
        status = "[OK]" if count >= expected * 0.8 else "[WARN]" # Success if >= 80%
        print(f"   {status} {ai_name}: {count} items ({pct:.1f}%) - Target {expected} items")

    print(f"\n[GRAPH] By Data Type:")
    for dtype, count in total_by_type.items():
        print(f"   - {dtype.upper()}: {count} items")

    print(f"\n[GRAPH] Total: {total} items")

    # Ratio verification
    if total > 0:
        gemini_pct = total_by_ai["Gemini"] / total * 100
        perplexity_pct = total_by_ai["Perplexity"] / total * 100 

        print(f"\n[TARGET] Ratio Verification (Target: Gemini ~60%, Perplexity ~40%):")
        print(f"   Gemini: {gemini_pct:.1f}% {'[OK]' if 50 <= gemini_pct <= 70 else '[WARN]'}") # 60% (30/50)
        print(f"   Perplexity: {perplexity_pct:.1f}% {'[OK]' if 30 <= perplexity_pct <= 50 else '[WARN]'}") # 40% (20/50)

    print(f"{ '='*60}")


def get_all_politicians():
    """Retrieve all politicians from DB"""
    try:
        result = supabase.table('politicians').select('id', 'name').execute()
        return result.data if result.data else []
    except Exception as e:
        print(f"[FAIL] Failed to retrieve politician list: {e}")
        return []


def collect_all_politicians(target_ai=None, target_category=None, parallel=False, test_mode=False):
    """Collect all data for all politicians"""
    politicians = get_all_politicians()

    if not politicians:
        print("[FAIL] No politicians to collect.")
        return

    mode_str = "[Mini Test]" if test_mode else ""
    print(f"\n{'#'*60}")
    print(f"# V30 {mode_str}Bulk Collection Start for All Politicians")
    print(f"# Total {len(politicians)} politicians")
    print(f"{ '#'*60}\n")

    success_count = 0
    fail_count = 0

    for i, p in enumerate(politicians, 1):
        pid = p['id']
        pname = p['name']

        print(f"\n[{i}/{len(politicians)}] Collection Start for {pname} (ID: {pid})...")

        try:
            saved = collect_all_for_politician(
                pid, pname,
                target_ai=target_ai,
                target_category=target_category,
                parallel=parallel,
                test_mode=test_mode
            )
            success_count += 1
            print(f"[OK] {pname}: {saved} items collected")
        except Exception as e:
            fail_count += 1
            print(f"[FAIL] {pname}: Collection failed - {e}")

        # Interval between politicians (to prevent API rate limits)
        if i < len(politicians):
            time.sleep(2 if not test_mode else 1)

    print(f"\n{'#'*60}")
    print(f"# {mode_str}Bulk Collection Complete")
    print(f"# Success: {success_count} politicians, Failed: {fail_count} politicians")
    print(f"{ '#'*60}")


def clear_politician_category_data(politician_id, category_name):
    """Clears all collected data for a specific politician and category."""
    try:
        response = supabase.table(TABLE_COLLECTED_DATA).delete()\
            .eq('politician_id', politician_id)\
            .eq('category', category_name)\
            .execute()
        print(f"  [OK] Cleared {politician_id}'s '{category_name}' data. Count: {response.count}")
        return True
    except Exception as e:
        print(f"  [FAIL] Failed to clear data for {politician_id}, '{category_name}': {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='V30 Web Search Collection (Gemini 90 + Perplexity 30 + Buffer 10 = 130)')
    parser.add_argument('--politician_id', help='Politician ID (cannot be used with --all)')
    parser.add_argument('--politician_name', help='Politician Name (cannot be used with --all)')
    parser.add_argument('--all', action='store_true', help='Collect for all politicians')
    parser.add_argument('--ai', choices=['Gemini', 'Perplexity'], help='Run only a specific AI (Gemini or Perplexity)') # Perplexity added
    parser.add_argument('--category', type=int, choices=range(1, 11), help='Specific category only (1-10)')
    parser.add_argument('--parallel', action='store_true', help='Parallel execution')
    parser.add_argument('--test', action='store_true', help='Mini test mode (10 items per category, auto validation)')
    parser.add_argument('--skip-validation', action='store_true', help='Skip auto validation after collection')
    parser.add_argument('--clear-data', action='store_true', help='Clear existing collected data for the politician/category before collection')

    args = parser.parse_args()

    # Test Mode Guidance
    if args.test:
        print(f"\n{'='*60}")
        print(f"[TEST] Mini Test Mode Activated")
        print(f"   - Per category: Gemini 10 items + Perplexity 3 items (Total 13 items)")
        print(f"   - Gemini, Perplexity sequential/parallel execution")
        print(f"   - Expected duration: 1-2 minutes (1 category)")
        print(f"   - Auto validation after completion")
        print(f"{ '='*60}\n")

    # Collect for all politicians
    if args.all:
        collect_all_politicians(
            target_ai=args.ai,
            target_category=args.category,
            parallel=args.parallel or args.test,
            test_mode=args.test
        )
        return

    # Collect for individual politician (existing method)
    if not args.politician_id or not args.politician_name:
        print("[FAIL] Please specify --politician_id and --politician_name, or use --all.")
        print("")
        print("Usage Example:")
        print("   # Mini Test (1 category, 3-5 minutes)")
        print("   python collect_v30.py --politician_id=xxx --politician_name=\"Hong Gil-dong\" --test --category=1")
        print("")
        print("   # Full Collection (Parallel)")
        print("   python collect_v30.py --politician_id=xxx --politician_name=\"Hong Gil-dong\" --parallel")
        print("")
        print("   # Bulk Collection for All Politicians")
        print("   python collect_v30.py --all --parallel")
        return

    # Check politician
    exists, db_name = check_politician_exists(args.politician_id)
    if not exists:
        print(f"[FAIL] Politician ID '{args.politician_id}' not found in politicians table.")
        print("   Please register the politician first.")
        return

    # Clear data if requested
    if args.clear_data:
        print(f"\n{'='*60}")
        print(f"[CLEAN] Clearing existing data for {args.politician_name} ({args.politician_id}) - Category: {CATEGORIES[args.category - 1][0] if args.category else 'All'}")
        print(f"{ '='*60}")
        if args.category:
            clear_politician_category_data(args.politician_id, CATEGORIES[args.category - 1][0])
        else:
            for cat_name, _ in CATEGORIES:
                clear_politician_category_data(args.politician_id, cat_name)
        print(f"[CLEAN] Data clearing complete.")
        # Exit after clearing if only clearing is requested
        if not (args.ai or args.parallel or args.test):
            return

    # Run collection
    collect_all_for_politician(
        args.politician_id,
        args.politician_name,
        target_ai=args.ai,
        target_category=args.category,
        parallel=args.parallel or args.test,
        test_mode=args.test
    )

    # Automatic validation trigger (Option 1: Main agent approach)
    if not args.skip_validation:
        pass # Disabling auto-validation for now

if __name__ == "__main__":
    main()
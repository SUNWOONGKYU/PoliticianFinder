#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ì¡°ì€í¬ V30 ìˆ˜ì§‘ ë°ì´í„° ë¹ ë¥¸ ê²€ì¦"""

import sys
import os
from supabase import create_client
from collections import Counter
from datetime import datetime, timedelta
from dotenv import load_dotenv

# UTF-8 ì¶œë ¥
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    except AttributeError:
        pass

load_dotenv(override=True)

# Supabase ì—°ê²°
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

politician_id = 'd0a5d6e1'
politician_name = 'ì¡°ì€í¬'

print("="*80)
print(f"ì¡°ì€í¬ ({politician_id}) V30 ìˆ˜ì§‘ ê²€ì¦")
print("="*80)
print()

# ì „ì²´ ë°ì´í„° ì¡°íšŒ
result = supabase.table('collected_data_v30')\
    .select('*')\
    .eq('politician_id', politician_id)\
    .execute()

data = result.data
total = len(data)

print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {total}ê°œ")
print()

if total == 0:
    print("âŒ ë°ì´í„° ì—†ìŒ!")
    sys.exit(1)

# 1. AIë³„ ë¶„í¬
print("="*80)
print("1ï¸âƒ£ AIë³„ ë¶„í¬ (ëª©í‘œ: Gemini 90% + Grok 10%)")
print("="*80)
ai_counts = Counter([d['collector_ai'] for d in data])
for ai in sorted(ai_counts.keys()):
    count = ai_counts[ai]
    pct = (count / total) * 100
    print(f"  {ai:12s}: {count:3d}ê°œ ({pct:5.1f}%)")
print()

# 2. ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
print("="*80)
print("2ï¸âƒ£ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (ëª©í‘œ: ê° 100ê°œ)")
print("="*80)
cat_counts = Counter([d['category'] for d in data])
for i in range(1, 11):
    cat = f"cat{i:02d}"
    count = cat_counts.get(cat, 0)
    status = "âœ…" if count >= 90 else "âš ï¸" if count >= 80 else "âŒ"
    print(f"  {status} {cat}: {count:3d}ê°œ")
print()

# 3. source_type ë¶„í¬
print("="*80)
print("3ï¸âƒ£ source_type ë¶„í¬ (ëª©í‘œ: OFFICIAL 50% + PUBLIC 50%)")
print("="*80)
type_counts = Counter([d['source_type'] for d in data])
for st in sorted(type_counts.keys()):
    count = type_counts[st]
    pct = (count / total) * 100
    print(f"  {st:8s}: {count:3d}ê°œ ({pct:5.1f}%)")
print()

# 4. í•„ìˆ˜ í•„ë“œ ì²´í¬
print("="*80)
print("4ï¸âƒ£ í•„ìˆ˜ í•„ë“œ ê²€ì¦")
print("="*80)
missing_title = sum(1 for d in data if not d.get('title'))
missing_content = sum(1 for d in data if not d.get('content'))
missing_url = sum(1 for d in data if not d.get('source_url'))
missing_date = sum(1 for d in data if not d.get('published_date'))

print(f"  title ëˆ„ë½: {missing_title}ê°œ {'âœ…' if missing_title == 0 else 'âŒ'}")
print(f"  content ëˆ„ë½: {missing_content}ê°œ {'âœ…' if missing_content == 0 else 'âŒ'}")
print(f"  source_url ëˆ„ë½: {missing_url}ê°œ {'âœ…' if missing_url == 0 else 'âŒ'}")
print(f"  published_date ëˆ„ë½: {missing_date}ê°œ {'âœ…' if missing_date == 0 else 'âŒ'}")
print()

# 5. ê¸°ê°„ ì œí•œ ê²€ì¦
print("="*80)
print("5ï¸âƒ£ ê¸°ê°„ ì œí•œ ê²€ì¦ (OFFICIAL 4ë…„, PUBLIC 2ë…„)")
print("="*80)
now = datetime.now()
official_limit = now - timedelta(days=365*4)
public_limit = now - timedelta(days=365*2)

out_of_range = 0
for d in data:
    if not d.get('published_date'):
        continue
    try:
        pub_date = datetime.fromisoformat(d['published_date'].replace('Z', '+00:00'))
        if d['source_type'] == 'OFFICIAL' and pub_date < official_limit:
            out_of_range += 1
        elif d['source_type'] == 'PUBLIC' and pub_date < public_limit:
            out_of_range += 1
    except:
        pass

print(f"  ê¸°ê°„ ì´ˆê³¼: {out_of_range}ê°œ {'âœ…' if out_of_range == 0 else 'âŒ'}")
print()

# 6. Grok = Xë§Œ? ê²€ì¦
print("="*80)
print("6ï¸âƒ£ Grok ìˆ˜ì§‘ ê²€ì¦ (X/íŠ¸ìœ„í„°ë§Œ ìˆ˜ì§‘í•´ì•¼ í•¨)")
print("="*80)
grok_data = [d for d in data if d['collector_ai'] == 'Grok']
grok_non_x = [d for d in grok_data if 'twitter.com' not in d.get('source_url', '') and 'x.com' not in d.get('source_url', '')]
print(f"  Grok ì´: {len(grok_data)}ê°œ")
print(f"  Grok X ì•„ë‹˜: {len(grok_non_x)}ê°œ {'âœ…' if len(grok_non_x) == 0 else 'âŒ'}")
if grok_non_x:
    print(f"  âš ï¸ Grokì´ X ì™¸ ì¶œì²˜ ìˆ˜ì§‘:")
    for d in grok_non_x[:3]:
        print(f"    - {d['source_url'][:80]}")
print()

# 7. ì¤‘ë³µ ê²€ì‚¬
print("="*80)
print("7ï¸âƒ£ ì¤‘ë³µ ê²€ì‚¬ (ê°™ì€ AI + ê°™ì€ URL)")
print("="*80)
seen = set()
duplicates = 0
for d in data:
    key = (d['collector_ai'], d.get('source_url', ''))
    if key in seen:
        duplicates += 1
    seen.add(key)
print(f"  ì¤‘ë³µ: {duplicates}ê°œ {'âœ…' if duplicates == 0 else 'âŒ'}")
print()

# ìµœì¢… íŒì •
print("="*80)
print("âœ… ê²€ì¦ ì™„ë£Œ")
print("="*80)
print()

# ìš”ì•½
issues = []
if ai_counts.get('Gemini', 0) < total * 0.85:
    issues.append("âš ï¸ Gemini ë¹„ìœ¨ ë¶€ì¡± (ëª©í‘œ 90%)")
if ai_counts.get('Grok', 0) < total * 0.05:
    issues.append("âš ï¸ Grok ë¹„ìœ¨ ë¶€ì¡± (ëª©í‘œ 10%)")
if missing_title or missing_content or missing_url:
    issues.append("âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
if out_of_range > 0:
    issues.append("âŒ ê¸°ê°„ ì œí•œ ìœ„ë°˜")
if len(grok_non_x) > 0:
    issues.append("âŒ Grokì´ X ì™¸ ìˆ˜ì§‘")
if duplicates > 0:
    issues.append("âš ï¸ ì¤‘ë³µ ë°ì´í„° ì¡´ì¬")

if issues:
    print("ğŸš¨ ë°œê²¬ëœ ë¬¸ì œ:")
    for issue in issues:
        print(f"  {issue}")
else:
    print("âœ… ëª¨ë“  ê²€ì¦ í†µê³¼!")

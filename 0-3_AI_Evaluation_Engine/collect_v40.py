# -*- coding: utf-8 -*-
#
# V40 Data Collection Script (50-50 Gemini+Naver ë°°ë¶„ í™•ì • - 2026-02-01)
#
# ============================================================
# ğŸš¨ ì ˆëŒ€ ê·œì¹™ (50-50 ë°°ë¶„, ì¹´í…Œê³ ë¦¬ë‹¹ 100ê°œ+20%ë²„í¼)
# ============================================================
# | êµ¬ë¶„            | ê¸°ë³¸ | ìµœëŒ€(120%) | ì—­í•                           |
# |-----------------|------|------------|-------------------------------|
# | Gemini OFFICIAL | 30ê°œ | 36ê°œ       | êµ­íšŒ, ì •ë¶€, ì§€ë°©ì •ë¶€, ê³µê³µê¸°ê´€ |
# | Gemini PUBLIC   | 20ê°œ | 24ê°œ       | ëª¨ë“  PUBLIC (ë‰´ìŠ¤, ë¸”ë¡œê·¸ ë“±)  |
# | Naver OFFICIAL  | 10ê°œ | 12ê°œ       | ì •ë¶€/ê³µê³µê¸°ê´€ (.go.kr)         |
# | Naver PUBLIC    | 40ê°œ | 48ê°œ       | ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì¹´í˜, ì»¤ë®¤ë‹ˆí‹°   |
# | ì´ê³„            | 100ê°œ| 120ê°œ      |                               |
# ============================================================
#
# ê²€ì¦ í›„ ì²˜ë¦¬:
#   - 100ê°œ ì´ìƒ â†’ íŒ¨ìŠ¤ âœ…
#   - 100ê°œ ë¯¸ë§Œ â†’ ì¶”ê°€ ìˆ˜ì§‘ ğŸ”„ (ë²„í¼ ë²”ìœ„ ë‚´ ìµœëŒ€ 120ê°œ)
#
# ì—­í•  ë¶„ë‹´:
#   - Gemini: OFFICIAL (ì •ë¶€/ê³µê³µ) + PUBLIC (ëª¨ë“  ì†ŒìŠ¤, ì†ŒìŠ¤ ì œí•œ ì—†ìŒ)
#   - Naver: OFFICIAL (.go.kr ë„ë©”ì¸) + PUBLIC (ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì¹´í˜ ë“±)
#
# V40 í•µì‹¬ ë³€ê²½:
#   - Perplexity â†’ Naver Search APIë¡œ êµì²´
#   - Gemini PUBLIC ë‰´ìŠ¤ ì°¨ë‹¨ ì™„ì „ ì œê±° (ì†ŒìŠ¤ ì œí•œ ì—†ìŒ)
#   - OFFICIAL 10-10-80 / PUBLIC 20-20-60
#
# Usage:
#     # Full Collection (Gemini + Naver)
#     python collect_v40.py --politician_id=d0a5d6e1 --politician_name="ì¡°ì€í¬"
#
#     # Run specific AI only
#     python collect_v40.py --politician_id=d0a5d6e1 --politician_name="ì¡°ì€í¬" --ai=Gemini
#     python collect_v40.py --politician_id=d0a5d6e1 --politician_name="ì¡°ì€í¬" --ai=Naver
#
#     # Specific Category only
#     python collect_v40.py --politician_id=d0a5d6e1 --politician_name="ì¡°ì€í¬" --category=1
#
#     # Mini Test (20 items per category)
#     python collect_v40.py --politician_id=d0a5d6e1 --politician_name="ì¡°ì€í¬" --test
#

import os
import sys
import json
import re
from duplicate_check_utils import normalize_url, normalize_title, is_duplicate_by_url, is_duplicate_by_title
import argparse
import time
import random
import requests
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from supabase import create_client
from dotenv import load_dotenv
import uuid
from html.parser import HTMLParser
from urllib.parse import urlparse, urlencode

# UTF-8 Output Setting
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except AttributeError:
        pass

# ============================================================
# HTML Tag Stripper
# ============================================================
class HTMLStripper(HTMLParser):
    """HTML íƒœê·¸ ì œê±°"""
    def __init__(self):
        super().__init__()
        self.reset()
        self.strict = False
        self.convert_charrefs = True
        self.text = []

    def handle_data(self, d):
        self.text.append(d)

    def get_data(self):
        return ''.join(self.text)

def strip_html_tags(html_text):
    """HTML íƒœê·¸ ì œê±° í›„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë°˜í™˜"""
    if not html_text:
        return ""
    s = HTMLStripper()
    s.feed(html_text)
    return s.get_data()

# ============================================================
# URL ê²€ì¦ í•¨ìˆ˜ (V30 ë™ì¼)
# ============================================================

def validate_url(url: str, timeout: float = 5.0) -> bool:
    """URL ê²€ì¦ (GET stream=True + User-Agent) - 90%+ ì„±ê³µë¥ """
    if not url or 'dummy' in url.lower():
        return False

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    try:
        response = requests.get(url, timeout=timeout, allow_redirects=True,
                               headers=headers, stream=True)
        response.close()
        return response.status_code < 400
    except:
        return False


def resolve_redirect_url(redirect_url: str, timeout: float = 10.0) -> str:
    """Gemini redirect URLì„ ì‹¤ì œ URLë¡œ ë³€í™˜"""
    if not redirect_url or 'grounding-api-redirect' not in redirect_url:
        return redirect_url

    try:
        response = requests.head(redirect_url, timeout=timeout, allow_redirects=False)
        if response.status_code in [301, 302, 303, 307, 308] and 'Location' in response.headers:
            return response.headers['Location']
    except:
        pass

    return redirect_url


def extract_json_from_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ë°°ì—´ì„ ì¶”ì¶œ (robust)"""
    if not text:
        return "[]"

    # ë°©ë²• 1: ```json ë¸”ë¡ ì¶”ì¶œ
    if '```json' in text:
        match = re.search(r'```json\s*([\s\S]*?)\s*```', text)
        if match:
            return match.group(1).strip()

    # ë°©ë²• 2: ``` ë¸”ë¡ ì¶”ì¶œ
    if '```' in text:
        match = re.search(r'```\s*([\s\S]*?)\s*```', text)
        if match:
            content = match.group(1).strip()
            if content.startswith('['):
                return content

    # ë°©ë²• 3: [ ... ] ë°°ì—´ ì§ì ‘ ì¶”ì¶œ
    match = re.search(r'\[\s*\{[\s\S]*\}\s*\]', text)
    if match:
        return match.group(0)

    # ë°©ë²• 4: í…ìŠ¤íŠ¸ ìì²´ê°€ JSONì¼ ê²½ìš°
    stripped = text.strip()
    if stripped.startswith('[') and stripped.endswith(']'):
        return stripped

    # ë°©ë²• 5: ê°œë³„ {} ê°ì²´ ìˆ˜ì§‘í•˜ì—¬ ë°°ì—´ë¡œ ë§Œë“¤ê¸°
    objects = re.findall(r'\{[^{}]+\}', text)
    if objects:
        valid_objects = []
        for obj in objects:
            try:
                parsed = json.loads(obj)
                if isinstance(parsed, dict) and ('data_title' in parsed or 'title' in parsed or 'source_url' in parsed or 'url' in parsed):
                    valid_objects.append(obj)
            except:
                pass
        if valid_objects:
            return '[' + ','.join(valid_objects) + ']'

    return "[]"


# ============================================================
# topic_mode -> DB sentiment mapping
# ============================================================
def topic_mode_to_sentiment(topic_mode):
    """Converts topic_mode to DB sentiment value
    âš ï¸ sentiment ê°’: instructions/2_collect/cat01~10 Section 12 ì°¸ì¡°
    ìœ íš¨ê°’: negative / positive / free
    """
    mapping = {
        'negative': 'negative',
        'positive': 'positive',
        'free': 'free'
    }
    return mapping.get(topic_mode, 'free')


# Load environment variables
load_dotenv(override=True)

# Supabase client
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

# V40 Table Name
TABLE_COLLECTED_DATA = "collected_data_v40"

# AI Client Cache
ai_clients = {}

# Category Definitions (V40)
CATEGORIES = [
    ("expertise", "Expertise"),
    ("leadership", "Leadership"),
    ("vision", "Vision"),
    ("integrity", "Integrity"),
    ("ethics", "Ethics"),
    ("accountability", "Accountability"),
    ("transparency", "Transparency"),
    ("communication", "Communication"),
    ("responsiveness", "Responsiveness"),
    ("publicinterest", "PublicInterest")
]

# ============================================================
# ğŸš¨ V40 ì ˆëŒ€ ê·œì¹™ - ìˆ˜ì§‘ ë°°ë¶„ (50-50, 20% ë²„í¼)
# ============================================================
# | êµ¬ë¶„            | ê¸°ë³¸ | ìµœëŒ€(120%) | ì—­í•                           |
# |-----------------|------|------------|-------------------------------|
# | Gemini OFFICIAL | 30   | 36ê°œ       | êµ­íšŒ, ì •ë¶€, ì§€ë°©ì •ë¶€, ê³µê³µê¸°ê´€ |
# | Gemini PUBLIC   | 20   | 24ê°œ       | ëª¨ë“  PUBLIC (ì†ŒìŠ¤ ì œí•œ ì—†ìŒ)   |
# | Naver OFFICIAL  | 10   | 12ê°œ       | ì •ë¶€/ê³µê³µê¸°ê´€ (.go.kr)         |
# | Naver PUBLIC    | 40   | 48ê°œ       | ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì¹´í˜, ì»¤ë®¤ë‹ˆí‹°   |
# | ì´ê³„            | 100  | 120ê°œ      |                               |
# ============================================================
COLLECT_DISTRIBUTION = {
    "Gemini": {
        "official": 36,  # OFFICIAL - êµ­íšŒ, ì •ë¶€, ì§€ë°©ì •ë¶€ (ìµœì†Œ ëª©í‘œ, ì´ˆê³¼ í—ˆìš©)
        "public": 24,    # PUBLIC - ëª¨ë“  ì†ŒìŠ¤ í—ˆìš©
        "total": 60      # 60ê°œ (ìµœì†Œ ëª©í‘œ + ë²„í¼ 20%)
    },
    "Naver": {
        "official": 12,  # OFFICIAL - .go.kr ë„ë©”ì¸
        "public": 48,    # PUBLIC - ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì¹´í˜ ë“±
        "total": 60      # 60ê°œ (ìµœì†Œ ëª©í‘œ + ë²„í¼ 20%)
    }
    # ì´ ìˆ˜ì§‘: ìµœì†Œ 120ê°œ â†’ ê²€ì¦ â†’ 100ê°œ í™•ë³´ (ì´ˆê³¼ ìˆ˜ì§‘ í—ˆìš©)
}

# ì¶”ê°€ ë²„í¼ ì—†ìŒ (120% ë²„í¼ê°€ COLLECT_DISTRIBUTIONì— ì´ë¯¸ í¬í•¨)
EXTRA_BUFFER = 0

# Mini Test Allocation (1/5 scale) - ì ˆëŒ€ ê·œì¹™ ë¹„ìœ¨ ìœ ì§€
TEST_DISTRIBUTION = {
    "Gemini": {
        "official": 6,   # OFFICIAL (6ê°œ)
        "public": 4,     # PUBLIC (4ê°œ)
        "total": 10      # 10ê°œ
    },
    "Naver": {
        "official": 2,   # OFFICIAL (2ê°œ)
        "public": 8,     # PUBLIC (8ê°œ)
        "total": 10      # 10ê°œ
    }
    # í…ŒìŠ¤íŠ¸ ì´: 20ê°œ
}

# ============================================================
# Sentimentë³„ MIN/MAX ê·œì¹™
# - MIN: ê¸°ë³¸ ëª©í‘œ (ë²„í¼ ì—†ìŒ). ì´ ì´í•˜ë©´ ë°˜ë“œì‹œ ì¶”ê°€ ìˆ˜ì§‘
# - MAX: ë²„í¼ í¬í•¨ ìµœëŒ€ëŸ‰. ì´ ì´ìƒì´ë©´ ì¶”ê°€ ìˆ˜ì§‘ ê¸ˆì§€
# - MIN~MAX ì‚¬ì´: ì¶©ë¶„. ì¶”ê°€ ìˆ˜ì§‘ ë¶ˆí•„ìš”
# OFFICIAL: neg/pos ê° 10% ì´ìƒ
# PUBLIC:   neg/pos ê° 20% ì´ìƒ
# ============================================================
# âš ï¸ sentiment í‚¤: instructions/2_collect/cat01~10 Section 12 ì°¸ì¡°
# ìœ íš¨ê°’: negative / positive / free
SENTIMENT_MIN = {
    "Gemini": {
        "official": {"negative": 3, "positive": 3, "free": 24},    # ê¸°ë³¸ 30ê°œ
        "public": {"negative": 4, "positive": 4, "free": 12}       # ê¸°ë³¸ 20ê°œ
    },
    "Naver": {
        "official": {"negative": 1, "positive": 1, "free": 8},     # ê¸°ë³¸ 10ê°œ
        "public": {"negative": 8, "positive": 8, "free": 24}       # ê¸°ë³¸ 40ê°œ
    }
}
SENTIMENT_MAX = {
    "Gemini": {
        "official": {"negative": 4, "positive": 4, "free": 28},    # ë²„í¼ í¬í•¨ 36ê°œ (MAX)
        "public": {"negative": 5, "positive": 5, "free": 14}       # ë²„í¼ í¬í•¨ 24ê°œ (MAX)
    },
    "Naver": {
        "official": {"negative": 2, "positive": 2, "free": 8},     # ë²„í¼ í¬í•¨ 12ê°œ (MAX)
        "public": {"negative": 10, "positive": 10, "free": 28}     # ë²„í¼ í¬í•¨ 48ê°œ (MAX)
    }
}
# í•˜ìœ„ í˜¸í™˜: ê¸°ì¡´ ì½”ë“œì—ì„œ SENTIMENT_DISTRIBUTION ì°¸ì¡°í•˜ëŠ” ê³³ ëŒ€ì‘
SENTIMENT_DISTRIBUTION = SENTIMENT_MAX

# Test Mode 20-20-60 Allocation (1/5 scale)
TEST_SENTIMENT_DISTRIBUTION = {
    "Gemini": {
        "official": {"negative": 1, "positive": 1, "free": 4},     # 6ê°œ
        "public": {"negative": 1, "positive": 1, "free": 2}        # 4ê°œ
    },
    "Naver": {
        "official": {"negative": 0, "positive": 0, "free": 2},     # 2ê°œ
        "public": {"negative": 2, "positive": 2, "free": 4}        # 8ê°œ
    }
}

# AI Model Configuration
AI_CONFIGS = {
    "Gemini": {
        "model": "gemini-2.0-flash",
        "env_key": "GEMINI_API_KEY"
    },
    "Naver": {
        "env_key_id": "NAVER_CLIENT_ID",
        "env_key_secret": "NAVER_CLIENT_SECRET"
    }
}

# Naver Search API Endpoints
NAVER_SEARCH_ENDPOINTS = {
    'news': 'https://openapi.naver.com/v1/search/news.json',
    'blog': 'https://openapi.naver.com/v1/search/blog.json',
    'cafearticle': 'https://openapi.naver.com/v1/search/cafearticle.json',
    'kin': 'https://openapi.naver.com/v1/search/kin.json',
    'webkr': 'https://openapi.naver.com/v1/search/webkr.json',
    'doc': 'https://openapi.naver.com/v1/search/doc.json',
    'encyc': 'https://openapi.naver.com/v1/search/encyc.json',
}

# Official Data Domains (Gemini OFFICIAL Source)
OFFICIAL_DOMAINS = [
    "assembly.go.kr",
    "likms.assembly.go.kr",
    "mois.go.kr",
    "korea.kr",
    "nec.go.kr",
    "bai.go.kr",
    "pec.go.kr",
    "scourt.go.kr",
    "nesdc.go.kr",
    "manifesto.or.kr",
    "peoplepower21.org",
    "theminjoo.kr",
    "seoul.go.kr",
    "gg.go.kr",
    "busan.go.kr",
    "incheon.go.kr",
    "daegu.go.kr",
    "daejeon.go.kr",
    "gwangju.go.kr",
    "ulsan.go.kr",
    "sejong.go.kr",
    "open.go.kr",
    "acrc.go.kr",
    "humanrights.go.kr"
]

# ============================================================
# Gemini ë„ë©”ì¸ ìˆœí™˜ ë¦¬ìŠ¤íŠ¸ (URL ë‹¤ì–‘ì„± í™•ë³´)
# ============================================================
GEMINI_OFFICIAL_DOMAIN_HINTS = [
    "site:assembly.go.kr",          # 0: êµ­íšŒ
    "site:likms.assembly.go.kr",    # 1: ì˜ì•ˆì •ë³´ì‹œìŠ¤í…œ
    "site:korea.kr",                # 2: ì •ë¶€ ëŒ€í‘œ
    "site:open.go.kr",              # 3: ì •ë³´ê³µê°œ
    "site:seoul.go.kr",             # 4: ì„œìš¸ì‹œ
    "site:manifesto.or.kr OR site:nec.go.kr",  # 5: ì„ ê±°/ê³µì•½
    "site:mois.go.kr OR site:acrc.go.kr",      # 6: í–‰ì•ˆë¶€/ê¶Œìµìœ„
    "site:bai.go.kr OR site:peoplepower21.org", # 7: ê°ì‚¬ì›/ì°¸ì—¬ì—°ëŒ€
    "",                             # 8: ììœ  ê²€ìƒ‰
    "",                             # 9: ììœ  ê²€ìƒ‰
]

# V40: Gemini PUBLIC - ì†ŒìŠ¤ ì œí•œ ì—†ìŒ! (ë‰´ìŠ¤ ì°¨ë‹¨ ì™„ì „ ì œê±°)
GEMINI_PUBLIC_PLATFORM_HINTS = [
    "",  # ì œí•œ ì—†ìŒ - ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ììœ ë¡­ê²Œ ìˆ˜ì§‘
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
]

# Politician Details Cache
_politician_info_cache = {}

def get_politician_info(politician_id):
    """Retrieve detailed politician information (for distinguishing identical names)"""
    if politician_id in _politician_info_cache:
        return _politician_info_cache[politician_id]

    try:
        # politicians í…Œì´ë¸” ìš°ì„  (ëª¨ë“  í•„ë“œ ë³´ìœ : previous_position, birth_date ë“±)
        result = supabase.table('politicians').select('*').eq('id', politician_id).execute()
        if not result.data:
            result = supabase.table('politicians_v40').select('*').eq('id', politician_id).execute()

        if result.data:
            p = result.data[0]
            name = p.get('name', '')
            party = p.get('party', '')
            position = p.get('position', 'National Assembly Member')
            district = p.get('district', '')
            previous_position = p.get('previous_position', '')
            gender = p.get('gender', '')
            birth_year = ""
            if p.get('birth_date'):
                birth_year = str(p.get('birth_date'))[:4] + "ë…„ìƒ"

            # search_string êµ¬ì„±: í•µì‹¬ì •ë³´ + êµ¬ë¶„ì •ë³´
            search_string = f"{party} {name} {position}"
            if district:
                search_string += f" {district}"
            if previous_position:
                search_string += f" (ì „ì§: {previous_position})"
            if gender:
                search_string += f" {gender}"
            if birth_year:
                search_string += f" {birth_year}"

            info = {
                'name': name,
                'party': party,
                'position': position,
                'district': district,
                'birth_year': birth_year,
                'search_string': search_string
            }
            _politician_info_cache[politician_id] = info
            return info
    except Exception as e:
        print(f"  [WARN] Failed to retrieve politician info: {e}")

    return {'name': '', 'party': '', 'position': 'National Assembly Member', 'district': '', 'birth_year': '', 'search_string': ''}

# Category Items (V40)
def load_category_items_from_instructions():
    """instructions íŒŒì¼ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ 10ê°œ í•­ëª©ì„ ë™ì ìœ¼ë¡œ ë¡œë“œ.

    íŒŒì¼ ìœ„ì¹˜: ì„¤ê³„ë¬¸ì„œ_V7.0/V40/instructions/2_collect/cat{01~10}_{category}.md
    í…Œì´ë¸” í˜•ì‹: | # | **í•­ëª©ëª…** | ì„¤ëª… |

    Returns:
        dict: { "expertise": [("í•­ëª©ëª…", "ì„¤ëª…"), ...], ... }
    """
    import glob

    base_dir = os.path.join(os.path.dirname(__file__), "ì„¤ê³„ë¬¸ì„œ_V7.0", "V40", "instructions", "2_collect")
    category_file_map = {
        "expertise": "cat01_expertise.md",
        "leadership": "cat02_leadership.md",
        "vision": "cat03_vision.md",
        "integrity": "cat04_integrity.md",
        "ethics": "cat05_ethics.md",
        "accountability": "cat06_accountability.md",
        "transparency": "cat07_transparency.md",
        "communication": "cat08_communication.md",
        "responsiveness": "cat09_responsiveness.md",
        "publicinterest": "cat10_publicinterest.md",
    }

    result = {}
    for category_name, filename in category_file_map.items():
        filepath = os.path.join(base_dir, filename)
        items = []
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            # í…Œì´ë¸”ì—ì„œ í•­ëª© ì¶”ì¶œ: | N-N | **í•­ëª©ëª…** | ì„¤ëª… |
            import re as _re
            pattern = r'\|\s*\d+-\d+\s*\|\s*\*\*(.+?)\*\*\s*\|\s*(.+?)\s*\|'
            matches = _re.findall(pattern, content)

            for item_name, description in matches:
                # (í•­ëª©ëª…, ì„¤ëª…) - ì„¤ëª…ì„ ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ í™œìš©
                items.append((item_name.strip(), description.strip()))

            if len(items) != 10:
                print(f"  [WARN] {category_name}: {len(items)}ê°œ í•­ëª© ë¡œë“œ (10ê°œ ì˜ˆìƒ)")

        except FileNotFoundError:
            print(f"  [ERROR] íŒŒì¼ ì—†ìŒ: {filepath}")
        except Exception as e:
            print(f"  [ERROR] {category_name} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ê¸°ë³¸ê°’
        if not items:
            items = [("", "")]

        result[category_name] = items

    return result

# instructions íŒŒì¼ì—ì„œ ë™ì  ë¡œë“œ (í•˜ë“œì½”ë”© ì œê±°)
CATEGORY_ITEMS = load_category_items_from_instructions()

# Translate CATEGORIES to a dictionary for easier lookup
CATEGORIES_DICT = {item[0]: item[1] for item in CATEGORIES}

# ============================================================
# SafeFormatDict: .format_map()ì—ì„œ ëˆ„ë½ í‚¤ë¥¼ ì›ë³¸ ìœ ì§€
# ============================================================
class SafeFormatDict(dict):
    """str.format_map()ì—ì„œ ëˆ„ë½ í‚¤ë¥¼ {key} í˜•íƒœë¡œ ìœ ì§€"""
    def __missing__(self, key):
        return '{' + key + '}'

# ============================================================
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë” (Single Source of Truth)
# ============================================================
_prompt_cache = {}

def load_prompt_template(ai_name, data_type):
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ì—ì„œ search_instruction + prompt_body ë¡œë“œ

    íŒŒì¼ ìœ„ì¹˜: ì„¤ê³„ë¬¸ì„œ_V7.0/V40/instructions/2_collect/prompts/
    êµ¬ë¶„ì: ---SEARCH_INSTRUCTION_START/END---, ---PROMPT_BODY_START/END---

    Returns:
        (search_instruction_template, prompt_body_template) íŠœí”Œ
        ì‹¤íŒ¨ ì‹œ (None, None)
    """
    cache_key = f"{ai_name}_{data_type}"
    if cache_key in _prompt_cache:
        return _prompt_cache[cache_key]

    file_map = {
        ("Gemini", "official"): "gemini_official.md",
        ("Gemini", "public"): "gemini_public.md",
        ("Naver", "public"): "naver_public.md",  # âš ï¸ instructions/2_collect/prompts/naver_public.md ì°¸ì¡°
        ("Naver", "official"): None,  # Naver OFFICIAL uses direct API, no prompt needed
    }
    filename = file_map.get((ai_name, data_type))
    if not filename:
        # For Naver OFFICIAL, return empty templates (direct API call)
        if ai_name == "Naver" and data_type == "official":
            return "", ""
        print(f"  [ERROR] No prompt template for {ai_name}/{data_type}")
        return None, None

    filepath = os.path.join(
        os.path.dirname(__file__),
        "ì„¤ê³„ë¬¸ì„œ_V7.0", "V40", "instructions", "2_collect", "prompts", filename
    )

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()

        # search_instruction ì¶”ì¶œ
        si_match = re.search(
            r'---SEARCH_INSTRUCTION_START---\s*([\s\S]*?)\s*---SEARCH_INSTRUCTION_END---',
            content
        )
        search_instruction = si_match.group(1).strip() if si_match else ""

        # prompt_body ì¶”ì¶œ
        pb_match = re.search(
            r'---PROMPT_BODY_START---\s*([\s\S]*?)\s*---PROMPT_BODY_END---',
            content
        )
        prompt_body = pb_match.group(1).strip() if pb_match else ""

        if not search_instruction and not prompt_body:
            print(f"  [ERROR] Empty template: {filepath}")
            return None, None

        _prompt_cache[cache_key] = (search_instruction, prompt_body)
        return search_instruction, prompt_body

    except FileNotFoundError:
        print(f"  [ERROR] Template file not found: {filepath}")
        return None, None
    except Exception as e:
        print(f"  [ERROR] Failed to load template {filepath}: {e}")
        return None, None

def get_exact_count(table_name, filters=None):
    """Retrieve exact count"""
    try:
        query = supabase.table(table_name).select('*', count='exact')
        if filters:
            for key, value in filters.items():
                if value is not None:
                    query = query.eq(key, value)
        response = query.limit(1).execute()
        return response.count if response.count else 0
    except Exception as e:
        print(f"  [WARN] Failed to retrieve count: {e}")
        return 0


def normalize_date(date_str):
    """Normalize date string to YYYY-MM-DD format, return None on failure"""
    if not date_str or not isinstance(date_str, str):
        return None

    date_str = date_str.strip()

    # Clearly invalid values
    if date_str.upper() in ['N/A', 'NA', 'UNKNOWN', 'UNCLEAR', 'NONE', '-', '']:
        return None
    if 'Unknown' in date_str or 'Unclear' in date_str or 'None' in date_str:
        return None

    import re

    # Check YYYY-MM-DD format
    match = re.match(r'^(\d{4})-(\d{2})-(\d{2})$', date_str)
    if match:
        year, month, day = match.groups()
        # Correct invalid month/day (00 -> 01)
        month = int(month) if int(month) > 0 else 1
        day = int(day) if int(day) > 0 else 1
        # Validate month/day range
        month = min(month, 12)
        day = min(day, 28)  # Safely limit to 28 days
        return f"{year}-{month:02d}-{day:02d}"

    # YYYY-MM (only month available) -> Convert to YYYY-MM-01
    match = re.match(r'^(\d{4})-(\d{1,2})$', date_str)
    if match:
        year, month = match.groups()
        month = int(month) if int(month) > 0 else 1
        month = min(month, 12)
        return f"{year}-{month:02d}-01"

    # YYYY (only year) -> Convert to YYYY-01-01
    if re.match(r'^\d{4}$', date_str):
        return f"{date_str}-01-01"

    # yyyyMMdd format (Naver postdate)
    if re.match(r'^\d{8}$', date_str):
        year = date_str[:4]
        month = date_str[4:6]
        day = date_str[6:8]
        month = int(month) if int(month) > 0 else 1
        day = int(day) if int(day) > 0 else 1
        month = min(month, 12)
        day = min(day, 28)
        return f"{year}-{month:02d}-{day:02d}"

    # Ignore other formats
    return None


def check_politician_exists(politician_id):
    """Check politician ID"""
    try:
        # politicians í…Œì´ë¸” ìš°ì„ , V40 í…Œì´ë¸”ì€ fallback
        result = supabase.table('politicians').select('*').eq('id', politician_id).execute()
        if not result.data:
            result = supabase.table('politicians_v40').select('*').eq('id', politician_id).execute()

        if result.data and len(result.data) > 0:
            return True, result.data[0].get('name', '')
        return False, None
    except Exception as e:
        print(f"  [FAIL] Politician check error: {e}")
        return False, None


def sync_politician_to_v40(politician_id):
    """politicians í…Œì´ë¸” â†’ politicians_v40 í…Œì´ë¸” ìë™ ë™ê¸°í™”

    politicians í…Œì´ë¸”ì˜ ìµœì‹  ë°ì´í„°ë¥¼ politicians_v40ì— ë°˜ì˜í•œë‹¤.
    ì»¬ëŸ¼ ë§¤í•‘: birth_dateâ†’birth_year, website_urlâ†’website, profile_image_urlâ†’image_url
    """
    try:
        src = supabase.table('politicians').select('*').eq('id', politician_id).execute()
        if not src.data:
            return  # politicians í…Œì´ë¸”ì— ì—†ìœ¼ë©´ ìŠ¤í‚µ

        p = src.data[0]

        # birth_date(TEXT, '1965-01-01') â†’ birth_year(INTEGER)
        birth_year = None
        if p.get('birth_date'):
            try:
                birth_year = int(str(p['birth_date'])[:4])
            except (ValueError, TypeError):
                pass

        v40_data = {
            'id': p['id'],
            'name': p.get('name'),
            'party': p.get('party'),
            'position': p.get('position'),
            'identity': p.get('identity'),
            'title': p.get('title'),
            'region': p.get('region'),
            'district': p.get('district'),
            'gender': p.get('gender'),
            'birth_year': birth_year,
            'age': p.get('age'),
            'email': p.get('email'),
            'website': p.get('website_url'),
            'image_url': p.get('profile_image_url'),
        }

        # upsert: ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
        supabase.table('politicians_v40').upsert(v40_data, on_conflict='id').execute()
        print(f"  âœ… politicians â†’ politicians_v40 ë™ê¸°í™” ì™„ë£Œ ({p.get('name')})")

    except Exception as e:
        print(f"  âš ï¸ politicians_v40 ë™ê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")


def init_ai_client(ai_name):
    """Initialize AI client"""
    global ai_clients

    if ai_name in ai_clients:
        return ai_clients[ai_name]

    config = AI_CONFIGS.get(ai_name)
    if not config:
        raise ValueError(f"Unknown AI: {ai_name}")

    if ai_name == "Naver":
        # NaverëŠ” ë³„ë„ client ê°ì²´ ë¶ˆí•„ìš” (HTTP GETë§Œ ì‚¬ìš©)
        # í™˜ê²½ë³€ìˆ˜ë§Œ í™•ì¸
        client_id = os.getenv(config['env_key_id'])
        client_secret = os.getenv(config['env_key_secret'])
        if not client_id or not client_secret:
            raise ValueError(f"{config['env_key_id']}, {config['env_key_secret']} environment variables are not set.")
        ai_clients[ai_name] = {'client_id': client_id, 'client_secret': client_secret}
    elif ai_name == "Gemini":
        # Gemini: API ëª¨ë“œ ë˜ëŠ” CLI ëª¨ë“œ
        # CLI ëª¨ë“œëŠ” gemini_collect_helper.py + Gemini CLI ì§ì ‘ ì‚¬ìš©
        api_key = os.getenv(config['env_key'])
        if not api_key:
            print(f"  âš ï¸ {config['env_key']} ë¯¸ì„¤ì • â†’ Gemini CLI ì§ì ‘ ìˆ˜ì§‘ ëª¨ë“œ ì‚¬ìš©")
            print(f"  ğŸ’¡ Gemini CLIì—ì„œ: python gemini_collect_helper.py fetch â†’ ì›¹ê²€ìƒ‰ â†’ save")
            ai_clients[ai_name] = None
            return None
        from google import genai
        client = genai.Client(api_key=api_key)
        ai_clients[ai_name] = client

    return ai_clients[ai_name]


# ============================================================
# Naver Search API í˜¸ì¶œ
# ============================================================
def call_naver_search(client_config, politician_name, keywords, data_type, topic_mode, count, exclude_urls=None):
    """Naver Search API í˜¸ì¶œ

    Args:
        client_config: {'client_id': ..., 'client_secret': ...}
        politician_name: ì •ì¹˜ì¸ ì´ë¦„ (ë˜ëŠ” search_string ì „ì²´)
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¹´í…Œê³ ë¦¬ í•­ëª© ì„¤ëª…)
        data_type: 'official' or 'public'
        topic_mode: 'negative', 'positive', 'free'
        count: ìš”ì²­í•  ê²°ê³¼ ê°œìˆ˜
        exclude_urls: ì œì™¸í•  URL set (endpoint ê°„ ì¤‘ë³µ ë°©ì§€)

    Returns:
        JSON string of collected data or None on failure
    """
    print(f"  [Naver] API í˜¸ì¶œ ì¤‘... (data_type: {data_type}, topic_mode: {topic_mode})")

    client_id = client_config['client_id']
    client_secret = client_config['client_secret']

    headers = {
        'X-Naver-Client-Id': client_id,
        'X-Naver-Client-Secret': client_secret
    }

    # [Fix 4] politician_nameì—ì„œ ì‹¤ì œ ì´ë¦„ + ì‹ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    # search_string í˜•ì‹: "{ë‹¹} {ì´ë¦„} {ì§ì±…} {ì§€ì—­} ..."
    name_tokens = politician_name.split()
    actual_name = name_tokens[1] if len(name_tokens) >= 2 else politician_name
    # ë™ëª…ì´ì¸ êµ¬ë¶„ì„ ìœ„í•œ ì‹ë³„ í‚¤ì›Œë“œ (ë‹¹ëª…, ì§ì±… ë“±)
    party_name = name_tokens[0] if len(name_tokens) >= 2 else ""
    id_keywords = []
    if party_name:
        id_keywords.append(party_name)
    for token in name_tokens[2:]:
        if token.startswith('(') and token.endswith(')'):
            continue  # ê´„í˜¸ ì•ˆ ì „ì§ ì •ë³´ ì œì™¸
        if token in ['êµ­íšŒì˜ì›', 'ì˜ì›', 'êµ¬ì²­ì¥', 'ì‹œì¥']:
            id_keywords.append(token)
            break

    # Build query - ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ ì¡°í•©ìœ¼ë¡œ ê²°ê³¼ ê·¹ëŒ€í™”
    # ë¼ìš´ë“œë³„ë¡œ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ ì‚¬ìš© (í˜¸ì¶œ ì‹œë§ˆë‹¤ ë‹¤ë¥¸ ê²°ê³¼)
    import random
    query_variants = [
        f'"{actual_name}" {keywords}',                              # ì •í™•í•œ ì´ë¦„ ë§¤ì¹­
        f'{actual_name} {" ".join(id_keywords)} {keywords}',       # ì´ë¦„+ë‹¹ëª…+ì§ì±…+í‚¤ì›Œë“œ
        f'"{actual_name} ì˜ì›" {keywords}',                         # "ì¡°ì€í¬ ì˜ì›" + í‚¤ì›Œë“œ
        f'{actual_name} {keywords} êµ­íšŒ',                           # ì´ë¦„+í‚¤ì›Œë“œ+êµ­íšŒ
        f'{actual_name} {keywords} ì •ì±…',                           # ì´ë¦„+í‚¤ì›Œë“œ+ì •ì±…
    ]
    query = random.choice(query_variants)

    # Add sentiment keywords to query
    if topic_mode == 'negative':
        neg_variants = [" ë…¼ë€", " ì˜í˜¹", " ë¹„íŒ", " ë¬¸ì œ", " ì§€ì "]
        query += random.choice(neg_variants)
    elif topic_mode == 'positive':
        pos_variants = [" ì„±ê³¼", " ì—…ì ", " ì¶”ì§„", " ë°œì˜", " í†µê³¼"]
        query += random.choice(pos_variants)
    # 'free' - no extra keywords

    # For OFFICIAL, add site:.go.kr filter
    if data_type == 'official':
        query += " site:go.kr"

    print(f"    ê²€ìƒ‰ì–´: {query[:60]}...")

    # Select API endpoints based on data_type (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
    if data_type == 'official':
        # OFFICIAL: webkr ìš°ì„  (ê°€ì¥ ê²°ê³¼ ë§ìŒ), ë¶€ì¡±í•˜ë©´ doc, encyc
        endpoints = ['webkr', 'doc', 'encyc']
    else:
        # PUBLIC: news ìš°ì„  (ê°€ì¥ ê²°ê³¼ ë§ìŒ), ë¶€ì¡±í•˜ë©´ blog, cafearticle, kin
        endpoints = ['news', 'blog', 'cafearticle', 'kin']

    all_items = []
    # [Fix 3] endpoint ê°„ ì¤‘ë³µ ë°©ì§€ìš© local seen URLs
    local_seen = set(exclude_urls) if exclude_urls else set()

    for endpoint_key in endpoints:
        if len(all_items) >= count:
            break

        endpoint_url = NAVER_SEARCH_ENDPOINTS[endpoint_key]

        try:
            # 1í˜ì´ì§€ë§Œ í˜¸ì¶œ (display=100ì´ë©´ ì¶©ë¶„, API í˜¸ì¶œ ì ˆì•½)
            params = {
                'query': query,
                'display': min(100, max(10, count - len(all_items))),
                'start': 1,
                'sort': 'date'  # ìµœì‹ ìˆœ
            }

            response = requests.get(endpoint_url, headers=headers, params=params, timeout=10)

            if response.status_code != 200:
                print(f"  [Naver] {endpoint_key} API ì—ëŸ¬: {response.status_code}")
                continue  # ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸ ì‹œë„

            data = response.json()
            items = data.get('items', [])

            if not items:
                continue  # ê²°ê³¼ ì—†ìœ¼ë©´ ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸

            for item in items:
                if len(all_items) >= count:
                    break

                # Extract fields
                title = strip_html_tags(item.get('title', ''))
                link = item.get('link', '')
                description = strip_html_tags(item.get('description', ''))
                postdate = item.get('postdate', '')  # yyyyMMdd format

                # Skip if no URL
                if not link or 'dummy' in link.lower():
                    continue

                # [Fix 3] endpoint ê°„ URL ì¤‘ë³µ ì²´í¬
                url_norm = normalize_url(link)
                if url_norm and url_norm in local_seen:
                    continue
                if url_norm:
                    local_seen.add(url_norm)

                # [Fix 4] ê´€ë ¨ì„± í•„í„°: ì´ë¦„ OR ë‹¹ëª…+ì§ì±…ì´ title/descriptionì— ìˆìœ¼ë©´ í†µê³¼
                name_in_text = actual_name in title or actual_name in description
                if not name_in_text:
                    continue

                # [Fix 5] ë¬´ì˜ë¯¸ ë¬¸ì„œ í•„í„°: ì—‘ì…€/PDF ë‹¤ìš´ë¡œë“œ í˜ì´ì§€ ì œì™¸
                skip_extensions = ['.xlsx', '.xls', '.csv', '.hwp', '.pdf', '.zip']
                if any(link.lower().endswith(ext) for ext in skip_extensions):
                    continue

                # Normalize date
                date_normalized = normalize_date(postdate) if postdate else None

                collected_item = {
                    'title': title,
                    'content': description,  # Naver returns description (summary)
                    'source': endpoint_key.upper(),  # NEWS, BLOG, CAFEARTICLE, KIN, WEBKR, DOC, ENCYC
                    'source_url': link,
                    'date': date_normalized or ''
                }

                all_items.append(collected_item)

            # Rate limiting: 100ms between requests
            time.sleep(0.1)

        except Exception as e:
            print(f"  [Naver] {endpoint_key} ì—ëŸ¬: {e}")
            continue

    if all_items:
        print(f"  [Naver] ìµœì¢… ìˆ˜ì§‘: {len(all_items)}ê°œ")
        return json.dumps(all_items, ensure_ascii=False, indent=2)
    else:
        return None


# --- Gemini API í˜¸ì¶œ (V40) ---
def call_gemini_with_search(client, prompt, data_type="public"):
    """Gemini API í˜¸ì¶œ (Google Search Grounding + URL ê²€ì¦)

    API í‚¤ê°€ ì—†ìœ¼ë©´ Gemini CLI ì§ì ‘ ìˆ˜ì§‘ ëª¨ë“œ ì‚¬ìš©:
    â†’ gemini_collect_helper.py fetch/save + Gemini CLI í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰
    """
    if client is None:
        print(f"  [Gemini] API í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ â†’ Gemini CLI ì§ì ‘ ìˆ˜ì§‘ ëª¨ë“œ ì‚¬ìš©")
        return None

    from google.genai import types

    print(f"  [Gemini] API í˜¸ì¶œ ì¤‘...")

    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())]
            )
        )

        response_text = response.text if response.text else ""
        json_text = extract_json_from_text(response_text)

        try:
            raw_data = json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"  [Gemini] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            items = re.findall(r'\{[^{}]*\}', json_text)
            raw_data = []
            for item in items[:10]:
                try:
                    raw_data.append(json.loads(item))
                except:
                    pass
            if not raw_data:
                return None

        print(f"  [Gemini] ì›ë³¸ ë°ì´í„°: {len(raw_data)}ê°œ")

        # URL ê²€ì¦ ë° í•„í„°ë§
        verified_data = []
        invalid_count = 0

        for item in raw_data:
            url = item.get('source_url') or item.get('url') or ''

            if 'dummy' in url.lower() or not url:
                invalid_count += 1
                continue

            if 'grounding-api-redirect' in url:
                real_url = resolve_redirect_url(url)
                if real_url != url:
                    item['source_url'] = real_url
                    url = real_url
                else:
                    invalid_count += 1
                    continue

            if not validate_url(url):
                invalid_count += 1
                continue

            verified_item = {
                'title': item.get('title') or item.get('data_title') or '',
                'content': item.get('content') or item.get('data_content') or '',
                'source': item.get('source') or item.get('data_source') or '',
                'source_url': url,
                'date': item.get('date') or item.get('data_date') or ''
            }
            verified_data.append(verified_item)

        if invalid_count > 0:
            print(f"  [Gemini] ë¬´íš¨ URL ì œì™¸: {invalid_count}ê°œ")
        print(f"  [Gemini] ìµœì¢… í†µê³¼: {len(verified_data)}ê°œ")

        return json.dumps(verified_data, ensure_ascii=False, indent=2) if verified_data else None

    except Exception as e:
        print(f"  âŒ Gemini API ì—ëŸ¬: {e}")
        return None


def build_search_prompt(ai_name, data_type, topic_mode, politician_full, item_keywords, count, year_hint, item_name, exclude_urls=None, domain_hint=""):
    """Build search prompt from template"""

    # Load template
    search_instruction, prompt_body = load_prompt_template(ai_name, data_type)
    if not search_instruction and not prompt_body:
        # For Naver OFFICIAL, no prompt needed (direct API)
        if ai_name == "Naver" and data_type == "official":
            return None
        print(f"  [ERROR] Failed to load prompt template for {ai_name}/{data_type}")
        return None

    # Build topic_instruction
    if topic_mode == 'negative':
        topic_instruction = "ë¶€ì •ì  ì£¼ì œ ìš°ì„  ìˆ˜ì§‘: ë…¼ë€, ì˜í˜¹, ë¹„íŒ, ë°˜ëŒ€, ì‹¤íŒ¨, ë¬¸ì œì "
    elif topic_mode == 'positive':
        topic_instruction = "ê¸ì •ì  ì£¼ì œ ìš°ì„  ìˆ˜ì§‘: ì„±ê³¼, ì—…ì , ì¹­ì°¬, ì§€ì§€, ì„±ê³µ, ê¸ì •í‰ê°€"
    else:
        topic_instruction = "ììœ  ìˆ˜ì§‘ (ê¸ì •/ë¶€ì • ì œí•œ ì—†ìŒ)"

    # Build exclude_instruction
    exclude_instruction = ""
    if exclude_urls and len(exclude_urls) > 0:
        exclude_list = list(exclude_urls)[:20]  # ìµœëŒ€ 20ê°œë§Œ í‘œì‹œ
        exclude_urls_str = "\n".join([f"  - {url}" for url in exclude_list])
        exclude_instruction = f"ë‹¤ìŒ URLì€ ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë¯€ë¡œ ì œì™¸:\n{exclude_urls_str}"

    # Format template
    format_dict = SafeFormatDict({
        'politician_full': politician_full,
        'extra_keyword': item_keywords,
        'domain_hint': domain_hint,
        'topic_instruction': topic_instruction,
        'exclude_instruction': exclude_instruction,
        'year_hint': year_hint,
        'remaining': count
    })

    prompt = prompt_body.format_map(format_dict)
    return prompt


def call_ai(ai_name, client, prompt, data_type):
    """Call AI API"""
    if ai_name == "Gemini":
        return call_gemini_with_search(client, prompt, data_type)
    elif ai_name == "Naver":
        # Naver uses direct API call, not prompt-based
        # This function should not be called for Naver
        print(f"  [ERROR] call_ai should not be called for Naver")
        return None
    else:
        print(f"  [ERROR] Unknown AI: {ai_name}")
        return None


def extract_url(item):
    """Extract URL from item"""
    return item.get('source_url') or item.get('url') or ''


def validate_collected_data(data, data_type, ai_name):
    """Validate collected data items"""
    if not isinstance(data, list):
        return [], ["Data is not a list"]

    valid_items = []
    errors = []

    for idx, item in enumerate(data):
        if not isinstance(item, dict):
            errors.append(f"Item {idx} is not a dict")
            continue

        # Required fields
        title = item.get('title') or item.get('data_title') or ''
        content = item.get('content') or item.get('data_content') or ''
        source = item.get('source') or item.get('data_source') or ''
        url = extract_url(item)
        date = item.get('date') or item.get('data_date') or ''

        # Validate
        if not title:
            errors.append(f"Item {idx}: Missing title")
            continue
        if not url:
            errors.append(f"Item {idx}: Missing URL")
            continue
        if 'dummy' in url.lower():
            errors.append(f"Item {idx}: Dummy URL")
            continue

        # Normalize fields
        normalized_item = {
            'title': title,
            'content': content,
            'source': source,
            'source_url': url,
            'date': normalize_date(date) if date else None
        }

        valid_items.append(normalized_item)

    return valid_items, errors


def collect_with_ai(ai_name, politician_id, politician_name, category_name, test_mode=False):
    """Collect data with specific AI for a category

    V40: Naver í†µí•©, OFFICIAL 10-10-80 / PUBLIC 20-20-60
    """
    print(f"\n{'='*60}")
    print(f"[{ai_name}] Collecting {category_name}...")
    print(f"{'='*60}")

    client = init_ai_client(ai_name)
    politician_info = get_politician_info(politician_id)
    politician_full = politician_info['search_string']

    distribution = TEST_DISTRIBUTION if test_mode else COLLECT_DISTRIBUTION
    sentiment_max = TEST_SENTIMENT_DISTRIBUTION if test_mode else SENTIMENT_MAX

    ai_distribution = distribution.get(ai_name, {})
    ai_max = sentiment_max.get(ai_name, {})

    # [MIN/MAX] DBì—ì„œ ê¸°ì¡´ ìˆ˜ëŸ‰ ì¡°íšŒ â†’ MAX ë„ë‹¬ ì‹œ ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸°
    existing_counts = {}  # (data_type, sentiment) â†’ count
    try:
        existing = supabase.table(TABLE_COLLECTED_DATA).select('data_type, sentiment', count='exact')\
            .eq('politician_id', politician_id)\
            .eq('category', category_name)\
            .eq('collector_ai', ai_name).execute()
        from collections import Counter
        for item in existing.data:
            key = (item['data_type'], item['sentiment'])
            existing_counts[key] = existing_counts.get(key, 0) + 1
        print(f"  [DB] ê¸°ì¡´ ë°ì´í„° {len(existing.data)}ê°œ ë¡œë“œ (MAX ì²´í¬ìš©)")
    except Exception as e:
        print(f"  [DB] ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # [Fix 2] data_type ë‹¨ìœ„ë¡œ shared_seen_urls ìœ ì§€ â†’ sentiment ê°„ ì¤‘ë³µ ë°©ì§€
    # Process OFFICIAL, then PUBLIC
    for data_type in ['official', 'public']:
        type_count = ai_distribution.get(data_type, 0)
        if type_count == 0:
            continue

        print(f"\n  [{data_type.upper()}] Target: {type_count}ê°œ")

        type_max = ai_max.get(data_type, {})
        shared_seen_urls = set()  # sentiment ê°„ ê³µìœ ë˜ëŠ” seen_urls

        # Process sentiments: negative, positive, free
        for topic_mode in ['negative', 'positive', 'free']:
            max_target = type_max.get(topic_mode, 0)
            if max_target == 0:
                continue

            # [MIN/MAX] DB ê¸°ì¡´ ìˆ˜ëŸ‰ í™•ì¸ â†’ MAX ë„ë‹¬ ì‹œ ê±´ë„ˆë›°ê¸°
            db_sentiment = topic_mode_to_sentiment(topic_mode)
            existing = existing_counts.get((data_type, db_sentiment), 0)
            remaining = max_target - existing
            if remaining <= 0:
                print(f"    {topic_mode.upper()}: MAX ë„ë‹¬ ({existing}/{max_target}), ê±´ë„ˆë›°ê¸°")
                continue

            print(f"    {topic_mode.upper()}: DB {existing}ê°œ / MAX {max_target}ê°œ â†’ {remaining}ê°œ ì¶”ê°€ ìˆ˜ì§‘")

            # Call collect_data_type with shared seen_urls (remainingë§Œí¼ë§Œ ìˆ˜ì§‘)
            collected_items = collect_data_type(
                ai_name, client, politician_id, politician_full,
                category_name, data_type, topic_mode, remaining,
                pre_seen_urls=shared_seen_urls
            )

            if not collected_items:
                print(f"    âš ï¸ {topic_mode.upper()}: No data collected")
                continue

            # [Fix 2] ìˆ˜ì§‘ëœ URLì„ shared_seen_urlsì— ì¶”ê°€ â†’ ë‹¤ìŒ sentimentì—ì„œ ì¤‘ë³µ ë°©ì§€
            for ci in collected_items:
                ci_url = normalize_url(ci.get('source_url', ''))
                if ci_url:
                    shared_seen_urls.add(ci_url)

            # Save to DB
            save_count = 0
            skipped_date = 0
            # ê¸°ê°„ ì œí•œ ê¸°ì¤€ì¼ ê³„ì‚°
            date_limit_official = (datetime.now() - timedelta(days=365*4)).strftime('%Y-%m-%d')
            date_limit_public = (datetime.now() - timedelta(days=365*2)).strftime('%Y-%m-%d')

            for item in collected_items:
                try:
                    # Map sentiment
                    db_sentiment = topic_mode_to_sentiment(item.get('sentiment', 'free'))

                    # content â†’ summary (30% ìš”ì•½)
                    content_str = str(item.get('content', ''))[:2000]
                    summary_len = max(30, int(len(content_str) * 0.3))
                    summary = content_str[:summary_len]

                    # published_date ì •ê·œí™”
                    raw_date = item.get('date') or item.get('published_date') or ''
                    pub_date = normalize_date(raw_date)

                    # ê¸°ê°„ ì´ˆê³¼ ì‚¬ì „ í•„í„°ë§ (DB ì €ì¥ ì „ ì°¨ë‹¨)
                    if pub_date:
                        item_data_type = item.get('data_type', data_type)
                        date_limit = date_limit_official if item_data_type == 'official' else date_limit_public
                        if pub_date < date_limit:
                            skipped_date += 1
                            continue

                    db_item = {
                        'politician_id': politician_id,
                        'politician_name': politician_name,
                        'category': category_name,
                        'data_type': item.get('data_type', data_type),
                        'sentiment': db_sentiment,
                        'collector_ai': ai_name,
                        'title': str(item.get('title', ''))[:200],
                        'content': content_str,
                        'summary': summary,
                        'source_url': item.get('source_url', ''),
                        'source_name': str(item.get('source', '')),
                        'published_date': pub_date,
                        'is_verified': False
                    }

                    supabase.table(TABLE_COLLECTED_DATA).insert(db_item).execute()
                    save_count += 1

                except Exception as e:
                    print(f"    [ERROR] DB ì €ì¥ ì‹¤íŒ¨: {e}")

            date_msg = f" (ê¸°ê°„ì´ˆê³¼ {skipped_date}ê°œ ì œì™¸)" if skipped_date > 0 else ""
            print(f"    âœ… {topic_mode.upper()}: {save_count}ê°œ ì €ì¥ ì™„ë£Œ{date_msg}")

    print(f"\nâœ… [{ai_name}] {category_name} ìˆ˜ì§‘ ì™„ë£Œ\n")


def collect_data_type(ai_name, client, politician_id, politician_full, category_name, data_type, topic_mode, count, pre_seen_urls=None):
    """Collects data for a specific AI, category, data type, and sentiment.

    V40: Naver ì§ì ‘ API í˜¸ì¶œ vs Gemini í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í˜¸ì¶œ ë¶„ê¸°
    """
    # AIë³„ íŒŒë¼ë¯¸í„° ì°¨ë“± ì ìš©
    if ai_name == "Naver":
        MAX_ROUNDS = 10      # Naver ë¼ìš´ë“œ ì¦ê°€ (ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ ì¡°í•© í™œìš©)
        MAX_PER_CALL = 30    # í•œë²ˆì— 30ê°œ ìš”ì²­ (5í˜ì´ì§€ í™œìš©)
        MAX_EMPTY_KW = 15    # ë¹ˆ í‚¤ì›Œë“œ í—ˆìš© ì¦ê°€ (ê²€ìƒ‰ì–´ ë‹¤ì–‘í™”ë¡œ ê²°ê³¼ ë³€ë™)
        MAX_EMPTY_ROUNDS = 3
    else:  # Gemini
        MAX_ROUNDS = 7
        MAX_PER_CALL = 5
        MAX_EMPTY_KW = 7
        MAX_EMPTY_ROUNDS = 2
    MAX_429_RETRIES = 3

    print(f"    -> {topic_mode.upper()} ({count}ê°œ) ìˆ˜ì§‘ ì‹œì‘...")
    all_items = []
    # [Fix 2] ì°¸ì¡° ê³µìœ : pre_seen_urlsê°€ ìˆìœ¼ë©´ ê°™ì€ set ê°ì²´ë¥¼ ì‚¬ìš© â†’ í˜¸ì¶œìì™€ ê³µìœ 
    if pre_seen_urls is None:
        pre_seen_urls = set()
    seen_urls = pre_seen_urls  # ì°¸ì¡° ìœ ì§€ (ë³µì‚¬ ê¸ˆì§€)
    seen_titles = set()  # title ê¸°ë°˜ ì¤‘ë³µ ê°ì§€ìš©

    # [Fix 1] DBì—ì„œ ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ ê¸°ì¡´ URLë§Œ ë¡œë“œ â†’ ì¹´í…Œê³ ë¦¬ ê°„ ì¤‘ë³µì€ í—ˆìš©
    try:
        all_db_data = []
        db_offset = 0
        while True:
            page = supabase.table(TABLE_COLLECTED_DATA).select('source_url,title')\
                .eq('politician_id', politician_id)\
                .eq('category', category_name)\
                .range(db_offset, db_offset + 999).execute()
            if page.data:
                all_db_data.extend(page.data)
            if not page.data or len(page.data) < 1000:
                break
            db_offset += 1000
        existing_data = type('obj', (object,), {'data': all_db_data})()
        db_url_count = 0
        for item in existing_data.data:
            if item.get('source_url'):
                seen_urls.add(normalize_url(item['source_url']))
                db_url_count += 1
            if item.get('title'):
                seen_titles.add(normalize_title(item['title']))
        if db_url_count > 0:
            print(f"    [DB] ê¸°ì¡´ URL {db_url_count}ê°œ ë¡œë“œ (ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ ì¤‘ë³µ ë°©ì§€)")
    except Exception as e:
        print(f"    [DB] ê¸°ì¡´ URL ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

    category_items = CATEGORY_ITEMS.get(category_name, [("", "")])
    current_year = datetime.now().year
    year_hint = f"({current_year}, {current_year - 1})"

    consecutive_empty_rounds = 0

    for round_num in range(MAX_ROUNDS):
        unique_count = len(all_items)
        remaining = count - unique_count
        if remaining <= 0:
            break

        if round_num > 0:
            print(f"    [ë¼ìš´ë“œ {round_num+1}] í˜„ì¬ {unique_count}ê°œ / ëª©í‘œ {count}ê°œ")

        round_added_total = 0

        consecutive_empty_keywords = 0
        for kw_idx, (item_name, item_keywords) in enumerate(category_items):
            unique_count = len(all_items)
            remaining = count - unique_count
            if remaining <= 0:
                break

            if consecutive_empty_keywords >= MAX_EMPTY_KW:
                print(f"    âš ï¸ ì—°ì† {consecutive_empty_keywords}ê°œ í‚¤ì›Œë“œ ê²°ê³¼ ì—†ìŒ â†’ ë¼ìš´ë“œ ì¡°ê¸° ì¢…ë£Œ")
                break

            request_count = min(remaining, MAX_PER_CALL)

            # 429 ì—ëŸ¬ ì¬ì‹œë„ ë£¨í”„
            for retry in range(MAX_429_RETRIES):
                try:
                    # AI ë¶„ê¸°: Naver vs Gemini
                    if ai_name == "Naver":
                        # Naver: Direct API call (Fix 3: exclude_urls ì „ë‹¬)
                        response_text = call_naver_search(
                            client, politician_full, item_keywords,
                            data_type, topic_mode, request_count,
                            exclude_urls=seen_urls
                        )
                    else:
                        # Gemini: Prompt-based
                        domain_hint = ""
                        if data_type == "official":
                            hints = GEMINI_OFFICIAL_DOMAIN_HINTS
                        else:
                            hints = GEMINI_PUBLIC_PLATFORM_HINTS
                        domain_hint = hints[kw_idx % len(hints)]

                        prompt = build_search_prompt(
                            ai_name, data_type, topic_mode, politician_full,
                            item_keywords, request_count, year_hint, item_name,
                            exclude_urls=seen_urls,
                            domain_hint=domain_hint
                        )

                        if not prompt:
                            break

                        response_text = call_ai(ai_name, client, prompt, data_type)

                    if not response_text:
                        break

                    # JSON Parsing
                    json_match = re.search(r'```json\s*([\s\S]+?)\s*```', response_text)
                    if json_match:
                        response_json_str = json_match.group(1)
                    else:
                        response_json_str = response_text

                    data = json.loads(response_json_str)

                    # Data validation
                    valid_data, errors = validate_collected_data(data, data_type, ai_name)

                    if not valid_data:
                        break

                    # ë©”ëª¨ë¦¬ ë‚´ ì¤‘ë³µ ì œê±° + ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    added = 0
                    for item in valid_data:
                        if len(all_items) >= count:
                            break

                        # URL ì •ê·œí™”ë¡œ ì¤‘ë³µ ì²´í¬
                        url = extract_url(item)
                        url_normalized = normalize_url(url) if url else ''

                        if url_normalized and url_normalized in seen_urls:
                            continue

                        # [Fix 5] title ê¸°ë°˜ ì¤‘ë³µ ê°ì§€ (URLì´ ë‹¬ë¼ë„ ê°™ì€ ê¸°ì‚¬ì¸ ê²½ìš°)
                        title_norm = normalize_title(item.get('title', ''))
                        if title_norm and title_norm in seen_titles:
                            continue
                        if title_norm:
                            seen_titles.add(title_norm)

                        # ìœ ë‹ˆí¬ ì•„ì´í…œ ì¶”ê°€
                        item['sentiment'] = topic_mode
                        item['data_type'] = data_type
                        item['collector_ai'] = ai_name
                        all_items.append(item)
                        if url_normalized:
                            seen_urls.add(url_normalized)
                        added += 1

                    if added > 0:
                        short_name = item_name[:5] if len(item_name) > 5 else item_name
                        print(f"      [{short_name}] +{added}ê°œ â†’ ëˆ„ì  {len(all_items)}ê°œ")
                        consecutive_empty_keywords = 0
                        round_added_total += added
                    else:
                        consecutive_empty_keywords += 1

                    break

                except json.JSONDecodeError as e:
                    print(f"    [{ai_name}] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                    break

                except Exception as e:
                    error_str = str(e)
                    if '429' in error_str or 'RESOURCE_EXHAUSTED' in error_str:
                        wait_time = 2 ** (retry + 1)
                        print(f"  âŒ {ai_name} API ì—ëŸ¬: 429 RESOURCE_EXHAUSTED. {error_str[:100]}")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"  âŒ {ai_name} API ì—ëŸ¬: {e}")
                        break

        # ë¼ìš´ë“œ ì¢…ë£Œ ì²´í¬
        if round_added_total == 0:
            consecutive_empty_rounds += 1
            if consecutive_empty_rounds >= MAX_EMPTY_ROUNDS:
                print(f"    âš ï¸ ì—°ì† {consecutive_empty_rounds}ê°œ ë¼ìš´ë“œ ê²°ê³¼ ì—†ìŒ â†’ ì¡°ê¸° ì¢…ë£Œ")
                break
        else:
            consecutive_empty_rounds = 0

    print(f"    -> {topic_mode.upper()} ìµœì¢…: {len(all_items)}ê°œ")
    return all_items


def verify_test_results(politician_id, test_mode=True, category_list=None):
    """Verify test collection results

    Returns:
        dict: {category_name: total_count} - ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ ìˆ˜ì§‘ëŸ‰
    """
    print(f"\n{'='*60}")
    print("ê²°ê³¼ ê²€ì¦")
    print(f"{'='*60}\n")

    distribution = TEST_DISTRIBUTION if test_mode else COLLECT_DISTRIBUTION
    check_categories = category_list if category_list else CATEGORIES
    category_totals = {}

    for category_name, category_label in check_categories:
        print(f"[{category_label}]")
        cat_total = 0

        for ai_name in ["Gemini", "Naver"]:
            ai_dist = distribution.get(ai_name, {})

            for data_type in ['official', 'public']:
                type_count = ai_dist.get(data_type, 0)
                if type_count == 0:
                    continue

                actual_count = get_exact_count(
                    TABLE_COLLECTED_DATA,
                    {
                        'politician_id': politician_id,
                        'category': category_name,
                        'data_type': data_type,
                        'collector_ai': ai_name
                    }
                )
                cat_total += actual_count

                status = "âœ…" if actual_count >= type_count else "âš ï¸"
                print(f"  {status} {ai_name} {data_type.upper()}: {actual_count}/{type_count}ê°œ")

        category_totals[category_name] = cat_total
        target = 20 if test_mode else 100
        status = "âœ…" if cat_total >= target else "âš ï¸"
        print(f"  {status} ì¹´í…Œê³ ë¦¬ í•©ê³„: {cat_total}ê°œ (ìµœì†Œ ëª©í‘œ: {target})")

    print(f"\n{'='*60}\n")
    return category_totals


# ============================================================
# ìµœì†Œ ëª©í‘œ ë¯¸ë‹¬ ì¹´í…Œê³ ë¦¬ ì¬ìˆ˜ì§‘ ìƒìˆ˜
# ============================================================
MIN_TARGET_FULL = 100    # í’€ ëª¨ë“œ ìµœì†Œ ëª©í‘œ
MIN_TARGET_TEST = 20     # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ìµœì†Œ ëª©í‘œ
MAX_RETRY_ROUNDS = 3     # ì¬ìˆ˜ì§‘ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜


def main():
    parser = argparse.ArgumentParser(description='V40 Data Collection Script')
    parser.add_argument('--politician_id', type=str, required=True, help='Politician ID (8-char hex)')
    parser.add_argument('--politician_name', type=str, required=True, help='Politician Name')
    parser.add_argument('--ai', type=str, choices=['Gemini', 'Naver'], help='Specific AI to run')
    parser.add_argument('--category', type=int, choices=range(1, 11), help='Specific category (1-10)')
    parser.add_argument('--test', action='store_true', help='Mini test mode (20 items per category)')
    args = parser.parse_args()

    # Validate politician
    exists, db_name = check_politician_exists(args.politician_id)
    if not exists:
        print(f"âŒ Politician ID not found: {args.politician_id}")
        return

    print(f"\n{'='*60}")
    print(f"V40 Data Collection - {args.politician_name}")
    print(f"{'='*60}")
    print(f"Politician ID: {args.politician_id}")
    print(f"DB Name: {db_name}")
    print(f"Test Mode: {args.test}")
    print(f"{'='*60}\n")

    # politicians â†’ politicians_v40 ë™ê¸°í™”
    sync_politician_to_v40(args.politician_id)

    # Select AIs to run
    ai_list = [args.ai] if args.ai else ['Gemini', 'Naver']

    # Select categories to run
    if args.category:
        category_list = [CATEGORIES[args.category - 1]]
    else:
        category_list = CATEGORIES

    # Run collection
    for category_name, category_label in category_list:
        for ai_name in ai_list:
            collect_with_ai(
                ai_name,
                args.politician_id,
                args.politician_name,
                category_name,
                test_mode=args.test
            )

    # Verify results + ì¬ìˆ˜ì§‘ ë£¨í”„
    min_target = MIN_TARGET_TEST if args.test else MIN_TARGET_FULL

    for retry_round in range(MAX_RETRY_ROUNDS + 1):
        # ê²€ì¦
        category_totals = verify_test_results(
            args.politician_id, test_mode=args.test, category_list=category_list
        )

        # ë¯¸ë‹¬ ì¹´í…Œê³ ë¦¬ í™•ì¸
        deficit_categories = [
            (cat_name, cat_label)
            for cat_name, cat_label in category_list
            if category_totals.get(cat_name, 0) < min_target
        ]

        if not deficit_categories:
            print(f"âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìµœì†Œ ëª©í‘œ {min_target}ê°œ ë‹¬ì„±!")
            break

        if retry_round >= MAX_RETRY_ROUNDS:
            print(f"\nâš ï¸ ì¬ìˆ˜ì§‘ {MAX_RETRY_ROUNDS}íšŒ ì™„ë£Œ. ì•„ì§ ë¯¸ë‹¬ ì¹´í…Œê³ ë¦¬ ì¡´ì¬:")
            for cat_name, cat_label in deficit_categories:
                total = category_totals.get(cat_name, 0)
                print(f"  âš ï¸ {cat_label}: {total}/{min_target}ê°œ")
            break

        # ì¬ìˆ˜ì§‘ ì‹¤í–‰
        print(f"\n{'='*60}")
        print(f"ğŸ”„ ì¬ìˆ˜ì§‘ ë¼ìš´ë“œ {retry_round + 1}/{MAX_RETRY_ROUNDS}")
        print(f"ë¯¸ë‹¬ ì¹´í…Œê³ ë¦¬: {len(deficit_categories)}ê°œ")
        for cat_name, cat_label in deficit_categories:
            total = category_totals.get(cat_name, 0)
            print(f"  âš ï¸ {cat_label}: {total}/{min_target}ê°œ (ë¶€ì¡±: {min_target - total}ê°œ)")
        print(f"{'='*60}\n")

        for cat_name, cat_label in deficit_categories:
            for ai_name in ai_list:
                collect_with_ai(
                    ai_name,
                    args.politician_id,
                    args.politician_name,
                    cat_name,
                    test_mode=args.test
                )

    print("\nâœ… V40 ìˆ˜ì§‘ ì™„ë£Œ!\n")


if __name__ == "__main__":
    main()

# 중복 방지 전략 - 카테고리별 공통 섹션

**이 섹션을 모든 카테고리 파일(cat01~cat10)에 삽입**

---

## 3. 중복 방지 전략 ✨

### 🔥 고급 중복 검사 (URL + 제목)

```
V40부터 URL + 제목 이중 검사로 강화!
```

### 3.1 URL 정규화

**파라미터, 앵커 제거로 같은 페이지 식별**

```
예시:
✅ 중복으로 감지 (같은 페이지):
- https://namu.wiki/w/김민석/비판%20및%20논란
- https://namu.wiki/w/김민석/비판%20및%20논란?rev=456
- https://namu.wiki/w/김민석/비판%20및%20논란#s-1
- https://namu.wiki/w/김민석/비판%20및%20논란?from=search

→ 모두 https://namu.wiki/w/김민석/비판%20및%20논란 로 정규화
→ 1개만 수집, 나머지는 중복으로 스킵
```

### 3.2 제목 유사도 검사

**80% 이상 유사한 제목 = 중복**

```
예시:
✅ 중복으로 감지 (80% 이상 유사):
- "김민석/비판 및 논란/국무총리 후보자 - 나무위키"
- "김민석 / 비판 및 논란 / 국무총리 후보자 - 나무위키"
- "김민석/비판 및 논란/국무총리 후보자"

→ 제목 정규화 후 비교
→ 1개만 수집, 나머지는 중복으로 스킵
```

### 3.3 중복 판단 규칙

```
┌─────────────────────────────────────────┐
│ 같은 AI가 수집한 데이터 중복 체크        │
├─────────────────────────────────────────┤
│ [조건 1] URL 정규화 후 일치             │
│    → 중복으로 판단, 스킵                │
│                                         │
│ [조건 2] 제목 80% 이상 유사             │
│    → 중복으로 판단, 스킵                │
│                                         │
│ [조건 3] 다른 AI가 같은 데이터 수집     │
│    → 중복 아님, 모두 저장 (자연 가중치) │
└─────────────────────────────────────────┘
```

### 3.4 왜 제목 검사가 필요한가?

**실제 발생한 문제 (2026-01-21 발견):**

```
김민석 정치인 평가에서:
- "김민석/비판 및 논란" 나무위키 페이지
- Gemini가 19번 중복 수집 (URL 파라미터만 다름)
- 각각 4개 AI가 평가 → 19 × 4 = 76번 부정 평가
- 실제로는 1개 기사인데 76번 부정 영향
- 점수 648점(C)으로 부당하게 하락

⚠️ URL만 체크하면:
- https://namu.wiki/w/김민석?rev=1
- https://namu.wiki/w/김민석?rev=2
- https://namu.wiki/w/김민석#s-1

→ URL이 다르면 "다른 데이터"로 인식
→ 같은 내용인데 중복 수집됨
→ 점수 왜곡 발생!

✅ URL + 제목 체크:
→ 정규화 후 비교
→ 같은 페이지를 정확히 감지
→ 중복 제거 성공
→ 점수 왜곡 방지
```

### 3.5 AI에게 지시사항

**📌 수집 채널 (Gemini, Naver):**

```
⚠️ 중복 방지 필수:
1. 같은 소스의 다른 버전/섹션 수집 금지
   ❌ 나무위키 같은 페이지의 다른 섹션
   ❌ 뉴스 같은 기사의 다른 버전

2. 이미 수집한 제목과 80% 이상 유사하면 수집 금지
   ❌ "김민석 의혹 논란" vs "김민석 논란 의혹"
   ❌ "김민석/비판" vs "김민석 / 비판"

3. 다양한 소스에서 수집 (같은 소스 반복 금지)
   ✅ 나무위키 1개, 뉴스 1개, 블로그 1개
   ❌ 나무위키 10개 (다른 섹션)
```

---

## 4. URL 검증 기술 (듀얼 테스트 결과 - 2026-01-29)

### 4.1 URL 검증 방식

**기존 문제**: HEAD 요청 시 일부 서버 차단으로 검증 실패

**해결책**: GET stream=True + User-Agent

```python
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
response = requests.get(url, timeout=5, allow_redirects=True, headers=headers, stream=True)
response.close()  # 전체 다운로드 방지
return response.status_code < 400
```

### 4.2 Gemini redirect URL 해결

**문제**: Gemini가 `grounding-api-redirect/...` 형태 URL 반환

**해결**: Location 헤더에서 실제 URL 추출

```python
response = requests.head(redirect_url, allow_redirects=False)
if response.status_code in [301, 302, 303, 307, 308]:
    real_url = response.headers['Location']
```

### 4.3 V40 소스 제한 원칙

**V40에서는 도메인 필터링 없음!**

```
제한은 딱 2개뿐:
1. OFFICIAL 수집 → OFFICIAL 소스만
2. PUBLIC 수집 → PUBLIC 소스만

그 외 소스 제한 일체 없음!
- 뉴스 차단 금지
- 도메인 필터 금지
- 플랫폼 지정 금지
```

---

## 5. summary 규칙

### 5.1 요약 생성 규칙

**content의 30% 수준으로 summary 자동 생성**

```
규칙:
- summary = content 앞부분의 30% (글자 수 기준)
- 최소 30자 이상
- 수집 스크립트(collect_v40.py)에서 자동 생성
- AI에게 별도 요청 불필요 (코드에서 처리)
```

### 5.2 summary 용도

```
- 평가 AI에게 전달 시 토큰 절감 (원문 대신 요약 사용)
- 빠른 데이터 미리보기
- 비용 30% 절감 효과
```

---

**이 섹션을 "## X. 평가 범위" 앞에 삽입하고, 이후 섹션 번호를 +1씩 조정**

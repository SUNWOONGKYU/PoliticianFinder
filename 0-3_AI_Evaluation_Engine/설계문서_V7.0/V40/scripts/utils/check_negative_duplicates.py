# -*- coding: utf-8 -*-
"""
ë¶€ì •ì  ë°ì´í„° ì¤‘ë³µ ê²€ì‚¬
"""
import os
import sys
from supabase import create_client
from dotenv import load_dotenv
from collections import Counter
import re

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

load_dotenv(override=True)
supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_SERVICE_ROLE_KEY'))

politician_id = 'f9e00370'
politician_name = 'ê¹€ë¯¼ì„'

print('='*100)
print(f'ë¶€ì •ì  ë°ì´í„° ì¤‘ë³µ ê²€ì‚¬ - {politician_name}')
print('='*100)
print()

# 1. ì „ì²´ ìˆ˜ì§‘ ë°ì´í„° sentiment ë¶„í¬
print('ğŸ“Š [1] ìˆ˜ì§‘ ë°ì´í„° sentiment ë¶„í¬:')
print('-'*100)

result = supabase.table('collected_data_v40')\
    .select('sentiment, data_type, collector_ai')\
    .eq('politician_id', politician_id)\
    .execute()

total_data = result.data
sentiments = [item['sentiment'] for item in total_data]
sentiment_counts = Counter(sentiments)

print(f'{"Sentiment":<15} {"ê°œìˆ˜":>10} {"ë¹„ìœ¨":>10}')
print('-'*100)
for sentiment, count in sentiment_counts.most_common():
    pct = (count / len(total_data) * 100) if total_data else 0
    print(f'{sentiment:<15} {count:>10} {pct:>9.1f}%')
print('-'*100)
print(f'{"ì „ì²´":<15} {len(total_data):>10}')
print('='*100)
print()

# 2. ë¶€ì •ì  ë°ì´í„° ìƒì„¸ ë¶„ì„
print('ğŸ“Š [2] ë¶€ì •ì (negative) ë°ì´í„° ìƒì„¸ ë¶„ì„:')
print('-'*100)

negative_result = supabase.table('collected_data_v40')\
    .select('*')\
    .eq('politician_id', politician_id)\
    .eq('sentiment', 'negative')\
    .execute()

negative_data = negative_result.data

print(f'ë¶€ì •ì  ë°ì´í„° ì´ ê°œìˆ˜: {len(negative_data)}ê°œ')
print()

# URL ì¤‘ë³µ ì²´í¬
print('[2-1] URL ì¤‘ë³µ ê²€ì‚¬:')
print('-'*100)

urls = [item.get('source_url', '') for item in negative_data if item.get('source_url')]
url_counts = Counter(urls)
duplicates = {url: count for url, count in url_counts.items() if count > 1}

if duplicates:
    print(f'âš ï¸ ì¤‘ë³µ URL ë°œê²¬: {len(duplicates)}ê°œ')
    print()
    for url, count in sorted(duplicates.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f'  [{count}ë²ˆ] {url[:80]}...')
        # í•´ë‹¹ URLì„ ìˆ˜ì§‘í•œ AIë“¤ í™•ì¸
        same_url_items = [item for item in negative_data if item.get('source_url') == url]
        collectors = [item.get('collector_ai') for item in same_url_items]
        categories = [item.get('category') for item in same_url_items]
        print(f'    ìˆ˜ì§‘ ì±„ë„: {", ".join(collectors)}')
        print(f'    ì¹´í…Œê³ ë¦¬: {", ".join(categories)}')
        print()
else:
    print('âœ… URL ì¤‘ë³µ ì—†ìŒ')
    print()

print('='*100)
print()

# ì œëª© ì¤‘ë³µ/ìœ ì‚¬ë„ ì²´í¬
print('[2-2] ì œëª© í‚¤ì›Œë“œ ì¤‘ë³µ ê²€ì‚¬ (ì£¼ìš” ì˜í˜¹):')
print('-'*100)

titles = [item.get('title', '') for item in negative_data]

# ì£¼ìš” ì˜í˜¹ í‚¤ì›Œë“œ
keywords = [
    'ì¬ì‚°', 'ì˜í˜¹', 'ë…¼ë€', 'ì²­ë¬¸íšŒ', 'ë¹„ë¦¬',
    'ëª¨ì¹œ', 'ë¹Œë¼', 'ì „ì„¸', 'í•™ìœ„', 'ìë…€',
    'ìœ í•™ë¹„', 'ì •ì¹˜ìê¸ˆ', 'ìŠ¤í™', 'ì…ë²•ê¶Œ'
]

keyword_counts = {}
for keyword in keywords:
    count = sum(1 for title in titles if keyword in title)
    if count > 0:
        keyword_counts[keyword] = count

print(f'{"í‚¤ì›Œë“œ":<15} {"ì–¸ê¸‰íšŸìˆ˜":>12} {"ë¹„ìœ¨":>10}')
print('-'*100)
for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True):
    pct = (count / len(negative_data) * 100) if negative_data else 0
    print(f'{keyword:<15} {count:>12} {pct:>9.1f}%')
print('='*100)
print()

# 3. ê°™ì€ ë‚´ìš© ì¤‘ë³µ ì²´í¬ (ì œëª© ìœ ì‚¬ë„)
print('[2-3] ì œëª© ì™„ì „ ì¼ì¹˜ ì¤‘ë³µ ê²€ì‚¬:')
print('-'*100)

title_counts = Counter(titles)
duplicate_titles = {title: count for title, count in title_counts.items() if count > 1}

if duplicate_titles:
    print(f'âš ï¸ ì¤‘ë³µ ì œëª© ë°œê²¬: {len(duplicate_titles)}ê°œ')
    print()
    for title, count in sorted(duplicate_titles.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f'  [{count}ë²ˆ] {title}')
        # í•´ë‹¹ ì œëª©ì„ ê°€ì§„ í•­ëª©ë“¤
        same_title_items = [item for item in negative_data if item.get('title') == title]
        collectors = [item.get('collector_ai') for item in same_title_items]
        categories = [item.get('category') for item in same_title_items]
        data_types = [item.get('data_type') for item in same_title_items]
        print(f'    ìˆ˜ì§‘ ì±„ë„: {", ".join(collectors)}')
        print(f'    ì¹´í…Œê³ ë¦¬: {", ".join(set(categories))}')
        print(f'    ë°ì´í„° íƒ€ì…: {", ".join(set(data_types))}')
        print()
else:
    print('âœ… ì œëª© ì™„ì „ ì¼ì¹˜ ì¤‘ë³µ ì—†ìŒ')
    print()

print('='*100)
print()

# 4. AIë³„ ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë°ì´í„° ë¶„í¬
print('[2-4] AIë³„ Ã— ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë°ì´í„° ë¶„í¬:')
print('-'*100)

categories = ['expertise', 'leadership', 'vision', 'integrity', 'ethics',
              'accountability', 'transparency', 'communication', 'responsiveness', 'publicinterest']
ais = ['Gemini', 'Naver']

print(f'{"ì¹´í…Œê³ ë¦¬":<20} {"Gemini":>8} {"Naver":>8} {"í•©ê³„":>8}')
print('-'*100)

for cat in categories:
    cat_negative = [item for item in negative_data if item.get('category') == cat]
    ai_counts = {}
    for ai in ais:
        count = sum(1 for item in cat_negative if item.get('collector_ai') == ai)
        ai_counts[ai] = count
    total = sum(ai_counts.values())
    print(f'{cat:<20} {ai_counts.get("Gemini", 0):>8} {ai_counts.get("Naver", 0):>8} {total:>8}')

print('='*100)
print()

# 5. í‰ê°€ ê²°ê³¼ì—ì„œ ë¶€ì • ë“±ê¸‰ ë¶„í¬
print('ğŸ“Š [3] í‰ê°€ ê²°ê³¼ ë¶€ì • ë“±ê¸‰ ë¶„í¬:')
print('-'*100)

eval_result = supabase.table('evaluations_v40')\
    .select('rating, evaluator_ai, category')\
    .eq('politician_id', politician_id)\
    .execute()

eval_data = eval_result.data
ratings = [item['rating'] for item in eval_data]
rating_counts = Counter(ratings)

negative_ratings = ['-1', '-2', '-3', '-4']
positive_ratings = ['+1', '+2', '+3', '+4']

neg_count = sum(rating_counts.get(r, 0) for r in negative_ratings)
pos_count = sum(rating_counts.get(r, 0) for r in positive_ratings)

print(f'{"ë“±ê¸‰":<10} {"ê°œìˆ˜":>10} {"ë¹„ìœ¨":>10}')
print('-'*100)
for rating in ['+4', '+3', '+2', '+1', '-1', '-2', '-3', '-4']:
    count = rating_counts.get(rating, 0)
    pct = (count / len(eval_data) * 100) if eval_data else 0
    marker = ' âœ…' if rating in positive_ratings else ' âŒ'
    print(f'{rating:<10} {count:>10} {pct:>9.1f}%{marker}')
print('-'*100)
print(f'{"ê¸ì •í‰ê°€":<10} {pos_count:>10} {pos_count/len(eval_data)*100:>9.1f}%')
print(f'{"ë¶€ì •í‰ê°€":<10} {neg_count:>10} {neg_count/len(eval_data)*100:>9.1f}%')
print('='*100)
print()

# 6. íŠ¹ì • ìˆ˜ì§‘ ë°ì´í„°ê°€ ì—¬ëŸ¬ AIì—ê²Œ ì¤‘ë³µ í‰ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
print('ğŸ“Š [4] ìˆ˜ì§‘ ë°ì´í„°ë³„ í‰ê°€ íšŸìˆ˜ ê²€ì‚¬:')
print('-'*100)

eval_counts = Counter(item['collected_data_id'] for item in eval_data if item.get('collected_data_id'))

# ê° ë°ì´í„°ëŠ” 4ë²ˆì”© í‰ê°€ë˜ì–´ì•¼ í•¨ (4ê°œ í‰ê°€ AI: Claude, ChatGPT, Gemini, Grok)
expected_count = 4
abnormal = {data_id: count for data_id, count in eval_counts.items() if count != expected_count}

if abnormal:
    print(f'âš ï¸ ë¹„ì •ìƒ í‰ê°€ íšŸìˆ˜ ë°œê²¬: {len(abnormal)}ê°œ')
    print()
    for data_id, count in list(abnormal.items())[:10]:
        print(f'  Data ID: {data_id}')
        print(f'  í‰ê°€ íšŸìˆ˜: {count}íšŒ (ì •ìƒ: 4íšŒ)')

        # í•´ë‹¹ ë°ì´í„° ì •ë³´
        data_info = next((item for item in total_data if item['id'] == data_id), None)
        if data_info:
            print(f'  ì œëª©: {data_info.get("title", "N/A")[:60]}...')
            print(f'  sentiment: {data_info.get("sentiment", "N/A")}')
        print()
else:
    print('âœ… ëª¨ë“  ë°ì´í„°ê°€ ì •í™•íˆ 4ë²ˆì”© í‰ê°€ë¨')
    print()

print('='*100)
print()

# 7. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
print('ğŸ“Š [5] ì¢…í•© ë¶„ì„ ê²°ê³¼:')
print('='*100)
print()
print('âœ… ì •ìƒ ì‚¬í•­:')
if not duplicates:
    print('  - URL ì¤‘ë³µ ì—†ìŒ')
if not duplicate_titles:
    print('  - ì œëª© ì™„ì „ ì¼ì¹˜ ì¤‘ë³µ ì—†ìŒ')
if not abnormal:
    print('  - ëª¨ë“  ë°ì´í„° ì •í™•íˆ 4ë²ˆì”© í‰ê°€ë¨ (4ê°œ í‰ê°€ AI: Claude, ChatGPT, Gemini, Grok)')
print()

if len(negative_data) > 0:
    neg_ratio = len(negative_data) / len(total_data) * 100
    print('âš ï¸ ì£¼ì˜ ì‚¬í•­:')
    print(f'  - ë¶€ì •ì  ë°ì´í„° ë¹„ìœ¨: {len(negative_data)}ê°œ / {len(total_data)}ê°œ = {neg_ratio:.1f}%')

    if neg_ratio > 30:
        print(f'    â†’ ë¶€ì • ë°ì´í„°ê°€ {neg_ratio:.1f}%ë¡œ ë†’ì€ í¸ì…ë‹ˆë‹¤.')
        print(f'    â†’ ì‹¤ì œ ì˜í˜¹/ë…¼ë€ì´ ë§ì•˜ê±°ë‚˜, ë¶€ì • ë°ì´í„° ìˆ˜ì§‘ì´ ê³¼ë‹¤í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')

    print()
    print('  - ì£¼ìš” ë¶€ì • ì´ìŠˆ:')
    for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f'    â€¢ {keyword}: {count}íšŒ ì–¸ê¸‰')

print()
print('ğŸ’¡ ì ìˆ˜ê°€ ë‚®ì€ ì£¼ìš” ì›ì¸:')
neg_eval_ratio = neg_count / len(eval_data) * 100 if eval_data else 0
print(f'  - ì „ì²´ í‰ê°€ ì¤‘ ë¶€ì • í‰ê°€: {neg_count}ê°œ ({neg_eval_ratio:.1f}%)')
print(f'  - ì²­ë ´ì„± ì¹´í…Œê³ ë¦¬ í‰ê· : 0.10 (ê±°ì˜ 0ì ì— ê°€ê¹Œì›€)')
print(f'  - ìœ¤ë¦¬ì„± ì¹´í…Œê³ ë¦¬ í‰ê· : 0.58 (ë‚®ìŒ)')
print(f'  - íˆ¬ëª…ì„± ì¹´í…Œê³ ë¦¬ í‰ê· : 0.94 (ë‚®ìŒ)')
print()
print('  â†’ ë¶€ì • ë°ì´í„° ì¤‘ë³µë³´ë‹¤ëŠ”, ì‹¤ì œë¡œ ë¶€ì •ì  í‰ê°€ê°€ ë§ì´ í¬í•¨ë˜ì—ˆìŒ')
print('  â†’ íŠ¹íˆ ì²­ë ´ì„±/ìœ¤ë¦¬ì„± ê´€ë ¨ ì˜í˜¹ì´ ì ìˆ˜ë¥¼ í¬ê²Œ í•˜ë½ì‹œí‚´')
print()
print('='*100)

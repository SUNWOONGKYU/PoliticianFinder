#!/usr/bin/env python3
"""
V40 ì¹´í…Œê³ ë¦¬ë³„ ì´ìƒì¹˜ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
AI ê°„ ì ìˆ˜ ì°¨ì´ê°€ í° ì¹´í…Œê³ ë¦¬ì˜ ì›ë³¸ ë°ì´í„° ë¶„ì„
"""

import os
import sys
from pathlib import Path
from collections import defaultdict
import statistics

# V40 ë£¨íŠ¸ë¡œ ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
V40_ROOT = SCRIPT_DIR.parent.parent
sys.path.insert(0, str(V40_ROOT))

from dotenv import load_dotenv
load_dotenv(V40_ROOT / '.env')

from supabase import create_client

# UTF-8 ì¶œë ¥ ì„¤ì • (Windows)
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_KEY')
)

AIS = ['Claude', 'ChatGPT', 'Gemini', 'Grok']

# í¸ì°¨ê°€ í° ì¹´í…Œê³ ë¦¬ (ìƒìœ„ 5ê°œ)
HIGH_VARIANCE_CATEGORIES = [
    ('publicinterest', 'ê³µìµì„±', 13.5),
    ('transparency', 'íˆ¬ëª…ì„±', 13.0),
    ('responsiveness', 'ëŒ€ì‘ì„±', 13.0),
    ('vision', 'ë¹„ì „', 12.0),
    ('accountability', 'ì±…ì„ê°', 12.0)
]

def get_category_evaluations(category):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  í‰ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    result = supabase.table('evaluations_v40')\
        .select('*')\
        .eq('category', category)\
        .execute()
    return result.data

def analyze_category(category, category_kr, expected_variance):
    """ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„"""

    print("=" * 100)
    print(f"ğŸ“Š {category_kr} ({category}) - ì˜ˆìƒ í¸ì°¨: {expected_variance}ì ")
    print("=" * 100)

    evals = get_category_evaluations(category)

    if not evals:
        print("âŒ í‰ê°€ ë°ì´í„° ì—†ìŒ\n")
        return

    # AIë³„ ë°ì´í„° ë¶„ë¥˜
    ai_data = defaultdict(list)
    ai_excluded = defaultdict(int)
    ai_total = defaultdict(int)

    for ev in evals:
        ai = ev['evaluator_ai']
        rating = ev['rating']
        ai_total[ai] += 1

        if rating == 'X':
            ai_excluded[ai] += 1
        else:
            try:
                score = int(rating)
                ai_data[ai].append(score)
            except:
                print(f"âš ï¸  ë³€í™˜ ì‹¤íŒ¨: {rating} (AI: {ai}, ID: {ev['id']})")

    print(f"\nğŸ“ˆ 1. ë°ì´í„° ê°œìˆ˜ ë° ì œì™¸ìœ¨")
    print("-" * 100)
    print(f"{'AI':<12} {'ì „ì²´':<8} {'í‰ê°€':<8} {'ì œì™¸(X)':<8} {'ì œì™¸ìœ¨':<10}")
    print("-" * 100)

    for ai in AIS:
        total = ai_total.get(ai, 0)
        evaluated = len(ai_data.get(ai, []))
        excluded = ai_excluded.get(ai, 0)
        exc_rate = (excluded / total * 100) if total > 0 else 0

        print(f"{ai:<12} {total:<8} {evaluated:<8} {excluded:<8} {exc_rate:<10.1f}%")

    # ë“±ê¸‰ ë¶„í¬
    print(f"\nğŸ“Š 2. ë“±ê¸‰ ë¶„í¬")
    print("-" * 100)
    print(f"{'AI':<12}", end="")
    for rating in ['+4', '+3', '+2', '+1', '-1', '-2', '-3', '-4']:
        print(f"{rating:>6}", end="")
    print(f"{'í‰ê· ':>8}")
    print("-" * 100)

    ai_averages = {}

    for ai in AIS:
        scores = ai_data.get(ai, [])
        if not scores:
            print(f"{ai:<12} (ë°ì´í„° ì—†ìŒ)")
            continue

        print(f"{ai:<12}", end="")

        # ë“±ê¸‰ë³„ ê°œìˆ˜
        for rating in [4, 3, 2, 1, -1, -2, -3, -4]:
            count = scores.count(rating)
            pct = (count / len(scores) * 100) if scores else 0
            print(f"{count:>3}({pct:>2.0f}%)", end=" ")

        # í‰ê· 
        avg = statistics.mean(scores)
        ai_averages[ai] = avg
        print(f"{avg:>7.2f}")

    # í†µê³„ ë¶„ì„
    print(f"\nğŸ“ˆ 3. í†µê³„ ë¶„ì„")
    print("-" * 100)
    print(f"{'AI':<12} {'í‰ê· ':<10} {'ì¤‘ì•™ê°’':<10} {'í‘œì¤€í¸ì°¨':<10} {'ìµœì†Œ':<8} {'ìµœëŒ€':<8}")
    print("-" * 100)

    for ai in AIS:
        scores = ai_data.get(ai, [])
        if not scores:
            continue

        avg = statistics.mean(scores)
        median = statistics.median(scores)
        stdev = statistics.stdev(scores) if len(scores) > 1 else 0
        min_score = min(scores)
        max_score = max(scores)

        print(f"{ai:<12} {avg:<10.2f} {median:<10.1f} {stdev:<10.2f} {min_score:<8} {max_score:<8}")

    # AI ê°„ í¸ì°¨
    if len(ai_averages) >= 2:
        print(f"\nğŸ“Š 4. AI ê°„ ì°¨ì´")
        print("-" * 100)

        sorted_ais = sorted(ai_averages.items(), key=lambda x: x[1], reverse=True)

        print("ìˆœìœ„:")
        for i, (ai, avg) in enumerate(sorted_ais, 1):
            print(f"  {i}. {ai:<10}: {avg:>+6.2f}")

        highest = sorted_ais[0]
        lowest = sorted_ais[-1]
        actual_diff = highest[1] - lowest[1]

        print(f"\nìµœê³ : {highest[0]} ({highest[1]:>+6.2f})")
        print(f"ìµœì €: {lowest[0]} ({lowest[1]:>+6.2f})")
        print(f"ì‹¤ì œ í¸ì°¨: {actual_diff:.2f}ì ")
        print(f"ì˜ˆìƒ í¸ì°¨: {expected_variance}ì ")

        # ì ìˆ˜ í™˜ì‚° (rating â†’ category_score)
        # category_score = (6.0 + avg_score * 0.5) * 10
        highest_cat_score = (6.0 + highest[1] * 0.5) * 10
        lowest_cat_score = (6.0 + lowest[1] * 0.5) * 10
        cat_diff = highest_cat_score - lowest_cat_score

        print(f"\nì¹´í…Œê³ ë¦¬ ì ìˆ˜ í™˜ì‚°:")
        print(f"  ìµœê³ : {highest_cat_score:.1f}ì ")
        print(f"  ìµœì €: {lowest_cat_score:.1f}ì ")
        print(f"  í™˜ì‚° í¸ì°¨: {cat_diff:.1f}ì ")

        # ê²€ì¦
        print(f"\nğŸ” ê²€ì¦:")
        if abs(cat_diff - expected_variance) < 1.0:
            print(f"  âœ… ì •ìƒ (í™˜ì‚° í¸ì°¨ {cat_diff:.1f}ì  â‰ˆ ì˜ˆìƒ í¸ì°¨ {expected_variance}ì )")
        else:
            print(f"  âš ï¸  ë¶ˆì¼ì¹˜ (í™˜ì‚° í¸ì°¨ {cat_diff:.1f}ì  â‰  ì˜ˆìƒ í¸ì°¨ {expected_variance}ì )")

    # ì´ìƒì¹˜ ê²€ì¶œ
    print(f"\nğŸ” 5. ì´ìƒì¹˜ ê²€ì¶œ")
    print("-" * 100)

    anomalies_found = False

    for ai in AIS:
        scores = ai_data.get(ai, [])
        if not scores or len(scores) < 3:
            continue

        avg = statistics.mean(scores)
        stdev = statistics.stdev(scores)

        # í‰ê·  Â± 2*í‘œì¤€í¸ì°¨ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’
        outliers = [s for s in scores if abs(s - avg) > 2 * stdev]

        if outliers:
            anomalies_found = True
            print(f"\n{ai}:")
            print(f"  ì´ìƒì¹˜ ê°œìˆ˜: {len(outliers)}ê°œ")
            print(f"  ì´ìƒì¹˜ ê°’: {sorted(set(outliers))}")
            print(f"  í‰ê· : {avg:.2f}, í‘œì¤€í¸ì°¨: {stdev:.2f}")

    if not anomalies_found:
        print("  âœ… ì´ìƒì¹˜ ì—†ìŒ")

    # ì •ì¹˜ì¸ë³„ ë¶„ì„
    print(f"\nğŸ‘¤ 6. ì •ì¹˜ì¸ë³„ AI í‰ê°€")
    print("-" * 100)

    pol_data = defaultdict(lambda: defaultdict(list))

    for ev in evals:
        pol_id = ev['politician_id']
        ai = ev['evaluator_ai']
        rating = ev['rating']

        if rating != 'X':
            try:
                score = int(rating)
                pol_data[pol_id][ai].append(score)
            except:
                pass

    politicians = {
        '8c5dcc89': 'ë°•ì£¼ë¯¼',
        'd0a5d6e1': 'ì¡°ì€í¬'
    }

    for pol_id, pol_name in politicians.items():
        if pol_id not in pol_data:
            continue

        print(f"\n[{pol_name}]")

        pol_ai_avgs = []
        for ai in AIS:
            if pol_data[pol_id][ai]:
                avg = statistics.mean(pol_data[pol_id][ai])
                count = len(pol_data[pol_id][ai])
                pol_ai_avgs.append((ai, avg, count))

        pol_ai_avgs.sort(key=lambda x: x[1], reverse=True)

        for ai, avg, count in pol_ai_avgs:
            print(f"  {ai:<10}: {avg:>+6.2f} ({count}ê°œ)")

        if len(pol_ai_avgs) >= 2:
            highest = pol_ai_avgs[0]
            lowest = pol_ai_avgs[-1]
            gap = highest[1] - lowest[1]
            print(f"  í¸ì°¨: {gap:.2f}ì  (ìµœê³  {highest[0]} vs ìµœì € {lowest[0]})")

    print("\n")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    print("=" * 100)
    print("V40 ì¹´í…Œê³ ë¦¬ë³„ ì´ìƒì¹˜ ê²€ì¦")
    print("í¸ì°¨ê°€ í° ìƒìœ„ 5ê°œ ì¹´í…Œê³ ë¦¬ ë¶„ì„")
    print("=" * 100)
    print()

    for category, category_kr, variance in HIGH_VARIANCE_CATEGORIES:
        analyze_category(category, category_kr, variance)

    print("=" * 100)
    print("âœ… ë¶„ì„ ì™„ë£Œ")
    print("=" * 100)

if __name__ == '__main__':
    main()

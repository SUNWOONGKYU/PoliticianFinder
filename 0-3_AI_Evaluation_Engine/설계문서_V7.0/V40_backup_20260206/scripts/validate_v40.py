# -*- coding: utf-8 -*-
"""
V40 ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (ìˆ˜ì • ë²„ì „)

í•µì‹¬ ë³€ê²½ì‚¬í•­:
1. validate_event_date ì œê±° (ê³¼ê±° ì—°ë„ ì–¸ê¸‰ ì˜¤íŒ ë¬¸ì œ)
2. URL ê²€ì¦ ê°œì„  (timeout 30ì´ˆ, 3íšŒ ì¬ì‹œë„, ì—ëŸ¬ ì„¸ë¶„í™”)
3. published_dateë§Œìœ¼ë¡œ ê¸°ê°„ ê²€ì¦ (ê°„ë‹¨, 95% ì •í™•)
4. ê°€ì§œ URL íŒ¨í„´ ì²´í¬
5. ì¤‘ë³µ ë°ì´í„° ì œê±° (ê°™ì€ AI + ê°™ì€ URL)

ê²€ì¦ ì›ì¹™:
- ìµœì†Œ ê²€ì¦ (ëª…ë°±í•œ ë¶ˆëŸ‰ë§Œ ì œê±°)
- AI í‰ê°€ ë‹¨ê³„ì—ì„œ ìµœì¢… í’ˆì§ˆ íŒë‹¨
- ì˜¤íŒ ìµœì†Œí™”

í”„ë¡œì„¸ìŠ¤:
[1] ê²€ì¦ (validate): ìˆ˜ì§‘ëœ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    - ì¤‘ë³µ ë°œê²¬ ì‹œ ìë™ ì‚­ì œ (evaluations í¬í•¨)
[2] ì¬ìˆ˜ì§‘ (recollect): ê²€ì¦ ì‹¤íŒ¨ë¶„ í•´ë‹¹ AIë¡œ ì¬ìˆ˜ì§‘
[3] ì¬ê²€ì¦: ì¬ìˆ˜ì§‘ ë°ì´í„° ë‹¤ì‹œ ê²€ì¦

ì‚¬ìš©ë²•:
    # ì „ì²´ ê²€ì¦ + ì¬ìˆ˜ì§‘
    python validate_v40.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --mode=all

    # ê²€ì¦ë§Œ
    python validate_v40.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --mode=validate

    # ì¬ìˆ˜ì§‘ë§Œ
    python validate_v40.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --mode=recollect

    # íŠ¹ì • AIë§Œ
    python validate_v40.py --politician_id=62e7b453 --politician_name="ì˜¤ì„¸í›ˆ" --ai=Gemini
"""

import os
import sys
import json
import re
import argparse
import time
import requests
from datetime import datetime, timedelta
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from supabase import create_client
from dotenv import load_dotenv
from duplicate_check_utils import normalize_url, normalize_title, is_duplicate_by_url, is_duplicate_by_title
# validate_event_date ì œê±° (ê³¼ê±° ì—°ë„ ì–¸ê¸‰ ì˜¤íŒ ë¬¸ì œ)

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except AttributeError:
        # ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆê±°ë‚˜ bufferê°€ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
        pass

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

# V40 í…Œì´ë¸”ëª…
TABLE_COLLECTED_DATA = "collected_data_v40"
TABLE_EVALUATIONS = "evaluations_v40"

# ê³µì‹ ë°ì´í„° ë„ë©”ì¸ (OFFICIALë¡œ ì¸ì •ë˜ëŠ” ë„ë©”ì¸)
OFFICIAL_DOMAINS = [
    "assembly.go.kr",
    "likms.assembly.go.kr",
    "mois.go.kr",
    "korea.kr",
    "nec.go.kr",
    "bai.go.kr",
    "pec.go.kr",
    "scourt.go.kr",
    "nesdc.go.kr",
    "manifesto.or.kr",
    "peoplepower21.org",
    "theminjoo.kr",
    "seoul.go.kr",
    "gg.go.kr",
    "busan.go.kr",
    "incheon.go.kr",
    "daegu.go.kr",
    "daejeon.go.kr",
    "gwangju.go.kr",
    "ulsan.go.kr",
    "sejong.go.kr",
    "open.go.kr",
    "acrc.go.kr",
    "humanrights.go.kr"
]

# SNS ë„ë©”ì¸ (URL ê²€ì¦ ì œì™¸)
SNS_DOMAINS = [
    "twitter.com",
    "x.com",
    "facebook.com",
    "instagram.com",
    "youtube.com",
    "youtu.be",
    "tiktok.com"
]

# ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES = [
    ("expertise", "ì „ë¬¸ì„±"),
    ("leadership", "ë¦¬ë”ì‹­"),
    ("vision", "ë¹„ì „"),
    ("integrity", "ì²­ë ´ì„±"),
    ("ethics", "ìœ¤ë¦¬ì„±"),
    ("accountability", "ì±…ì„ê°"),
    ("transparency", "íˆ¬ëª…ì„±"),
    ("communication", "ì†Œí†µëŠ¥ë ¥"),
    ("responsiveness", "ëŒ€ì‘ì„±"),
    ("publicinterest", "ê³µìµì„±")
]

# ê²€ì¦ ê²°ê³¼ ì½”ë“œ
VALIDATION_CODES = {
    "VALID": "ìœ íš¨",
    "INVALID_URL": "URL ì ‘ì† ë¶ˆê°€",
    "EMPTY_URL": "URL ë¹„ì–´ìˆìŒ",
    "FAKE_URL": "ê°€ì§œ URL íŒ¨í„´",
    "WRONG_SOURCE_TYPE": "source_type ë¶ˆì¼ì¹˜",
    "MISSING_FIELD": "í•„ìˆ˜ í•„ë“œ ëˆ„ë½",
    "DATE_OUT_OF_RANGE": "ê¸°ê°„ ì´ˆê³¼",
    "DUPLICATE": "ì¤‘ë³µ ë°ì´í„°"
}


def get_date_range():
    """V40 ê¸°ê°„ ì œí•œ ê³„ì‚°"""
    evaluation_date = datetime.now()
    official_start = evaluation_date - timedelta(days=365*4)  # 4ë…„
    public_start = evaluation_date - timedelta(days=365*2)    # 2ë…„

    return {
        'official_start': official_start,
        'official_end': evaluation_date,
        'public_start': public_start,
        'public_end': evaluation_date,
    }


def is_sns_url(url):
    """SNS URL ì—¬ë¶€ í™•ì¸"""
    if not url:
        return False
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
        return any(sns in domain for sns in SNS_DOMAINS)
    except:
        return False


def is_official_domain(url):
    """ê³µì‹ ë„ë©”ì¸ ì—¬ë¶€ í™•ì¸"""
    if not url:
        return False
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
        return any(official in domain for official in OFFICIAL_DOMAINS)
    except:
        return False


def is_fake_url_pattern(url):
    """ê°€ì§œ URL íŒ¨í„´ ê°ì§€"""
    if not url:
        return True

    fake_patterns = [
        r'example\.com',
        r'test\.com',
        r'localhost',
        r'^https?://\d+\.\d+\.\d+\.\d+',  # IP ì£¼ì†Œ URL
        r'dummy',              # dummy URL
    ]

    for pattern in fake_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return True

    return False


def check_url_exists(url, timeout=30, max_retries=3):
    """
    URL ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ê°œì„  ë²„ì „)

    ë³€ê²½ì‚¬í•­:
    - timeout: 10ì´ˆ â†’ 30ì´ˆ (ì •ë¶€ ì‚¬ì´íŠ¸ ëŒ€ì‘)
    - ì¬ì‹œë„: 3íšŒ (ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì • ëŒ€ì‘)
    - ì—ëŸ¬ ì„¸ë¶„í™”: 403/401ì€ ì ‘ê·¼ ì œí•œì´ì§€ ë¶ˆëŸ‰ ì•„ë‹˜
    """
    if not url or url.strip() == '':
        return False, "EMPTY_URL"

    # SNSëŠ” ê²€ì¦ ì œì™¸ (ì ‘ê·¼ ì œí•œ ìˆìŒ)
    if is_sns_url(url):
        return True, "VALID"

    # ê°€ì§œ URL íŒ¨í„´ ì²´í¬
    if is_fake_url_pattern(url):
        return False, "FAKE_URL"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    # ì¬ì‹œë„ ë¡œì§
    for attempt in range(max_retries):
        try:
            # HEAD ìš”ì²­ ë¨¼ì € ì‹œë„
            try:
                response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)

                # 403/401ì€ ì ‘ê·¼ ì œí•œì´ì§€ URL ë¶ˆëŸ‰ ì•„ë‹˜
                if response.status_code in [401, 403]:
                    return True, "VALID"

                if response.status_code < 400:
                    return True, "VALID"
            except:
                pass

            # GET ì‹œë„
            response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)

            # 403/401ì€ ì ‘ê·¼ ì œí•œ
            if response.status_code in [401, 403]:
                return True, "VALID"

            if response.status_code < 400:
                return True, "VALID"

            # 404ëŠ” ì¬ì‹œë„ ë¶ˆí•„ìš”
            if response.status_code == 404:
                return False, "NOT_FOUND"

            # 5xxëŠ” ì¬ì‹œë„
            if 500 <= response.status_code < 600:
                if attempt < max_retries - 1:
                    time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    continue
                return False, "SERVER_ERROR"

            return False, "INVALID_URL"

        except requests.exceptions.Timeout:
            # timeoutì€ ì¬ì‹œë„
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            return False, "TIMEOUT"

        except requests.exceptions.ConnectionError:
            # ì—°ê²° ì—ëŸ¬ë„ ì¬ì‹œë„
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            return False, "CONNECTION_ERROR"

        except Exception as e:
            # ê¸°íƒ€ ì—ëŸ¬
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
            return False, "INVALID_URL"

    return False, "MAX_RETRIES"


def validate_source_type(item):
    """source_type ê·œì¹™ ê²€ì¦

    V40 ë³€ê²½: ë„ë©”ì¸ ê¸°ë°˜ ê²€ì¦ ì œê±°
    - V40ì—ì„œ OFFICIAL/PUBLICì€ 'ë°ì´í„° ì„±ê²©'ì´ì§€ 'ë„ë©”ì¸'ì´ ì•„ë‹˜
    - OFFICIAL = ê³µì‹ í™œë™ ê´€ë ¨ ë°ì´í„° (ë‰´ìŠ¤ì—ì„œ ë³´ë„í•´ë„ OFFICIAL)
    - PUBLIC = ì—¬ë¡ /í‰íŒ ê´€ë ¨ ë°ì´í„°
    - ìˆ˜ì§‘ í”„ë¡¬í”„íŠ¸ê°€ ì†ŒìŠ¤ ìœ í˜•ì„ ì œì–´í•˜ë¯€ë¡œ ë„ë©”ì¸ ê²€ì¦ ë¶ˆí•„ìš”
    """
    return True, "VALID"


def validate_required_fields(item):
    """í•„ìˆ˜ í•„ë“œ ê²€ì¦"""
    required = ['title', 'content', 'source_url']

    for field in required:
        value = item.get(field, '')
        if not value or str(value).strip() == '':
            # source_urlì´ ë¹„ì–´ìˆì–´ë„ ì˜ˆì™¸ í—ˆìš©
            return False, "MISSING_FIELD"

    return True, "VALID"


def validate_date_range(item):
    """ê¸°ê°„ ì œí•œ ê²€ì¦"""
    date_range = get_date_range()
    data_type = item.get('data_type', 'public').lower()
    pub_date_str = item.get('published_date')

    if not pub_date_str:
        return True, "VALID"  # ë‚ ì§œ ì—†ìœ¼ë©´ íŒ¨ìŠ¤

    try:
        if isinstance(pub_date_str, str):
            pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')
        else:
            pub_date = pub_date_str

        if data_type == 'official':
            if pub_date < date_range['official_start']:
                return False, "DATE_OUT_OF_RANGE"
        else:
            if pub_date < date_range['public_start']:
                return False, "DATE_OUT_OF_RANGE"

        return True, "VALID"

    except:
        return True, "VALID"  # íŒŒì‹± ì‹¤íŒ¨ë©´ íŒ¨ìŠ¤


def check_duplicate(item):
    """ì¤‘ë³µ ë°ì´í„° ê²€ì¦ (URL + ì œëª© ê¸°ë°˜)

    ê·œì¹™: ê°™ì€ AIê°€ ê°™ì€ URL ë˜ëŠ” ìœ ì‚¬í•œ ì œëª©ì„ ì´ë¯¸ ìˆ˜ì§‘í–ˆëŠ”ì§€ í™•ì¸
    - ê°™ì€ politician_id + ê°™ì€ collector_ai + (ê°™ì€ URL OR ìœ ì‚¬í•œ ì œëª©) = ì¤‘ë³µ
    - ë‹¤ë¥¸ AIê°€ ê°™ì€ URL/ì œëª© ìˆ˜ì§‘ = ì¤‘ë³µ ì•„ë‹˜ (ìì—° ê°€ì¤‘ì¹˜)
    """
    politician_id = item.get('politician_id')
    collector_ai = item.get('collector_ai')
    source_url = item.get('source_url', '')
    title = item.get('title', '')
    item_id = item.get('id')  # ìê¸° ìì‹  ì œì™¸ìš©

    # URL ì •ê·œí™” (íŒŒë¼ë¯¸í„°, ì•µì»¤ ì œê±°)
    source_url_normalized = normalize_url(source_url) if source_url else ''

    # ì œëª© ì •ê·œí™”
    title_normalized = normalize_title(title) if title else ''

    if not source_url_normalized and not title_normalized:
        # URLë„ ì œëª©ë„ ì—†ìœ¼ë©´ ì¤‘ë³µ ì²´í¬ ë¶ˆê°€ëŠ¥
        return True, "VALID"

    try:
        # ê°™ì€ ì •ì¹˜ì¸, ê°™ì€ AIê°€ ìˆ˜ì§‘í•œ ë°ì´í„° ì¡°íšŒ
        query = supabase.table(TABLE_COLLECTED_DATA)\
            .select('id, source_url, title')\
            .eq('politician_id', politician_id)\
            .eq('collector_ai', collector_ai)

        # ìê¸° ìì‹  ì œì™¸
        if item_id:
            query = query.neq('id', item_id)

        existing = query.execute()

        if existing.data:
            for existing_item in existing.data:
                # URL ì¤‘ë³µ ì²´í¬
                if source_url_normalized:
                    if is_duplicate_by_url(source_url, existing_item.get('source_url', '')):
                        return False, "DUPLICATE"

                # ì œëª© ì¤‘ë³µ ì²´í¬ (95% ìœ ì‚¬ë„)
                if title_normalized:
                    if is_duplicate_by_title(title, existing_item.get('title', ''), threshold=0.95):
                        return False, "DUPLICATE"

        return True, "VALID"
    except Exception as e:
        print(f"  ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {e}")
        return True, "VALID"  # ì˜¤ë¥˜ ì‹œ íŒ¨ìŠ¤


def validate_item(item):
    """ë‹¨ì¼ í•­ëª© ì¢…í•© ê²€ì¦"""
    results = []

    # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
    valid, code = validate_required_fields(item)
    if not valid:
        return False, code

    # 2. URL ì¡´ì¬ ê²€ì¦
    url = item.get('source_url', '')
    collector_ai = item.get('collector_ai', '').lower()

    # V40 ê·œì •: Grokì˜ X/íŠ¸ìœ„í„° ë°ì´í„°ëŠ” URL í•„ìš” ì—†ìŒ
    if collector_ai == 'grok':
        # X/íŠ¸ìœ„í„°ëŠ” URL ì—†ìŒ ë˜ëŠ” "X/@ê³„ì •ëª…" í˜•ì‹ í—ˆìš©
        if not url or url.startswith('X/@') or url.startswith('x.com') or url.startswith('twitter.com'):
            pass  # ìœ íš¨ - URL ê²€ì¦ ê±´ë„ˆë›°ê¸°
        elif url and not is_sns_url(url):
            valid, code = check_url_exists(url)
            if not valid:
                return False, code
    else:
        # GeminiëŠ” ì •ìƒ URL ê²€ì¦ (PerplexityëŠ” ë³„ë„ ì²˜ë¦¬)
        if url and not is_sns_url(url):
            valid, code = check_url_exists(url)
            if not valid:
                return False, code

    # 3. source_type ê²€ì¦
    valid, code = validate_source_type(item)
    if not valid:
        return False, code

    # 4. ê¸°ê°„ ê²€ì¦ (published_date ê¸°ì¤€)
    valid, code = validate_date_range(item)
    if not valid:
        return False, code

    # 5. validate_event_date ì œê±°!
    # ì´ìœ : ê³¼ê±° ì—°ë„ ì–¸ê¸‰ì„ ì‚¬ê±´ ë°œìƒìœ¼ë¡œ ì˜¤íŒ (ì •í™•ë„ 30%)
    # ëŒ€ì•ˆ: published_dateë§Œ ì‚¬ìš© (ì •í™•ë„ 95%)
    #       + AI í‰ê°€ ë‹¨ê³„ì—ì„œ ë§¥ë½ ì´í•´ ë° ì ìˆ˜ ë¶€ì—¬

    # 6. ì¤‘ë³µ ê²€ì¦
    valid, code = check_duplicate(item)
    if not valid:
        return False, code

    return True, "VALID"


def get_collected_data(politician_id, ai_name=None, category=None):
    """ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©, 1000ê°œ limit íšŒí”¼)"""
    try:
        all_data = []
        offset = 0
        page_size = 1000

        while True:
            query = supabase.table(TABLE_COLLECTED_DATA)\
                .select('*')\
                .eq('politician_id', politician_id)

            if ai_name:
                query = query.eq('collector_ai', ai_name.lower())

            if category:
                query = query.eq('category', category.lower())

            result = query.range(offset, offset + page_size - 1).execute()
            if result.data:
                all_data.extend(result.data)
            if not result.data or len(result.data) < page_size:
                break
            offset += page_size

        return all_data

    except Exception as e:
        print(f"  âš ï¸ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


def validate_collected_data(politician_id, politician_name, ai_name=None, category=None):
    """ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦"""
    print(f"\n{'='*60}")
    print(f"[ê²€ì¦] {politician_name} ({politician_id})")
    print(f"{'='*60}")

    items = get_collected_data(politician_id, ai_name, category)
    print(f"ì´ {len(items)}ê°œ í•­ëª© ê²€ì¦ ì‹œì‘...")

    valid_count = 0
    invalid_items = []
    duplicate_removed = 0  # ì¤‘ë³µ ì œê±° ì¹´ìš´íŠ¸

    for i, item in enumerate(items):
        item_id = item.get('id')
        title = item.get('title', '')[:30]

        valid, code = validate_item(item)

        if valid:
            valid_count += 1
            # is_verified = Trueë¡œ ì—…ë°ì´íŠ¸
            try:
                supabase.table(TABLE_COLLECTED_DATA)\
                    .update({'is_verified': True})\
                    .eq('id', item_id)\
                    .execute()
            except:
                pass
        else:
            # âœ… V40 ì¤‘ë³µ ìë™ ì œê±°: DUPLICATEì¸ ê²½ìš° ì¦‰ì‹œ ì‚­ì œ
            if code == "DUPLICATE":
                try:
                    # collected_data ì‚­ì œ (evaluations_v40ì— collected_data_id ì—†ìŒ)
                    supabase.table(TABLE_COLLECTED_DATA)\
                        .delete()\
                        .eq('id', item_id)\
                        .execute()

                    duplicate_removed += 1

                    # ë¡œê·¸ (10ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰)
                    if duplicate_removed % 10 == 0 or (i + 1) == len(items):
                        print(f"  ì¤‘ë³µ ì œê±° ì§„í–‰: {duplicate_removed}ê°œ")
                except Exception as e:
                    print(f"  âš ï¸ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨ ({item_id[:8]}...): {e}")
            else:
                # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¬ìˆ˜ì§‘ ëŒ€ìƒìœ¼ë¡œ ì¶”ê°€
                invalid_items.append({
                    'id': item_id,
                    'title': title,
                    'code': code,
                    'reason': VALIDATION_CODES.get(code, code),
                    'collector_ai': item.get('collector_ai'),
                    'category': item.get('category'),
                    'data_type': item.get('data_type'),
                    'source_url': item.get('source_url', '')[:50]
                })

        # ì§„í–‰ë¥  í‘œì‹œ
        if (i + 1) % 100 == 0:
            print(f"  ì§„í–‰: {i+1}/{len(items)} ({valid_count}ê°œ ìœ íš¨)")

    invalid_count = len(invalid_items)
    print(f"\nê²€ì¦ ì™„ë£Œ:")
    print(f"  âœ… ìœ íš¨: {valid_count}ê°œ")
    print(f"  ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {duplicate_removed}ê°œ")
    print(f"  âŒ ë¬´íš¨ (ì¬ìˆ˜ì§‘ í•„ìš”): {invalid_count}ê°œ")

    # ë¬´íš¨ í•­ëª© ìƒì„¸
    if invalid_items:
        print(f"\në¬´íš¨ í•­ëª© ìƒì„¸:")
        # ì½”ë“œë³„ ì§‘ê³„
        code_counts = {}
        for item in invalid_items:
            code = item['code']
            code_counts[code] = code_counts.get(code, 0) + 1

        for code, count in sorted(code_counts.items(), key=lambda x: -x[1]):
            print(f"  - {VALIDATION_CODES.get(code, code)}: {count}ê°œ")

        # AIë³„ ì§‘ê³„
        print(f"\nAIë³„ ë¬´íš¨ í•­ëª©:")
        ai_counts = {}
        for item in invalid_items:
            ai = item.get('collector_ai', 'unknown')
            ai_counts[ai] = ai_counts.get(ai, 0) + 1

        for ai, count in sorted(ai_counts.items(), key=lambda x: -x[1]):
            print(f"  - {ai}: {count}ê°œ")

    return {
        'total': len(items),
        'valid': valid_count,
        'invalid': invalid_count,
        'invalid_items': invalid_items
    }


def delete_invalid_items(invalid_items):
    """ë¬´íš¨ í•­ëª© ì‚­ì œ"""
    if not invalid_items:
        return 0

    deleted = 0
    for item in invalid_items:
        try:
            supabase.table(TABLE_COLLECTED_DATA)\
                .delete()\
                .eq('id', item['id'])\
                .execute()
            deleted += 1
        except Exception as e:
            print(f"  âš ï¸ ì‚­ì œ ì‹¤íŒ¨ ({item['id']}): {e}")

    print(f"  ğŸ—‘ï¸ {deleted}ê°œ ë¬´íš¨ í•­ëª© ì‚­ì œ")
    return deleted


def recollect_for_ai(ai_name, politician_id, politician_name, category, count, data_type):
    """íŠ¹ì • AIë¡œ ì¬ìˆ˜ì§‘"""
    print(f"  [{ai_name}] {category} {data_type} {count}ê°œ ì¬ìˆ˜ì§‘ ì¤‘...")

    # collect_v40ì˜ í•¨ìˆ˜ ì„í¬íŠ¸
    try:
        from collect_v40 import (
            init_ai_client, build_prompt_v40,
            call_naver, call_claude_with_websearch,
            call_gemini_with_search, call_grok,
            parse_json_response, AI_CONFIGS
        )
    except ImportError:
        print(f"  âŒ collect_v40.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    client = init_ai_client(ai_name)

    # 20-20-60 ë¶„ë°°
    neg = int(count * 0.2)
    pos = int(count * 0.2)
    neu = count - neg - pos
    sentiment_dist = {"negative": neg, "positive": pos, "neutral": neu}

    # ì¹´í…Œê³ ë¦¬ í•œê¸€ëª… ì°¾ê¸°
    cat_korean = next((k for c, k in CATEGORIES if c == category), category)

    prompt = build_prompt_v40(
        politician_name, category, cat_korean,
        data_type, count, sentiment_dist, ai_name
    )

    # AIë³„ í˜¸ì¶œ
    if ai_name == "Naver":
        # Naver Search API (V40ì—ì„œ Perplexity ëŒ€ì²´)
        result = call_naver(client, prompt, data_type)
    elif ai_name == "Claude":
        result = call_claude_with_websearch(client, prompt)
    elif ai_name == "Gemini":
        result = call_gemini_with_search(client, prompt)
    elif ai_name == "Grok":
        result = call_grok(client, prompt)
    else:
        result = None

    if not result:
        print(f"    âŒ ì¬ìˆ˜ì§‘ ì‹¤íŒ¨")
        return 0

    items = parse_json_response(result)

    # DB ì €ì¥
    saved = 0
    for item in items:
        try:
            record = {
                'politician_id': politician_id,
                'politician_name': politician_name,
                'category': category,
                'data_type': data_type,
                'collector_ai': ai_name.lower(),
                'title': item.get('data_title', '')[:200],
                'content': item.get('data_content', '')[:2000],
                'source_url': item.get('source_url', ''),
                'source_name': item.get('data_source', ''),
                'published_date': item.get('data_date'),
                'sentiment': item.get('sentiment', 'neutral'),
                'is_verified': False
            }
            supabase.table(TABLE_COLLECTED_DATA).insert(record).execute()
            saved += 1
        except Exception as e:
            pass

    print(f"    âœ… {saved}ê°œ ì¬ìˆ˜ì§‘ ì™„ë£Œ")
    return saved


def recollect_invalid(politician_id, politician_name, invalid_items):
    """ë¬´íš¨ í•­ëª© ì¬ìˆ˜ì§‘"""
    if not invalid_items:
        print("ì¬ìˆ˜ì§‘í•  í•­ëª© ì—†ìŒ")
        return 0

    print(f"\n{'='*60}")
    print(f"[ì¬ìˆ˜ì§‘] {len(invalid_items)}ê°œ í•­ëª©")
    print(f"{'='*60}")

    # AI/ì¹´í…Œê³ ë¦¬/data_typeë³„ ê·¸ë£¹í•‘
    groups = {}
    for item in invalid_items:
        key = (item['collector_ai'], item['category'], item['data_type'])
        if key not in groups:
            groups[key] = 0
        groups[key] += 1

    total_recollected = 0

    for (ai_name, category, data_type), count in groups.items():
        if ai_name and category and data_type:
            recollected = recollect_for_ai(
                ai_name.capitalize(),
                politician_id,
                politician_name,
                category,
                count,
                data_type
            )
            total_recollected += recollected
            time.sleep(1)

    print(f"\nì¬ìˆ˜ì§‘ ì™„ë£Œ: {total_recollected}ê°œ")
    return total_recollected


def run_validation_pipeline(politician_id, politician_name, mode='all', ai_name=None, max_iterations=3):
    """ê²€ì¦ + ì¬ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸"""
    print(f"\n{'#'*60}")
    print(f"# V40 ê²€ì¦ íŒŒì´í”„ë¼ì¸: {politician_name}")
    print(f"# ëª¨ë“œ: {mode}")
    print(f"{'#'*60}")

    if mode == 'validate':
        # ê²€ì¦ë§Œ
        result = validate_collected_data(politician_id, politician_name, ai_name)
        return result

    elif mode == 'recollect':
        # ì¬ìˆ˜ì§‘ë§Œ (is_verified=Falseì¸ í•­ëª©)
        items = get_collected_data(politician_id, ai_name)
        invalid_items = [
            {
                'id': item['id'],
                'collector_ai': item.get('collector_ai'),
                'category': item.get('category'),
                'data_type': item.get('data_type')
            }
            for item in items if not item.get('is_verified', False)
        ]

        if invalid_items:
            delete_invalid_items(invalid_items)
            recollect_invalid(politician_id, politician_name, invalid_items)

    else:  # mode == 'all'
        # ê²€ì¦ â†’ ì‚­ì œ â†’ ì¬ìˆ˜ì§‘ â†’ ì¬ê²€ì¦ (ë°˜ë³µ)
        for iteration in range(1, max_iterations + 1):
            print(f"\n{'='*60}")
            print(f"[ë°˜ë³µ {iteration}/{max_iterations}]")
            print(f"{'='*60}")

            # 1. ê²€ì¦
            result = validate_collected_data(politician_id, politician_name, ai_name)

            if result['invalid'] == 0:
                print(f"\nâœ… ëª¨ë“  ë°ì´í„° ìœ íš¨! ê²€ì¦ ì™„ë£Œ.")
                break

            # 2. ë¬´íš¨ í•­ëª© ì‚­ì œ
            delete_invalid_items(result['invalid_items'])

            # 3. ì¬ìˆ˜ì§‘
            recollect_invalid(politician_id, politician_name, result['invalid_items'])

            # ì ì‹œ ëŒ€ê¸°
            time.sleep(2)

        # ìµœì¢… ê²€ì¦
        print(f"\n{'='*60}")
        print(f"[ìµœì¢… ê²€ì¦]")
        print(f"{'='*60}")
        final_result = validate_collected_data(politician_id, politician_name, ai_name)

        return final_result


def main():
    parser = argparse.ArgumentParser(description='V40 ê²€ì¦ ë° ì¬ìˆ˜ì§‘')
    parser.add_argument('--politician_id', required=True, help='ì •ì¹˜ì¸ ID')
    parser.add_argument('--politician_name', required=True, help='ì •ì¹˜ì¸ ì´ë¦„')
    parser.add_argument('--mode', choices=['all', 'validate', 'recollect'], default='all',
                       help='ì‹¤í–‰ ëª¨ë“œ (all=ê²€ì¦+ì¬ìˆ˜ì§‘, validate=ê²€ì¦ë§Œ, recollect=ì¬ìˆ˜ì§‘ë§Œ)')
    parser.add_argument('--ai', choices=['Claude', 'Gemini', 'Grok', 'Naver'],
                       help='íŠ¹ì • AIë§Œ ê²€ì¦')
    parser.add_argument('--max_iterations', type=int, default=3,
                       help='ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 3)')

    args = parser.parse_args()

    run_validation_pipeline(
        args.politician_id,
        args.politician_name,
        mode=args.mode,
        ai_name=args.ai,
        max_iterations=args.max_iterations
    )


if __name__ == "__main__":
    main()

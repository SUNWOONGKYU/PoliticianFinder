# -*- coding: utf-8 -*-
"""
V40 ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (ìˆ˜ì • ë²„ì „)

ìˆ˜ì • ì‚¬í•­:
1. URL timeout: 10ì´ˆ â†’ 30ì´ˆ
2. validate_event_date: ì™„ì „íˆ ì œê±° (ê³¼ë„í•œ ì˜¤íŒ)
3. ê¸°ê°„ ê²€ì¦: published_dateë§Œ ì‚¬ìš© (event_year ë¬´ì‹œ)
4. URL ê²€ì¦: 3íšŒ ì¬ì‹œë„ (ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì • ëŒ€ì‘)
5. ê²€ì¦ ëª¨ë“œ: ì‚­ì œí•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ ê¸°ë¡

í•µì‹¬ ì›ì¹™:
- ê²€ì¦ì€ "ì°¸ê³ ìš©"
- ì‚­ì œëŠ” ì‹ ì¤‘í•˜ê²Œ
- AI í‰ê°€ ë‹¨ê³„ì—ì„œ ìµœì¢… í’ˆì§ˆ íŒë‹¨
"""

import os
import sys
import json
import re
import argparse
import time
import requests
from datetime import datetime, timedelta
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from supabase import create_client
from dotenv import load_dotenv
from duplicate_check_utils import normalize_url, normalize_title, is_duplicate_by_url, is_duplicate_by_title

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except AttributeError:
        pass

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

# V40 í…Œì´ë¸”ëª…
TABLE_COLLECTED_DATA = "collected_data_v40"

# SNS ë„ë©”ì¸ (URL ê²€ì¦ ì œì™¸)
SNS_DOMAINS = [
    "twitter.com", "x.com", "facebook.com", "instagram.com",
    "youtube.com", "youtu.be", "tiktok.com"
]

# ê²€ì¦ ê²°ê³¼ ì½”ë“œ
VALIDATION_CODES = {
    "VALID": "ìœ íš¨",
    "INVALID_URL": "URL ì ‘ì† ë¶ˆê°€",
    "EMPTY_URL": "URL ë¹„ì–´ìˆìŒ",
    "FAKE_URL": "ê°€ì§œ URL íŒ¨í„´",
    "WRONG_SOURCE_TYPE": "source_type ë¶ˆì¼ì¹˜",
    "MISSING_FIELD": "í•„ìˆ˜ í•„ë“œ ëˆ„ë½",
    "DATE_OUT_OF_RANGE": "ê¸°ê°„ ì´ˆê³¼",
    "DUPLICATE": "ì¤‘ë³µ ë°ì´í„°"
}


def is_sns_url(url):
    """SNS URL ì—¬ë¶€ í™•ì¸"""
    if not url:
        return False
    domain = urlparse(url).netloc.lower()
    return any(sns in domain for sns in SNS_DOMAINS)


def is_fake_url_pattern(url):
    """ê°€ì§œ URL íŒ¨í„´ ì²´í¬"""
    if not url:
        return False

    fake_patterns = [
        r'example\.com',
        r'test\.com',
        r'placeholder',
        r'\[URL\]',
        r'http://www\.example',
    ]

    for pattern in fake_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return True

    return False


def check_url_exists(url, timeout=30, max_retries=3):
    """
    URL ì‹¤ì œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸

    ìˆ˜ì • ì‚¬í•­:
    - timeout: 10ì´ˆ â†’ 30ì´ˆ
    - ì¬ì‹œë„: 3íšŒ
    """
    if not url or url.strip() == '':
        return False, "EMPTY_URL"

    # SNSëŠ” ê²€ì¦ ì œì™¸
    if is_sns_url(url):
        return True, "VALID"

    # ê°€ì§œ URL íŒ¨í„´
    if is_fake_url_pattern(url):
        return False, "FAKE_URL"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    # ì¬ì‹œë„ ë¡œì§
    for attempt in range(max_retries):
        try:
            # HEAD ë¨¼ì € ì‹œë„
            try:
                response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)
                if response.status_code < 400:
                    return True, "VALID"
            except:
                pass

            # GET ì‹œë„
            response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
            if response.status_code < 400:
                return True, "VALID"
            else:
                # ì¬ì‹œë„
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                return False, "INVALID_URL"

        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            return False, "INVALID_URL"
        except requests.exceptions.ConnectionError:
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            return False, "INVALID_URL"
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            return False, "INVALID_URL"

    return False, "INVALID_URL"


def validate_required_fields(item):
    """í•„ìˆ˜ í•„ë“œ ê²€ì¦"""
    required = ['title', 'content', 'source_url']

    for field in required:
        if not item.get(field):
            return False, "MISSING_FIELD"

    return True, "VALID"


def get_date_range():
    """V40 ê¸°ê°„ ì œí•œ"""
    evaluation_date = datetime.now()
    official_start = evaluation_date - timedelta(days=365*4)  # 4ë…„
    public_start = evaluation_date - timedelta(days=365*2)    # 2ë…„

    return {
        'official_start': official_start,
        'public_start': public_start,
    }


def validate_date_range(item):
    """
    ê¸°ê°„ ì œí•œ ê²€ì¦ (ìˆ˜ì • ë²„ì „)

    ìˆ˜ì • ì‚¬í•­:
    - published_dateë§Œ ì‚¬ìš©
    - event_year ë¬´ì‹œ (ì˜¤íŒ ë°©ì§€)
    """
    date_range = get_date_range()
    data_type = item.get('data_type', 'public').lower()
    pub_date_str = item.get('published_date')

    if not pub_date_str:
        return True, "VALID"  # ë‚ ì§œ ì—†ìœ¼ë©´ íŒ¨ìŠ¤

    try:
        if isinstance(pub_date_str, str):
            pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')
        else:
            pub_date = pub_date_str

        if data_type == 'official':
            if pub_date < date_range['official_start']:
                return False, "DATE_OUT_OF_RANGE"
        else:
            if pub_date < date_range['public_start']:
                return False, "DATE_OUT_OF_RANGE"

        return True, "VALID"

    except:
        return True, "VALID"  # íŒŒì‹± ì‹¤íŒ¨ë©´ íŒ¨ìŠ¤


def check_duplicate(item):
    """ì¤‘ë³µ ê²€ì¦ (ê°„ì†Œí™”)"""
    politician_id = item.get('politician_id')
    collector_ai = item.get('collector_ai')
    url = item.get('source_url', '')

    if not url:
        return True, "VALID"

    # ê°™ì€ AIê°€ ê°™ì€ URL ìˆ˜ì§‘í–ˆëŠ”ì§€ë§Œ ì²´í¬
    try:
        result = supabase.table(TABLE_COLLECTED_DATA)\
            .select('id')\
            .eq('politician_id', politician_id)\
            .eq('collector_ai', collector_ai)\
            .eq('source_url', url)\
            .limit(2)\
            .execute()

        if len(result.data) > 1:
            return False, "DUPLICATE"
    except:
        pass

    return True, "VALID"


def validate_item_fixed(item):
    """
    ë‹¨ì¼ í•­ëª© ê²€ì¦ (ìˆ˜ì • ë²„ì „)

    ìˆ˜ì • ì‚¬í•­:
    1. validate_event_date ì œê±°
    2. URL ê²€ì¦ ì™„í™” (timeout 30ì´ˆ, 3íšŒ ì¬ì‹œë„)
    3. ê¸°ê°„ ê²€ì¦ë§Œ (event_year ë¬´ì‹œ)
    """
    # 1. í•„ìˆ˜ í•„ë“œ
    valid, code = validate_required_fields(item)
    if not valid:
        return False, code

    # 2. URL ì¡´ì¬ (SNSëŠ” ì œì™¸)
    url = item.get('source_url', '')
    if url and not is_sns_url(url):
        valid, code = check_url_exists(url, timeout=30, max_retries=3)
        if not valid:
            return False, code

    # 3. ê¸°ê°„ ê²€ì¦ (published_dateë§Œ)
    valid, code = validate_date_range(item)
    if not valid:
        return False, code

    # 4. ì¤‘ë³µ ê²€ì¦
    valid, code = check_duplicate(item)
    if not valid:
        return False, code

    return True, "VALID"


def validate_collected_data_fixed(politician_id, politician_name, dry_run=True):
    """
    ìˆ˜ì§‘ ë°ì´í„° ê²€ì¦ (ìˆ˜ì • ë²„ì „)

    dry_run=True: ë¡œê·¸ë§Œ ê¸°ë¡, ì‚­ì œ ì•ˆ í•¨ (ê¸°ë³¸ê°’)
    dry_run=False: ì‹¤ì œ ì‚­ì œ
    """
    print(f"\n{'='*60}")
    print(f"[ê²€ì¦] {politician_name} ({politician_id})")
    if dry_run:
        print(f"[ëª¨ë“œ] DRY RUN - ì‚­ì œí•˜ì§€ ì•ŠìŒ, ë¡œê·¸ë§Œ ê¸°ë¡")
    else:
        print(f"[ëª¨ë“œ] ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰")
    print(f"{'='*60}")

    # ë°ì´í„° ì¡°íšŒ
    result = supabase.table(TABLE_COLLECTED_DATA)\
        .select('*')\
        .eq('politician_id', politician_id)\
        .execute()

    items = result.data
    print(f"ì´ {len(items)}ê°œ í•­ëª© ê²€ì¦ ì‹œì‘...")

    valid_count = 0
    invalid_items = []

    for i, item in enumerate(items):
        valid, code = validate_item_fixed(item)

        if valid:
            valid_count += 1
        else:
            invalid_items.append({
                'id': item.get('id'),
                'title': item.get('title', '')[:50],
                'code': code,
                'collector_ai': item.get('collector_ai'),
                'url': item.get('source_url', '')[:80]
            })

        if (i + 1) % 100 == 0:
            print(f"  ì§„í–‰: {i+1}/{len(items)} ({valid_count}ê°œ ìœ íš¨)")

    invalid_count = len(invalid_items)

    print(f"\nê²€ì¦ ì™„ë£Œ:")
    print(f"  âœ… ìœ íš¨: {valid_count}ê°œ ({valid_count/len(items)*100:.1f}%)")
    print(f"  âŒ ë¬´íš¨: {invalid_count}ê°œ ({invalid_count/len(items)*100:.1f}%)")

    # ë¬´íš¨ í•­ëª© ìƒì„¸
    if invalid_items:
        print(f"\në¬´íš¨ í•­ëª© ìƒì„¸:")
        code_counts = {}
        for item in invalid_items:
            code = item['code']
            code_counts[code] = code_counts.get(code, 0) + 1

        for code, count in sorted(code_counts.items(), key=lambda x: -x[1]):
            print(f"  - {VALIDATION_CODES.get(code, code)}: {count}ê°œ")

    # DRY RUN ëª¨ë“œ
    if dry_run:
        print(f"\nğŸ’¡ DRY RUN ëª¨ë“œ: ì‚­ì œí•˜ì§€ ì•ŠìŒ")
        print(f"   ì‹¤ì œ ì‚­ì œí•˜ë ¤ë©´ --no-dry-run ì˜µì…˜ ì‚¬ìš©")
    else:
        # ì‹¤ì œ ì‚­ì œ
        deleted = 0
        for item in invalid_items:
            try:
                supabase.table(TABLE_COLLECTED_DATA)\
                    .delete()\
                    .eq('id', item['id'])\
                    .execute()
                deleted += 1
            except:
                pass
        print(f"\nğŸ—‘ï¸ {deleted}ê°œ ë¬´íš¨ í•­ëª© ì‚­ì œ")

    return {
        'total': len(items),
        'valid': valid_count,
        'invalid': invalid_count,
        'invalid_rate': invalid_count / len(items) * 100 if len(items) > 0 else 0
    }


def main():
    parser = argparse.ArgumentParser(description='V40 ê²€ì¦ (ìˆ˜ì • ë²„ì „)')
    parser.add_argument('--politician_id', required=True)
    parser.add_argument('--politician_name', required=True)
    parser.add_argument('--no-dry-run', action='store_true', help='ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰ (ê¸°ë³¸: DRY RUN)')

    args = parser.parse_args()

    dry_run = not args.no_dry_run

    result = validate_collected_data_fixed(
        args.politician_id,
        args.politician_name,
        dry_run=dry_run
    )

    print(f"\n{'='*60}")
    print(f"ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
    print(f"  ì „ì²´: {result['total']}ê°œ")
    print(f"  ìœ íš¨: {result['valid']}ê°œ")
    print(f"  ë¬´íš¨: {result['invalid']}ê°œ")
    print(f"  ë¬´íš¨ìœ¨: {result['invalid_rate']:.1f}%")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()

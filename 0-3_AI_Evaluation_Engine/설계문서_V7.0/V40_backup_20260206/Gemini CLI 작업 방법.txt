✦ 정치인 평가 데이터를 수집하고 처리하는 제 프로세스는 크게 [검색 - 수집 - 정제 -
  평가 - 저장]의 5단계로 이루어집니다. 제가 직접 모든 것을 창조하는 것이 아니라,
  제공된 도구(Tools)와 전용 스크립트를 사용하여 체계적으로 수행합니다.

  상세 프로세스는 다음과 같습니다.


  1. 데이터 소스 식별 및 검색 (Discovery)
   * 구글 검색 엔진 활용: 말씀하신 대로 저는 google_web_search 도구를 직접 사용하여
     실시간 뉴스, 블로그, 포럼 데이터를 찾습니다. "조은희 의원 행보", "조은희 공약
     이행" 같은 키워드로 검색을 수행합니다.
   * 공식 데이터 매핑: politician_ids_mapping.txt나 official_data_full.txt 같은
     프로젝트 내부 파일을 참조하여 평가 대상의 정확한 식별자(ID)와 공식 활동 내역을
     확인합니다.


  2. 원천 데이터 수집 (Collection)
   * 전용 컬렉터(Scripts) 실행: 제가 직접 코드를 짜거나 이미 만들어진 collect_v40.py,
     collect_chatgpt_publicinterest.py 같은 수집 스크립트를 run_shell_command로
     실행합니다.
   * 웹 페칭(Web Fetching): 특정 뉴스 기사나 정부 웹사이트(국회, 지자체 등)의 상세
     내용이 필요할 때 web_fetch 도구로 해당 페이지의 HTML을 읽어와 텍스트를
     추출합니다.


  3. 노이즈 제거 및 데이터 정제 (Refining)
   * 동명이인 필터링: 예를 들어 '조은희'라는 이름을 가진 코로나 대응단장이나 다른
     일반인의 데이터는 정치인 평가에 방해가 됩니다. 저는 수집된 텍스트의 맥락을
     분석해 정치인 조은희와 관련 없는 데이터는 'X(제외)' 처리하여 걸러냅니다.
   * 중복 체크: duplicate_check_utils.py 등을 활용해 이미 수집된 동일한 뉴스나 문서는
     제외합니다.


  4. AI 심층 평가 (Evaluation - 나의 핵심 역할)
   * 카테고리별 분석: 수집된 텍스트 데이터를 읽고, 제가 직접 전문성, 리더십, 청렴성
     등 10개 카테고리에 맞춰 점수를 매깁니다.
   * 근거(Rationale) 생성: 단순 점수뿐만 아니라 "왜 이 점수인가"에 대한 근거를 한국어
     한 문장으로 작성합니다. 이 과정에서 저는 가이드라인에 정의된 논리 체계를
     따릅니다.


  5. 데이터베이스 저장 및 모니터링 (Storage & Monitor)
   * DB 반영: 평가가 완료된 JSON 결과물을 gemini_eval_helper.py나
     migrate_pooling_json_to_db.py 스크립트를 통해 PostgreSQL 데이터베이스에 최종
     저장합니다.
   * 상태 관리: status 명령을 통해 카테고리별로 미진한 부분이 있는지 체크하고 100%가
     될 때까지 반복합니다.


  요약하자면:
  누구한테 시키는 것이 아니라, 제가 프로젝트의 '오케스트라 지휘자'가 되어 구글 검색
  엔진, 웹 페칭 도구, 그리고 파이썬 수집 스크립트들을 적재적소에 가동시켜 데이터를
  뽑아내고, 그 알맹이를 제가 직접 분석해서 점수를 매기는 방식입니다.
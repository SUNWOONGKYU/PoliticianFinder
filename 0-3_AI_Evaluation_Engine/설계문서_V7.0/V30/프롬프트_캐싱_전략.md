# V30 í”„ë¡¬í”„íŠ¸ ìºì‹± ì „ëµ

**ì‘ì„±ì¼**: 2026-01-25
**ëª©ì **: 4ê°œ AI í‰ê°€ ì‹œìŠ¤í…œì—ì„œ í”„ë¡¬í”„íŠ¸ ìºì‹±ì„ í™œìš©í•˜ì—¬ í† í°/ë¹„ìš©/ì‹œê°„ ì ˆê°
**í•µì‹¬ ì œì•½**: 4ê°œ AI ëª¨ë‘ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ê³µì •ì„± ìœ ì§€)

---

## ğŸ“‹ ëª©ì°¨

1. [AIë³„ ìºì‹± ê¸°ëŠ¥ ì¡°ì‚¬](#1-aië³„-ìºì‹±-ê¸°ëŠ¥-ì¡°ì‚¬)
2. [í˜„ì¬ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¶„ì„](#2-í˜„ì¬-í”„ë¡¬í”„íŠ¸-êµ¬ì¡°-ë¶„ì„)
3. [ìºì‹œ ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì‹ë³„](#3-ìºì‹œ-ê°€ëŠ¥í•œ-í”„ë¡¬í”„íŠ¸-ë¶€ë¶„-ì‹ë³„)
4. [AIë³„ ìºì‹± ì ìš© ë°©ì•ˆ](#4-aië³„-ìºì‹±-ì ìš©-ë°©ì•ˆ)
5. [50ê°œ ë°°ì¹˜ì— ìºì‹± ì ìš© ì „ëµ](#5-50ê°œ-ë°°ì¹˜ì—-ìºì‹±-ì ìš©-ì „ëµ)
6. [4ê°œ AI ê³µì •ì„± ìœ ì§€ ì „ëµ](#6-4ê°œ-ai-ê³µì •ì„±-ìœ ì§€-ì „ëµ)
7. [ìµœì¢… ê¶Œì¥ì•ˆ](#7-ìµœì¢…-ê¶Œì¥ì•ˆ)

---

## 1. AIë³„ ìºì‹± ê¸°ëŠ¥ ì¡°ì‚¬

### 1.1 Claude (Anthropic) - Prompt Caching âœ…

**ì§€ì› ì—¬ë¶€**: âœ… ì§€ì› (ê³µì‹ ê¸°ëŠ¥)

**ì‚¬ìš© ë°©ë²•**:
```python
from anthropic import Anthropic

client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

message = client.messages.create(
    model="claude-3-5-haiku-20241022",
    max_tokens=4096,
    system=[
        {
            "type": "text",
            "text": "ê³ ì •ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸...",
            "cache_control": {"type": "ephemeral"}  # ìºì‹± í™œì„±í™”
        }
    ],
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "ê³ ì •ëœ í”„ë¡¬í”„íŠ¸...",
                    "cache_control": {"type": "ephemeral"}  # ìºì‹± í™œì„±í™”
                },
                {
                    "type": "text",
                    "text": "ë³€ë™ë˜ëŠ” í‰ê°€ ë°ì´í„°..."  # ìºì‹± ì•ˆ ë¨
                }
            ]
        }
    ]
)
```

**ìºì‹œ ìˆ˜ëª…**:
- ê¸°ë³¸ TTL: 5ë¶„ (ë§¤ ì‚¬ìš© ì‹œ ê°±ì‹ )
- 1ì‹œê°„ TTL ì˜µì…˜ ìˆìŒ

**ê°€ê²© êµ¬ì¡°**:
- Cache Write (5ë¶„ TTL): Base Input Ã— 1.25
- Cache Write (1ì‹œê°„ TTL): Base Input Ã— 2.0
- **Cache Read: Base Input Ã— 0.1 (90% ì ˆê°)** âœ…

**ì§€ì› ëª¨ë¸**:
- Claude Opus 4.1, Opus 4
- Claude Sonnet 4.5, Sonnet 4, Sonnet 3.7
- **Claude Haiku 4.5**, Haiku 3.5 âœ… (í˜„ì¬ ì‚¬ìš© ì¤‘)

**ì ˆê° íš¨ê³¼**:
- ì…ë ¥ ë¹„ìš©: ìµœëŒ€ **90% ì ˆê°**
- ì§€ì—° ì‹œê°„: ìµœëŒ€ **85% ë‹¨ì¶•**

**ê³µì‹ ë¬¸ì„œ**: [Prompt caching - Claude API Docs](https://docs.claude.com/en/docs/build-with-claude/prompt-caching)

**V30 ì ìš©**:
- âš ï¸ **í˜„ì¬ Subscription Mode ì‚¬ìš© â†’ API ì‚¬ìš© ì•ˆ í•¨ â†’ ìºì‹± ë¶ˆê°€**
- âœ… **ëŒ€ì‹ **: ë°°ì¹˜ í¬ê¸° ìµœëŒ€í™” (10ê°œì”©), ë³‘ë ¬ ì„¸ì…˜ ì‹¤í–‰

---

### 1.2 ChatGPT (OpenAI) - Prompt Caching âœ…

**ì§€ì› ì—¬ë¶€**: âœ… ìë™ ì§€ì› (ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”)

**ì‚¬ìš© ë°©ë²•**:
```python
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ìë™ ìºì‹± (1024+ í† í°)
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "ê³ ì •ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸..."},
        {"role": "user", "content": "ê³ ì •ëœ í”„ë¡¬í”„íŠ¸ + ë³€ë™ ë°ì´í„°..."}
    ]
)

# Extended Caching (ìµœëŒ€ 24ì‹œê°„)
response = client.chat.completions.create(
    model="gpt-4o-mini",
    prompt_cache_retention="24h",  # 24ì‹œê°„ ìºì‹œ ìœ ì§€
    messages=[...]
)
```

**ìºì‹± ì¡°ê±´**:
- **ìë™ í™œì„±í™”**: 1024+ í† í° í”„ë¡¬í”„íŠ¸
- **Prefix ë§¤ì¹­**: ê³µí†µ ì ‘ë‘ì‚¬ ìë™ ê°ì§€
- **ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”** âœ…

**ìºì‹œ ìˆ˜ëª…**:
- ê¸°ë³¸: 5-10ë¶„ ë¹„í™œì„± í›„ ì‚­ì œ
- Extended: ìµœëŒ€ 24ì‹œê°„ (2026ë…„ ì‹ ê¸°ëŠ¥)
- ìµœëŒ€ ë³´ì¡´: 1ì‹œê°„

**ê°€ê²© êµ¬ì¡°**:
- **Cache Read: Base Input Ã— 0.1 (90% ì ˆê°)** âœ…
- Cache Write: ì¶”ê°€ ë¹„ìš© ì—†ìŒ
- Storage: ë¬´ë£Œ

**ì§€ì› ëª¨ë¸**:
- GPT-4o, **GPT-4o-mini** âœ… (í˜„ì¬ ì‚¬ìš© ì¤‘)
- o1-preview, o1-mini
- íŒŒì¸íŠœë‹ ëª¨ë¸

**ì ˆê° íš¨ê³¼**:
- ì…ë ¥ ë¹„ìš©: ìµœëŒ€ **90% ì ˆê°**
- ì§€ì—° ì‹œê°„: ìµœëŒ€ **80% ë‹¨ì¶•**

**ê³µì‹ ë¬¸ì„œ**: [Prompt caching | OpenAI API](https://platform.openai.com/docs/guides/prompt-caching)

**V30 ì ìš©**:
- âœ… **ìë™ ìºì‹±** (ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”)
- âœ… **Extended Caching** ì˜µì…˜ ì¶”ê°€ ê¶Œì¥ (24ì‹œê°„)

---

### 1.3 Gemini (Google) - Context Caching âœ…

**ì§€ì› ì—¬ë¶€**: âœ… ì§€ì› (Implicit + Explicit)

**Implicit Caching (ìë™)** - 2026ë…„ ì‹ ê¸°ëŠ¥:
```python
from google import genai

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# ìë™ ìºì‹± (1024+ í† í°, Gemini 2.5 Flash)
response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents="í”„ë¡¬í”„íŠ¸..."
)
```

**Explicit Caching (ìˆ˜ë™)**:
```python
from google.generativeai import caching
import datetime

# ìºì‹œ ìƒì„± (2048+ í† í° í•„ìˆ˜)
cache = caching.CachedContent.create(
    model='gemini-2.0-flash',
    display_name='evaluation_system_prompt',
    system_instruction='ê³ ì •ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸...',
    contents=['ê³ ì •ëœ í”„ë¡¬í”„íŠ¸...'],
    ttl=datetime.timedelta(hours=1)
)

# ìºì‹œ ì‚¬ìš©
response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='ë³€ë™ ë°ì´í„°...',
    cached_content=cache.name
)
```

**ìºì‹± ì¡°ê±´**:
- **Implicit**: 1024+ í† í° (Gemini 2.5 Flash), 2048+ í† í° (Gemini 2.5 Pro)
- **Explicit**: 2048+ í† í° (Gemini 2.0), ìµœëŒ€ 1M í† í° (Gemini 2.5 Pro)

**ìºì‹œ ìˆ˜ëª…**:
- **Implicit**: 24ì‹œê°„ ì´ë‚´ ìë™ ì‚­ì œ
- **Explicit**: TTL ì„¤ì • ê°€ëŠ¥ (ìµœëŒ€ ì—¬ëŸ¬ ì‹œê°„)

**ê°€ê²© êµ¬ì¡°** (Explicit):
- **Cache Read (Gemini 2.5)**: Base Input Ã— 0.1 (90% ì ˆê°) âœ…
- **Cache Read (Gemini 2.0)**: Base Input Ã— 0.25 (75% ì ˆê°)
- Cache Write: Base Input Ã— 1.0 (ë™ì¼)
- Storage: ì‹œê°„ë‹¹ ê³¼ê¸ˆ

**ì§€ì› ëª¨ë¸**:
- Gemini 2.5 Pro, Gemini 2.5 Flash
- **Gemini 2.0 Flash** âœ… (í˜„ì¬ ì‚¬ìš© ì¤‘)

**ì ˆê° íš¨ê³¼**:
- ì…ë ¥ ë¹„ìš©: **75-90% ì ˆê°** (ëª¨ë¸ì— ë”°ë¼)
- ì§€ì—° ì‹œê°„: ìƒë‹¹í•œ ê°œì„ 

**ê³µì‹ ë¬¸ì„œ**: [Context caching | Gemini API](https://ai.google.dev/gemini-api/docs/caching)

**V30 ì ìš©**:
- âœ… **Implicit Caching** ìë™ ì ìš© (1024+ í† í°)
- âœ… **Explicit Caching** ì¶”ê°€ êµ¬í˜„ ê¶Œì¥ (90% ì ˆê°)

---

### 1.4 Grok (X.AI) - Prompt Caching âœ…

**ì§€ì› ì—¬ë¶€**: âœ… ìë™ ì§€ì›

**ì‚¬ìš© ë°©ë²•**:
```python
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("XAI_API_KEY"),
    base_url="https://api.x.ai/v1"
)

# ìë™ ìºì‹± (prefix matching)
response = client.chat.completions.create(
    model="grok-4-fast",
    messages=[
        {"role": "system", "content": "ê³ ì •ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸..."},
        {"role": "user", "content": "ê³ ì •ëœ í”„ë¡¬í”„íŠ¸ + ë³€ë™ ë°ì´í„°..."}
    ]
)
```

**ìºì‹± ì¡°ê±´**:
- **ìë™ í™œì„±í™”**: ëª¨ë“  ìš”ì²­
- **Prefix ë§¤ì¹­**: ì •í™•í•œ ì ‘ë‘ì‚¬ ì¼ì¹˜
- **ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”** âœ…

**ìºì‹œ ìˆ˜ëª…**:
- ìë™ ê´€ë¦¬ (í´ëŸ¬ìŠ¤í„°ë³„ ë¶„ì‚°)
- ëª…ì‹œì  TTL ì„¤ì • ì—†ìŒ

**ê°€ê²© êµ¬ì¡°**:
- **Cached Input (grok-4-fast)**: $0.02/1M í† í° (90% ì ˆê°) âœ…
- Input: $0.20/1M í† í°
- Output: $1.50/1M í† í°

**ì§€ì› ëª¨ë¸**:
- **grok-4-fast** âœ… (í˜„ì¬ ì‚¬ìš© ì¤‘)
- grok-4, grok-3, grok-3-fast, grok-3-mini
- grok-code-fast-1

**ì ˆê° íš¨ê³¼**:
- ì…ë ¥ ë¹„ìš©: **90% ì ˆê°**
- ìºì‹œ íˆíŠ¸ìœ¨: **90% ì´ìƒ** (íŒŒíŠ¸ë„ˆ ê¸°ì¤€)
- ì¶”ë¡  ì†ë„: ëŒ€í­ í–¥ìƒ (agentic ì‘ì—…ì—ì„œ)

**ê³µì‹ ë¬¸ì„œ**: [Consumption and Rate Limits | xAI](https://docs.x.ai/docs/key-information/consumption-and-rate-limits)

**V30 ì ìš©**:
- âœ… **ìë™ ìºì‹±** (ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”)
- âœ… **ë†’ì€ ìºì‹œ íˆíŠ¸ìœ¨** (90%+)

---

### 1.5 AIë³„ ìºì‹± ê¸°ëŠ¥ ìš”ì•½

| AI | ìºì‹± ì§€ì› | í™œì„±í™” ë°©ë²• | ìµœì†Œ í† í° | TTL | ì ˆê°ë¥  | V30 ì ìš© |
|----|----------|------------|----------|-----|--------|---------|
| **Claude** | âœ… | Cache-Control í—¤ë” | ì—†ìŒ | 5ë¶„~1ì‹œê°„ | **90%** | âš ï¸ Subscription (ìºì‹± ë¶ˆê°€) |
| **ChatGPT** | âœ… | ìë™ (1024+) | 1024 | 5ë¶„~24ì‹œê°„ | **90%** | âœ… ìë™ ì ìš© |
| **Gemini** | âœ… | Implicit(ìë™) / Explicit(ìˆ˜ë™) | 1024/2048 | 24ì‹œê°„ | **75-90%** | âœ… ìë™ + ìˆ˜ë™ |
| **Grok** | âœ… | ìë™ | ì—†ìŒ | ìë™ ê´€ë¦¬ | **90%** | âœ… ìë™ ì ìš© |

---

## 2. í˜„ì¬ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¶„ì„

### 2.1 evaluate_v30.pyì˜ evaluate_batch() í”„ë¡¬í”„íŠ¸

```python
def evaluate_batch(evaluator_ai, items, category_name, politician_id, politician_name):
    """ë°°ì¹˜ í‰ê°€ (ìµœëŒ€ 10ê°œì”©)"""
    cat_kor = CATEGORY_MAP.get(category_name.lower(), category_name)

    # ì •ì¹˜ì¸ í”„ë¡œí•„ ì •ë³´
    profile_info = format_politician_profile(politician_id, politician_name)

    # í‰ê°€í•  ë°ì´í„° ëª©ë¡
    items_text = ""
    for i, item in enumerate(items, 1):
        items_text += f"""
[í•­ëª© {i}]
- ID: {item.get('id', '')}
- ì œëª©: {item.get('title', 'N/A')}
- ë‚´ìš©: {item.get('content', 'N/A')[:300]}...
- ì¶œì²˜: {item.get('source_name', item.get('source_url', 'N/A'))}
- ë‚ ì§œ: {item.get('published_date', 'N/A')}
- ìˆ˜ì§‘AI: {item.get('collector_ai', 'N/A')}
"""

    prompt = f"""ë‹¹ì‹ ì€ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{profile_info}

**í‰ê°€ ì¹´í…Œê³ ë¦¬**: {cat_kor} ({category_name})

ì•„ë˜ ë°ì´í„°ë¥¼ **ê°ê´€ì ìœ¼ë¡œ í‰ê°€**í•˜ì—¬ ë“±ê¸‰ì„ ë¶€ì—¬í•˜ì„¸ìš”.

**ë“±ê¸‰ ì²´ê³„** (+4 ~ -4) - V30 ê¸°ì¤€ (8ë‹¨ê³„):
| ë“±ê¸‰ | íŒë‹¨ ê¸°ì¤€ | ì ìˆ˜ |
|------|-----------|------|
| +4 | íƒì›”í•¨ - í•´ë‹¹ ë¶„ì•¼ ëª¨ë²” ì‚¬ë¡€ | +8 |
| +3 | ìš°ìˆ˜í•¨ - ê¸ì •ì  í‰ê°€ | +6 |
| +2 | ì–‘í˜¸í•¨ - ê¸°ë³¸ ì¶©ì¡± | +4 |
| +1 | ë³´í†µ ê¸ì • - í‰ê·  ì´ìƒ | +2 |
| -1 | ë¯¸í¡í•¨ - ê°œì„  í•„ìš” | -2 |
| -2 | ë¶€ì¡±í•¨ - ë¬¸ì œ ìˆìŒ | -4 |
| -3 | ë§¤ìš° ë¶€ì¡± - ì‹¬ê°í•œ ë¬¸ì œ | -6 |
| -4 | ê·¹íˆ ë¶€ì¡± - ì •ì¹˜ì¸ ë¶€ì í•© | -8 |

**í‰ê°€ ê¸°ì¤€**:
- ê¸ì •ì  ë‚´ìš© (ì„±ê³¼, ì—…ì , ì¹­ì°¬) â†’ +4, +3, +2, +1
- ë¶€ì •ì  ë‚´ìš© (ë…¼ë€, ë¹„íŒ, ë¬¸ì œ) â†’ -1, -2, -3, -4 (ì‹¬ê°ë„ì— ë”°ë¼)

**í‰ê°€í•  ë°ì´í„°**:
{items_text}

**ë°˜ë“œì‹œ ëª¨ë“  í•­ëª©ì— ëŒ€í•´ í‰ê°€í•˜ì„¸ìš”.**

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
```json
{{
  "evaluations": [
    {{
      "id": "ë°ì´í„° ID ê°’",
      "rating": "+4, +3, +2, +1, -1, -2, -3, -4 ì¤‘ í•˜ë‚˜",
      "rationale": "í‰ê°€ ê·¼ê±° (1ë¬¸ì¥)"
    }}
  ]
}}
```"""

    return prompt
```

---

### 2.2 í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ë¶„í•´

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [A] ê³ ì • ë¶€ë¶„ (í•­ìƒ ë™ì¼) - ìºì‹œ ê°€ëŠ¥ âœ…                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. ì—­í•  ì •ì˜:                                               â”‚
â”‚    "ë‹¹ì‹ ì€ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."                        â”‚
â”‚                                                             â”‚
â”‚ 2. ë“±ê¸‰ ì²´ê³„ (+4 ~ -4):                                     â”‚
â”‚    - 8ë‹¨ê³„ ë“±ê¸‰í‘œ                                           â”‚
â”‚    - íŒë‹¨ ê¸°ì¤€                                              â”‚
â”‚    - ì ìˆ˜ ë³€í™˜                                              â”‚
â”‚                                                             â”‚
â”‚ 3. í‰ê°€ ê¸°ì¤€:                                               â”‚
â”‚    - ê¸ì •/ë¶€ì • íŒë‹¨ ê·œì¹™                                    â”‚
â”‚                                                             â”‚
â”‚ 4. JSON ì¶œë ¥ í˜•ì‹:                                          â”‚
â”‚    - ì˜ˆì‹œ JSON êµ¬ì¡°                                         â”‚
â”‚    - í•„ë“œ ì„¤ëª…                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [B] ì •ì¹˜ì¸ë³„ ê³ ì • (ì •ì¹˜ì¸ë‹¹ ë™ì¼) - ìºì‹œ ê°€ëŠ¥ âœ…             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. ì •ì¹˜ì¸ í”„ë¡œí•„:                                            â”‚
â”‚    - ì´ë¦„, ì‹ ë¶„, ì§ì±…, ì •ë‹¹, ì§€ì—­, ì„±ë³„                     â”‚
â”‚    - íŠ¹ë³„ ì§€ì‹œì‚¬í•­                                           â”‚
â”‚    - ì•Œë ¤ì§„ ë…¼ë€/ì„±ê³¼                                        â”‚
â”‚                                                             â”‚
â”‚ 2. í‰ê°€ ì¹´í…Œê³ ë¦¬:                                            â”‚
â”‚    - ì¹´í…Œê³ ë¦¬ëª… (ì „ë¬¸ì„±, ë¦¬ë”ì‹­ ë“±)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [C] ë°°ì¹˜ë³„ ë³€ë™ (ë°°ì¹˜ë§ˆë‹¤ ë³€ê²½) - ìºì‹œ ë¶ˆê°€ âŒ               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ í‰ê°€í•  ë°ì´í„° (10ê°œì”©):                                      â”‚
â”‚ [í•­ëª© 1]                                                    â”‚
â”‚ - ID: uuid...                                               â”‚
â”‚ - ì œëª©: "..."                                               â”‚
â”‚ - ë‚´ìš©: "..."                                               â”‚
â”‚ - ì¶œì²˜: "..."                                               â”‚
â”‚ - ë‚ ì§œ: "..."                                               â”‚
â”‚ - ìˆ˜ì§‘AI: "Gemini"                                          â”‚
â”‚                                                             â”‚
â”‚ [í•­ëª© 2] ... [í•­ëª© 10]                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.3 í† í° ìˆ˜ ì¶”ì •

| ë¶€ë¶„ | ë‚´ìš© | ì˜ˆìƒ í† í° |
|------|------|----------|
| **[A] ê³ ì • ë¶€ë¶„** | ì—­í•  + ë“±ê¸‰ì²´ê³„ + ê¸°ì¤€ + JSON í˜•ì‹ | ~500 í† í° |
| **[B] ì •ì¹˜ì¸ë³„ ê³ ì •** | í”„ë¡œí•„ + ì¹´í…Œê³ ë¦¬ | ~300 í† í° |
| **[C] ë°°ì¹˜ë³„ ë³€ë™** | 10ê°œ í•­ëª© ë°ì´í„° | ~2,000 í† í° |
| **ì „ì²´ í”„ë¡¬í”„íŠ¸** | A + B + C | **~2,800 í† í°** |

**ìºì‹œ ê°€ëŠ¥ í† í°**: ~800 í† í° (A + B)
**ìºì‹œ ë¶ˆê°€ í† í°**: ~2,000 í† í° (C)

---

## 3. ìºì‹œ ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì‹ë³„

### 3.1 ìºì‹œ ë ˆë²¨ êµ¬ì¡°

```
ë ˆë²¨ 1: ì „ì²´ ì‹œìŠ¤í…œ ê³µí†µ (ëª¨ë“  AI, ëª¨ë“  ì •ì¹˜ì¸, ëª¨ë“  ì¹´í…Œê³ ë¦¬)
â””â”€> ì—­í•  ì •ì˜ + ë“±ê¸‰ ì²´ê³„ + í‰ê°€ ê¸°ì¤€ + JSON í˜•ì‹
    ìºì‹œ í‚¤: "v30_evaluation_system_prompt"
    ì¬ì‚¬ìš©: ëª¨ë“  í‰ê°€ ìš”ì²­ (400íšŒ Ã— 10 ì¹´í…Œê³ ë¦¬ = 4,000íšŒ)

ë ˆë²¨ 2: ì •ì¹˜ì¸ë³„ ì¹´í…Œê³ ë¦¬ë³„ (AIë³„ë¡œ 10ë²ˆ ì¬ì‚¬ìš©)
â””â”€> ë ˆë²¨ 1 + ì •ì¹˜ì¸ í”„ë¡œí•„ + ì¹´í…Œê³ ë¦¬ëª…
    ìºì‹œ í‚¤: "v30_eval_{politician_id}_{category}"
    ì¬ì‚¬ìš©: 10ê°œ ë°°ì¹˜ ìš”ì²­ (100ê°œ ë°ì´í„° Ã· 10ê°œ/ë°°ì¹˜ = 10íšŒ)

ë ˆë²¨ 3: ë°°ì¹˜ë³„ (ìºì‹œ ë¶ˆê°€)
â””â”€> ë ˆë²¨ 2 + 10ê°œ í‰ê°€ ë°ì´í„°
    ìºì‹œ: ë¶ˆê°€ëŠ¥ (ë§¤ë²ˆ ë‹¤ë¥¸ ë°ì´í„°)
```

---

### 3.2 ìºì‹œ í‚¤ ì„¤ê³„

#### ë ˆë²¨ 1: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì „ì²´ ê³µí†µ)

```python
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë“±ê¸‰ ì²´ê³„** (+4 ~ -4) - V30 ê¸°ì¤€ (8ë‹¨ê³„):
| ë“±ê¸‰ | íŒë‹¨ ê¸°ì¤€ | ì ìˆ˜ |
|------|-----------|------|
| +4 | íƒì›”í•¨ - í•´ë‹¹ ë¶„ì•¼ ëª¨ë²” ì‚¬ë¡€ | +8 |
| +3 | ìš°ìˆ˜í•¨ - ê¸ì •ì  í‰ê°€ | +6 |
| +2 | ì–‘í˜¸í•¨ - ê¸°ë³¸ ì¶©ì¡± | +4 |
| +1 | ë³´í†µ ê¸ì • - í‰ê·  ì´ìƒ | +2 |
| -1 | ë¯¸í¡í•¨ - ê°œì„  í•„ìš” | -2 |
| -2 | ë¶€ì¡±í•¨ - ë¬¸ì œ ìˆìŒ | -4 |
| -3 | ë§¤ìš° ë¶€ì¡± - ì‹¬ê°í•œ ë¬¸ì œ | -6 |
| -4 | ê·¹íˆ ë¶€ì¡± - ì •ì¹˜ì¸ ë¶€ì í•© | -8 |

**í‰ê°€ ê¸°ì¤€**:
- ê¸ì •ì  ë‚´ìš© (ì„±ê³¼, ì—…ì , ì¹­ì°¬) â†’ +4, +3, +2, +1
- ë¶€ì •ì  ë‚´ìš© (ë…¼ë€, ë¹„íŒ, ë¬¸ì œ) â†’ -1, -2, -3, -4 (ì‹¬ê°ë„ì— ë”°ë¼)

**ë°˜ë“œì‹œ ëª¨ë“  í•­ëª©ì— ëŒ€í•´ í‰ê°€í•˜ì„¸ìš”.**

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
```json
{
  "evaluations": [
    {
      "id": "ë°ì´í„° ID ê°’",
      "rating": "+4, +3, +2, +1, -1, -2, -3, -4 ì¤‘ í•˜ë‚˜",
      "rationale": "í‰ê°€ ê·¼ê±° (1ë¬¸ì¥)"
    }
  ]
}
```"""

# ìºì‹œ í‚¤: "v30_system_prompt_v1"
# ì¬ì‚¬ìš©: ëª¨ë“  í‰ê°€ (4,000+ íšŒ)
```

#### ë ˆë²¨ 2: ì •ì¹˜ì¸ë³„ ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸

```python
def get_politician_category_prompt(politician_id, politician_name, category_name):
    """ì •ì¹˜ì¸ë³„ ì¹´í…Œê³ ë¦¬ë³„ ê³ ì • í”„ë¡¬í”„íŠ¸ (ìºì‹œ ê°€ëŠ¥)"""
    profile_info = format_politician_profile(politician_id, politician_name)
    cat_kor = CATEGORY_MAP.get(category_name.lower(), category_name)

    prompt = f"""{profile_info}

**í‰ê°€ ì¹´í…Œê³ ë¦¬**: {cat_kor} ({category_name})

ì•„ë˜ ë°ì´í„°ë¥¼ **ê°ê´€ì ìœ¼ë¡œ í‰ê°€**í•˜ì—¬ ë“±ê¸‰ì„ ë¶€ì—¬í•˜ì„¸ìš”."""

    return prompt

# ìºì‹œ í‚¤: f"v30_eval_{politician_id}_{category_name}_v1"
# ì¬ì‚¬ìš©: í•´ë‹¹ ì •ì¹˜ì¸ì˜ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ í‰ê°€ (10íšŒ)
```

---

### 3.3 ìºì‹œ ì ìš© ì „í›„ ë¹„êµ

#### í˜„ì¬ (ìºì‹± ì—†ìŒ)

```
ìš”ì²­ 1 (ì „ë¬¸ì„±, ë°°ì¹˜ 1): 2,800 í† í° ì „ì†¡
ìš”ì²­ 2 (ì „ë¬¸ì„±, ë°°ì¹˜ 2): 2,800 í† í° ì „ì†¡
ìš”ì²­ 3 (ì „ë¬¸ì„±, ë°°ì¹˜ 3): 2,800 í† í° ì „ì†¡
...
ìš”ì²­ 10 (ì „ë¬¸ì„±, ë°°ì¹˜ 10): 2,800 í† í° ì „ì†¡

ì´ ì „ì†¡: 2,800 Ã— 10 = 28,000 í† í°
ì´ ë¹„ìš©: 28,000 Ã— (Input Price)
```

#### ìºì‹± ì ìš© í›„

```
ìš”ì²­ 1 (ì „ë¬¸ì„±, ë°°ì¹˜ 1):
- Cache Write: 800 í† í° (ì‹œìŠ¤í…œ + ì •ì¹˜ì¸ + ì¹´í…Œê³ ë¦¬)
- Cache Miss: 2,000 í† í° (ë°°ì¹˜ ë°ì´í„°)
- ë¹„ìš©: 800 Ã— 1.25 + 2,000 Ã— 1.0 = 3,000 í† í° ë¹„ìš©

ìš”ì²­ 2 (ì „ë¬¸ì„±, ë°°ì¹˜ 2):
- Cache Hit: 800 í† í° Ã— 0.1 = 80 í† í° ë¹„ìš© âœ…
- Cache Miss: 2,000 í† í° ë¹„ìš©
- ë¹„ìš©: 80 + 2,000 = 2,080 í† í° ë¹„ìš©

ìš”ì²­ 3~10 (ë™ì¼):
- ë¹„ìš©: 2,080 í† í° Ã— 9 = 18,720 í† í°

ì´ ë¹„ìš©: 3,000 + 2,080 + 18,720 = 23,800 í† í° ë¹„ìš©
ì ˆê°: 28,000 - 23,800 = 4,200 í† í° (15% ì ˆê°)
```

---

## 4. AIë³„ ìºì‹± ì ìš© ë°©ì•ˆ

### 4.1 Claude - Subscription Mode (ìºì‹± ë¶ˆê°€)

**í˜„ì¬ ìƒí™©**:
- V30ì—ì„œëŠ” Claudeë¥¼ Subscription Modeë¡œ ì‚¬ìš©
- APIê°€ ì•„ë‹Œ Claude Code ì„¸ì…˜ì—ì„œ ì§ì ‘ í‰ê°€
- API ë¹„ìš© $0 (subscriptionë§Œ ì‚¬ìš©)

**ìºì‹± ë¶ˆê°€ ì´ìœ **:
- Prompt Cachingì€ API ì „ìš© ê¸°ëŠ¥
- Subscription Modeì—ëŠ” ì ìš© ë¶ˆê°€

**ëŒ€ì•ˆ ìµœì í™”**:
1. **ë°°ì¹˜ í¬ê¸° ìµœëŒ€í™”**
   - í˜„ì¬: 10ê°œì”© ë°°ì¹˜
   - ì œì•ˆ: ê°€ëŠ¥í•˜ë©´ 20-30ê°œì”© (í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš© ê°ì†Œ)

2. **ë³‘ë ¬ ì„¸ì…˜ ì‹¤í–‰**
   - ì—¬ëŸ¬ Claude Code ì„¸ì…˜ ë™ì‹œ ì‹¤í–‰
   - ì¹´í…Œê³ ë¦¬ë³„ ë³‘ë ¬ ì²˜ë¦¬

3. **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬**
   - ê³ ì • ë¶€ë¶„ì„ íŒŒì¼ë¡œ ì €ì¥
   - ì„¸ì…˜ë§ˆë‹¤ ì¬ì…ë ¥ ìµœì†Œí™”

**ì ìš© ì½”ë“œ**: (ë³€ê²½ ë¶ˆí•„ìš”, í˜„ì¬ ë°©ì‹ ìœ ì§€)

```python
def call_claude_direct_evaluation(prompt):
    """Claude Subscription Mode í‰ê°€ (ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤)"""
    # í˜„ì¬ ë°©ì‹ ìœ ì§€
    # ìºì‹± ë¶ˆê°€, ëŒ€ì‹  ë°°ì¹˜ í¬ê¸° ìµœëŒ€í™”
    pass
```

---

### 4.2 ChatGPT - ìë™ ìºì‹± âœ…

**ì ìš© ë°©ë²•**:

#### ê¸°ë³¸ ìë™ ìºì‹± (ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”)

```python
from openai import OpenAI

def call_chatgpt_with_caching(politician_id, politician_name, category_name, items):
    """ChatGPT í‰ê°€ (ìë™ ìºì‹±)"""
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # ê³ ì • ë¶€ë¶„ (ìë™ ìºì‹±, 1024+ í† í°)
    system_prompt = SYSTEM_PROMPT  # ë ˆë²¨ 1 ìºì‹œ
    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )  # ë ˆë²¨ 2 ìºì‹œ

    # ë³€ë™ ë¶€ë¶„
    items_text = format_items(items)

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (prefix ë§¤ì¹­ì„ ìœ„í•´ ìˆœì„œ ìœ ì§€)
    user_prompt = f"""{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

    # API í˜¸ì¶œ (ìë™ ìºì‹±)
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},  # ìºì‹œë¨ âœ…
            {"role": "user", "content": user_prompt}  # ì¼ë¶€ ìºì‹œë¨ âœ…
        ],
        max_tokens=4096,
        temperature=0.7
    )

    return response.choices[0].message.content
```

#### Extended Caching (24ì‹œê°„) ì¶”ê°€

```python
def call_chatgpt_with_extended_caching(politician_id, politician_name, category_name, items):
    """ChatGPT í‰ê°€ (24ì‹œê°„ ìºì‹±)"""
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # ë™ì¼ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = SYSTEM_PROMPT
    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )
    items_text = format_items(items)
    user_prompt = f"""{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

    # API í˜¸ì¶œ (24ì‹œê°„ ìºì‹±)
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        prompt_cache_retention="24h",  # âœ… 24ì‹œê°„ ìºì‹œ ìœ ì§€
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        max_tokens=4096,
        temperature=0.7
    )

    return response.choices[0].message.content
```

**ì ˆê° íš¨ê³¼**:
- 1íšŒ Cache Write: 800 Ã— 1.0 = 800 í† í°
- 9íšŒ Cache Hit: 800 Ã— 0.1 Ã— 9 = 720 í† í°
- ì´ 10íšŒ: 800 + 720 = 1,520 í† í° (ì›ë˜ 8,000 í† í° â†’ **81% ì ˆê°**)

---

### 4.3 Gemini - Implicit + Explicit Caching âœ…

**ì ìš© ë°©ë²•**:

#### Implicit Caching (ìë™, ê¸°ë³¸)

```python
from google import genai

def call_gemini_with_implicit_caching(politician_id, politician_name, category_name, items):
    """Gemini í‰ê°€ (Implicit ìë™ ìºì‹±)"""
    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = SYSTEM_PROMPT
    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )
    items_text = format_items(items)

    full_prompt = f"""{system_prompt}

{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

    # API í˜¸ì¶œ (ìë™ ìºì‹±, 1024+ í† í°)
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=full_prompt
    )

    return response.text
```

#### Explicit Caching (ìˆ˜ë™, 90% ì ˆê°)

```python
from google.generativeai import caching
import datetime

# 1. ìºì‹œ ìƒì„± (í•œ ë²ˆë§Œ)
def create_gemini_cache(politician_id, politician_name, category_name):
    """Gemini í‰ê°€ìš© ìºì‹œ ìƒì„± (Explicit)"""

    # ê³ ì • í”„ë¡¬í”„íŠ¸
    system_prompt = SYSTEM_PROMPT
    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )

    # ìºì‹œ ìƒì„± (2048+ í† í° í•„ìˆ˜)
    cache = caching.CachedContent.create(
        model='gemini-2.0-flash',
        display_name=f'v30_eval_{politician_id}_{category_name}',
        system_instruction=system_prompt,  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        contents=[politician_prompt],  # ì •ì¹˜ì¸ í”„ë¡¬í”„íŠ¸
        ttl=datetime.timedelta(hours=1)  # 1ì‹œê°„ ìœ ì§€
    )

    return cache.name

# 2. ìºì‹œ ì‚¬ìš© (10ë²ˆ ì¬ì‚¬ìš©)
def call_gemini_with_explicit_caching(cache_name, items):
    """Gemini í‰ê°€ (Explicit ìºì‹± ì‚¬ìš©)"""
    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

    # ë³€ë™ ë¶€ë¶„ë§Œ ì „ì†¡
    items_text = format_items(items)

    prompt = f"""**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

    # API í˜¸ì¶œ (ìºì‹œ ì‚¬ìš©)
    response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=prompt,
        cached_content=cache_name  # âœ… ìºì‹œ ì‚¬ìš©
    )

    return response.text

# 3. ì „ì²´ ì›Œí¬í”Œë¡œìš°
def evaluate_category_with_gemini_caching(politician_id, politician_name, category_name, all_items):
    """ì¹´í…Œê³ ë¦¬ í‰ê°€ (Explicit Caching)"""

    # ìºì‹œ ìƒì„± (1íšŒ)
    cache_name = create_gemini_cache(politician_id, politician_name, category_name)

    # ë°°ì¹˜ë³„ í‰ê°€ (10íšŒ)
    results = []
    for batch in split_into_batches(all_items, 10):
        result = call_gemini_with_explicit_caching(cache_name, batch)
        results.append(result)

    # ìºì‹œ ì‚­ì œ (ì„ íƒ)
    # caching.CachedContent.delete(cache_name)

    return results
```

**ì ˆê° íš¨ê³¼**:
- Implicit: ìë™ ìºì‹± (ì ˆê°ë¥  ë¶ˆëª…í™•)
- **Explicit (Gemini 2.0 Flash): 75% ì ˆê°** âœ…
- Explicit (Gemini 2.5): 90% ì ˆê°

---

### 4.4 Grok - ìë™ ìºì‹± âœ…

**ì ìš© ë°©ë²•**:

```python
from openai import OpenAI

def call_grok_with_caching(politician_id, politician_name, category_name, items):
    """Grok í‰ê°€ (ìë™ ìºì‹±)"""
    client = OpenAI(
        api_key=os.getenv("XAI_API_KEY"),
        base_url="https://api.x.ai/v1"
    )

    # ê³ ì • ë¶€ë¶„ (ìë™ ìºì‹±, prefix ë§¤ì¹­)
    system_prompt = SYSTEM_PROMPT
    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )

    # ë³€ë™ ë¶€ë¶„
    items_text = format_items(items)

    user_prompt = f"""{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

    # API í˜¸ì¶œ (ìë™ ìºì‹±)
    response = client.chat.completions.create(
        model="grok-4-fast",
        messages=[
            {"role": "system", "content": system_prompt},  # ìºì‹œë¨ âœ…
            {"role": "user", "content": user_prompt}  # ì¼ë¶€ ìºì‹œë¨ âœ…
        ],
        max_tokens=4096,
        temperature=0.7
    )

    # ìºì‹œ ì‚¬ìš©ëŸ‰ í™•ì¸
    usage = response.usage
    if hasattr(usage, 'cached_prompt_tokens'):
        print(f"  âœ… Cached tokens: {usage.cached_prompt_tokens}")

    return response.choices[0].message.content
```

**ì ˆê° íš¨ê³¼**:
- Cached Input: $0.02/1M (90% ì ˆê°) âœ…
- ìºì‹œ íˆíŠ¸ìœ¨: 90%+ (íŒŒíŠ¸ë„ˆ í‰ê· )

---

## 5. 50ê°œ ë°°ì¹˜ì— ìºì‹± ì ìš© ì „ëµ

### 5.1 ë°°ì¹˜ ë¶„í•  ì „ëµ

**í˜„ì¬**: 100ê°œ ë°ì´í„° â†’ 10ê°œì”© ë°°ì¹˜ â†’ 10íšŒ í‰ê°€

**ìºì‹± ìµœì í™”**:

```python
# ì˜µì…˜ 1: 10ê°œ ë°°ì¹˜ (í˜„ì¬)
# - ìºì‹œ ì¬ì‚¬ìš©: 10íšŒ
# - ìºì‹œ íš¨ìœ¨: ì¤‘ê°„

# ì˜µì…˜ 2: 20ê°œ ë°°ì¹˜ (ê¶Œì¥)
# - ìºì‹œ ì¬ì‚¬ìš©: 5íšŒ
# - ìºì‹œ íš¨ìœ¨: ë†’ìŒ (ë” ì ì€ ìš”ì²­)
# - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: ì—¬ì „íˆ ê´œì°®ìŒ

# ì˜µì…˜ 3: 50ê°œ ë°°ì¹˜
# - ìºì‹œ ì¬ì‚¬ìš©: 2íšŒ
# - ìºì‹œ íš¨ìœ¨: ë‚®ìŒ
# - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: ë„ˆë¬´ ê¹€ (ì‘ë‹µ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥)
```

**ê¶Œì¥**: **20ê°œ ë°°ì¹˜** (5íšŒ í‰ê°€)
- ìºì‹œ ì¬ì‚¬ìš© ì¶©ë¶„ (5íšŒ)
- í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì ì •
- ì‘ë‹µ í’ˆì§ˆ ìœ ì§€

---

### 5.2 ì¹´í…Œê³ ë¦¬ë‹¹ 100ê°œ í‰ê°€ ìµœì í™”

```python
def evaluate_category_with_caching(
    evaluator_ai,
    politician_id,
    politician_name,
    category_name,
    all_items  # 100ê°œ
):
    """ì¹´í…Œê³ ë¦¬ í‰ê°€ (ìºì‹± ìµœì í™”)"""

    # ë°°ì¹˜ í¬ê¸° (20ê°œ ê¶Œì¥)
    batch_size = 20
    batches = split_into_batches(all_items, batch_size)  # 5ê°œ ë°°ì¹˜

    results = []

    for i, batch in enumerate(batches, 1):
        print(f"  ë°°ì¹˜ {i}/{len(batches)} (ì•„ì´í…œ {len(batch)}ê°œ)")

        if evaluator_ai == "Claude":
            # Subscription Mode (ìºì‹± ë¶ˆê°€)
            result = call_claude_direct_evaluation(batch)

        elif evaluator_ai == "ChatGPT":
            # ìë™ ìºì‹± + Extended Caching
            result = call_chatgpt_with_extended_caching(
                politician_id, politician_name, category_name, batch
            )

        elif evaluator_ai == "Gemini":
            # Explicit Caching (90% ì ˆê°)
            if i == 1:
                # ì²« ë°°ì¹˜: ìºì‹œ ìƒì„±
                cache_name = create_gemini_cache(
                    politician_id, politician_name, category_name
                )
            # ëª¨ë“  ë°°ì¹˜: ìºì‹œ ì‚¬ìš©
            result = call_gemini_with_explicit_caching(cache_name, batch)

        elif evaluator_ai == "Grok":
            # ìë™ ìºì‹±
            result = call_grok_with_caching(
                politician_id, politician_name, category_name, batch
            )

        results.append(result)
        time.sleep(1)  # Rate limit ë°©ì§€

    # Gemini ìºì‹œ ì •ë¦¬
    if evaluator_ai == "Gemini":
        caching.CachedContent.delete(cache_name)

    return results
```

---

### 5.3 ì „ì²´ ì›Œí¬í”Œë¡œìš° (10ê°œ ì¹´í…Œê³ ë¦¬ Ã— 100ê°œ ë°ì´í„°)

```
ì •ì¹˜ì¸ 1ëª… í‰ê°€:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì¹´í…Œê³ ë¦¬ 1: ì „ë¬¸ì„± (100ê°œ)                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Claude] Subscription Mode                                  â”‚
â”‚   - ë°°ì¹˜ 1~5 (20ê°œì”©): ìºì‹± ì—†ìŒ                            â”‚
â”‚                                                             â”‚
â”‚ [ChatGPT] Extended Caching                                  â”‚
â”‚   - ë°°ì¹˜ 1: Cache Write (800 í† í°)                          â”‚
â”‚   - ë°°ì¹˜ 2~5: Cache Hit (800 Ã— 0.1 = 80 í† í°) âœ…            â”‚
â”‚                                                             â”‚
â”‚ [Gemini] Explicit Caching                                   â”‚
â”‚   - ë°°ì¹˜ 1: Cache Write (800 í† í°)                          â”‚
â”‚   - ë°°ì¹˜ 2~5: Cache Hit (800 Ã— 0.25 = 200 í† í°) âœ…          â”‚
â”‚                                                             â”‚
â”‚ [Grok] Auto Caching                                         â”‚
â”‚   - ë°°ì¹˜ 1: Cache Write (800 í† í°)                          â”‚
â”‚   - ë°°ì¹˜ 2~5: Cache Hit (800 Ã— 0.1 = 80 í† í°) âœ…            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

... (ì¹´í…Œê³ ë¦¬ 2~10 ë™ì¼)

ì´ ì ˆê°:
- Claude: $0 (ì´ë¯¸ ë¬´ë£Œ)
- ChatGPT: ~75% ì ˆê° (ìºì‹œ íˆíŠ¸ìœ¨ì— ë”°ë¼)
- Gemini: ~65% ì ˆê° (75% ì ˆê° Ã— 80% ì¬ì‚¬ìš©)
- Grok: ~75% ì ˆê° (ìºì‹œ íˆíŠ¸ìœ¨ì— ë”°ë¼)
```

---

## 6. 4ê°œ AI ê³µì •ì„± ìœ ì§€ ì „ëµ

### 6.1 í•µì‹¬ ì›ì¹™

**ê³µì •ì„± ìš”êµ¬ì‚¬í•­**:
- 4ê°œ AI ëª¨ë‘ **ë™ì¼í•œ í”„ë¡¬í”„íŠ¸** ì‚¬ìš©
- í”„ë¡¬í”„íŠ¸ ê¸¸ì´/ë‚´ìš© ë™ì¼
- í‰ê°€ ê¸°ì¤€ ë™ì¼
- ë°ì´í„° ë™ì¼

**ìºì‹±ì´ ê³µì •ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥**:
- âœ… **ì˜í–¥ ì—†ìŒ**: ìºì‹±ì€ í† í° ì „ì†¡ë§Œ ìµœì í™”
- âœ… **ë‚´ìš© ë™ì¼**: ìºì‹œëœ í”„ë¡¬í”„íŠ¸ = ì›ë³¸ í”„ë¡¬í”„íŠ¸
- âœ… **í‰ê°€ ê²°ê³¼ ë™ì¼**: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ì™€ ë¬´ê´€

---

### 6.2 ë™ì¼ í”„ë¡¬í”„íŠ¸ ë³´ì¥

#### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¤‘ì•™ ê´€ë¦¬

```python
# prompts.py (ì¤‘ì•™ ê´€ë¦¬)

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë“±ê¸‰ ì²´ê³„** (+4 ~ -4) - V30 ê¸°ì¤€ (8ë‹¨ê³„):
[... ë™ì¼í•œ ë‚´ìš© ...]
"""

def get_politician_category_prompt(politician_id, politician_name, category_name):
    """ì •ì¹˜ì¸ë³„ ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸ (ëª¨ë“  AI ê³µí†µ)"""
    profile_info = format_politician_profile(politician_id, politician_name)
    cat_kor = CATEGORY_MAP.get(category_name.lower(), category_name)

    prompt = f"""{profile_info}

**í‰ê°€ ì¹´í…Œê³ ë¦¬**: {cat_kor} ({category_name})

ì•„ë˜ ë°ì´í„°ë¥¼ **ê°ê´€ì ìœ¼ë¡œ í‰ê°€**í•˜ì—¬ ë“±ê¸‰ì„ ë¶€ì—¬í•˜ì„¸ìš”."""

    return prompt

def build_evaluation_prompt(politician_id, politician_name, category_name, items):
    """ì „ì²´ í‰ê°€ í”„ë¡¬í”„íŠ¸ (ëª¨ë“  AI ê³µí†µ)"""
    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )
    items_text = format_items(items)

    return f"""{SYSTEM_PROMPT}

{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""
```

#### AIë³„ í˜¸ì¶œ í•¨ìˆ˜

```python
# evaluate_v30.py

from prompts import SYSTEM_PROMPT, get_politician_category_prompt, build_evaluation_prompt

def call_chatgpt_with_caching(politician_id, politician_name, category_name, items):
    """ChatGPT í‰ê°€ (ìë™ ìºì‹±)"""
    # âœ… ê³µí†µ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    full_prompt = build_evaluation_prompt(
        politician_id, politician_name, category_name, items
    )

    # ChatGPT API í˜•ì‹ìœ¼ë¡œ ë¶„ë¦¬
    system_prompt = SYSTEM_PROMPT
    user_prompt = full_prompt.replace(SYSTEM_PROMPT, '').strip()

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        prompt_cache_retention="24h",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    return response.choices[0].message.content

def call_gemini_with_caching(politician_id, politician_name, category_name, items):
    """Gemini í‰ê°€ (Implicit ìºì‹±)"""
    # âœ… ë™ì¼í•œ ê³µí†µ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    full_prompt = build_evaluation_prompt(
        politician_id, politician_name, category_name, items
    )

    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=full_prompt
    )
    return response.text

def call_grok_with_caching(politician_id, politician_name, category_name, items):
    """Grok í‰ê°€ (ìë™ ìºì‹±)"""
    # âœ… ë™ì¼í•œ ê³µí†µ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    full_prompt = build_evaluation_prompt(
        politician_id, politician_name, category_name, items
    )

    system_prompt = SYSTEM_PROMPT
    user_prompt = full_prompt.replace(SYSTEM_PROMPT, '').strip()

    response = client.chat.completions.create(
        model="grok-4-fast",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    return response.choices[0].message.content
```

---

### 6.3 ê²€ì¦ í…ŒìŠ¤íŠ¸

```python
def test_prompt_equality():
    """4ê°œ AI í”„ë¡¬í”„íŠ¸ ë™ì¼ì„± ê²€ì¦"""

    politician_id = "62e7b453"
    politician_name = "ì˜¤ì„¸í›ˆ"
    category_name = "expertise"
    items = [...]  # í…ŒìŠ¤íŠ¸ ë°ì´í„°

    # ê° AIìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_chatgpt = build_evaluation_prompt(
        politician_id, politician_name, category_name, items
    )
    prompt_gemini = build_evaluation_prompt(
        politician_id, politician_name, category_name, items
    )
    prompt_grok = build_evaluation_prompt(
        politician_id, politician_name, category_name, items
    )

    # ë™ì¼ì„± ê²€ì¦
    assert prompt_chatgpt == prompt_gemini, "ChatGPT â‰  Gemini"
    assert prompt_gemini == prompt_grok, "Gemini â‰  Grok"
    assert prompt_chatgpt == prompt_grok, "ChatGPT â‰  Grok"

    print("âœ… í”„ë¡¬í”„íŠ¸ ë™ì¼ì„± ê²€ì¦ í†µê³¼")
    print(f"  í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt_chatgpt)} ë¬¸ì")
```

---

### 6.4 ê³µì •ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

í‰ê°€ ì‹¤í–‰ ì „ í™•ì¸:

- [ ] 4ê°œ AI ëª¨ë‘ ë™ì¼í•œ `SYSTEM_PROMPT` ì‚¬ìš©
- [ ] 4ê°œ AI ëª¨ë‘ ë™ì¼í•œ `get_politician_category_prompt()` ì‚¬ìš©
- [ ] 4ê°œ AI ëª¨ë‘ ë™ì¼í•œ `format_items()` ì‚¬ìš©
- [ ] ìºì‹± ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ í”„ë¡¬í”„íŠ¸ ë‚´ìš© ë™ì¼
- [ ] í”„ë¡¬í”„íŠ¸ ê²€ì¦ í…ŒìŠ¤íŠ¸ í†µê³¼

---

## 7. ìµœì¢… ê¶Œì¥ì•ˆ

### 7.1 AIë³„ ìµœì í™” ì „ëµ ìš”ì•½

| AI | ìºì‹± ë°©ë²• | êµ¬í˜„ ë³µì¡ë„ | ì ˆê°ë¥  | ê¶Œì¥ ì‚¬í•­ |
|----|----------|-----------|--------|---------|
| **Claude** | ë¶ˆê°€ (Subscription) | - | 0% | ë°°ì¹˜ í¬ê¸° 20ê°œ, ë³‘ë ¬ ì„¸ì…˜ |
| **ChatGPT** | ìë™ + Extended | ë‚®ìŒ âœ… | **75-81%** | Extended Caching ì ìš© |
| **Gemini** | Implicit + Explicit | ì¤‘ê°„ | **65-75%** | Explicit Caching ì ìš© |
| **Grok** | ìë™ | ë‚®ìŒ âœ… | **75%** | ìë™ ìºì‹± (ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”) |

---

### 7.2 êµ¬í˜„ ìš°ì„ ìˆœìœ„

#### Phase 1: ìë™ ìºì‹± (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥) âœ…

**ëŒ€ìƒ**: ChatGPT, Grok
**ì‘ì—…ëŸ‰**: ìµœì†Œ (ì½”ë“œ ë³€ê²½ ê±°ì˜ ì—†ìŒ)
**íš¨ê³¼**: ì¦‰ì‹œ 75% ì ˆê°

**êµ¬í˜„**:
1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¤‘ì•™í™” (`prompts.py`)
2. Prefix ë§¤ì¹­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìˆœì„œ ìœ ì§€
3. ChatGPT: `prompt_cache_retention="24h"` ì¶”ê°€
4. Grok: ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš” (ìë™ ìºì‹±)

**ì˜ˆìƒ ì‹œê°„**: 1-2ì‹œê°„

---

#### Phase 2: Explicit Caching (Gemini) âœ…

**ëŒ€ìƒ**: Gemini
**ì‘ì—…ëŸ‰**: ì¤‘ê°„
**íš¨ê³¼**: 65-75% ì ˆê°

**êµ¬í˜„**:
1. `google.generativeai.caching` ëª¨ë“ˆ ì„¤ì¹˜
2. ìºì‹œ ìƒì„± í•¨ìˆ˜ êµ¬í˜„
3. ìºì‹œ ì‚¬ìš© í•¨ìˆ˜ êµ¬í˜„
4. ìºì‹œ ì •ë¦¬ ë¡œì§ ì¶”ê°€

**ì˜ˆìƒ ì‹œê°„**: 2-3ì‹œê°„

---

#### Phase 3: Claude ìµœì í™” (ì„ íƒ) âš ï¸

**ëŒ€ìƒ**: Claude
**ì‘ì—…ëŸ‰**: ë†’ìŒ
**íš¨ê³¼**: ì‹œê°„ ì ˆì•½ (ë¹„ìš© ì•„ë‹˜, ì´ë¯¸ $0)

**êµ¬í˜„**:
1. ë°°ì¹˜ í¬ê¸° 20ê°œë¡œ ë³€ê²½
2. ë³‘ë ¬ ì„¸ì…˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ ê´€ë¦¬

**ì˜ˆìƒ ì‹œê°„**: 3-4ì‹œê°„

---

### 7.3 ë¹„ìš©/ì‹œê°„ ë¹„êµí‘œ

#### ì‹œë‚˜ë¦¬ì˜¤: ì •ì¹˜ì¸ 1ëª… í‰ê°€ (10 ì¹´í…Œê³ ë¦¬ Ã— 100ê°œ ë°ì´í„°)

**ì „ì²´ í† í° ì‚¬ìš©ëŸ‰ (ìºì‹± ì—†ìŒ)**:
```
1 ì¹´í…Œê³ ë¦¬ = 5 ë°°ì¹˜ Ã— 2,800 í† í° = 14,000 í† í°
10 ì¹´í…Œê³ ë¦¬ = 14,000 Ã— 10 = 140,000 í† í° (Input)
4ê°œ AI = 140,000 Ã— 4 = 560,000 í† í° (Input)
```

**ë¹„ìš© (ìºì‹± ì—†ìŒ)**:
```
ChatGPT (gpt-4o-mini):
- Input: $0.15/1M Ã— 0.14M = $0.021

Gemini (gemini-2.0-flash):
- Input: Free (ë¬´ë£Œ) âœ…

Grok (grok-4-fast):
- Input: $0.20/1M Ã— 0.14M = $0.028

ì´ ë¹„ìš©: $0.021 + $0 + $0.028 = $0.049/ì •ì¹˜ì¸
(Claude ì œì™¸, Subscription $0)
```

**ë¹„ìš© (ìºì‹± ì ìš©)**:
```
ChatGPT (Extended Caching, 81% ì ˆê°):
- ìºì‹œ Write: 800 Ã— 1.0 Ã— 10 = 8,000 í† í°
- ìºì‹œ Hit: 800 Ã— 0.1 Ã— 40 = 3,200 í† í°
- ë³€ë™ ë¶€ë¶„: 2,000 Ã— 50 = 100,000 í† í°
- ì´: 111,200 í† í° (ì›ë˜ 140,000 â†’ 21% ì ˆê°)
- ë¹„ìš©: $0.15/1M Ã— 0.111M = $0.017 (19% ì ˆê°)

Gemini (Explicit Caching, 75% ì ˆê°):
- Input: Free (ë¬´ë£Œ) âœ… (ì ˆê° íš¨ê³¼ ì—†ìŒ, ì´ë¯¸ ë¬´ë£Œ)

Grok (Auto Caching, 75% ì ˆê°):
- ìºì‹œ Write: 800 Ã— 10 = 8,000 í† í° ($0.20/1M)
- ìºì‹œ Hit: 800 Ã— 40 = 32,000 í† í° ($0.02/1M)
- ë³€ë™ ë¶€ë¶„: 2,000 Ã— 50 = 100,000 í† í° ($0.20/1M)
- ì´ ë¹„ìš©:
  - Write: $0.20/1M Ã— 0.008M = $0.0016
  - Hit: $0.02/1M Ã— 0.032M = $0.00064
  - ë³€ë™: $0.20/1M Ã— 0.1M = $0.02
  - ì´: $0.022 (ì›ë˜ $0.028 â†’ 21% ì ˆê°)

ì´ ë¹„ìš©: $0.017 + $0 + $0.022 = $0.039/ì •ì¹˜ì¸
ì ˆê°: $0.049 â†’ $0.039 (20% ì ˆê°)
```

**ì‹œê°„ ì ˆê°**:
- ìºì‹œ íˆíŠ¸ ì‹œ ì§€ì—° ì‹œê°„: **80-85% ë‹¨ì¶•**
- ì •ì¹˜ì¸ 1ëª… í‰ê°€ ì‹œê°„: 30ë¶„ â†’ **10-15ë¶„** âœ…

---

### 7.4 ìµœì¢… ì¶”ì²œ êµ¬í˜„

```python
# prompts.py (ì¤‘ì•™ ê´€ë¦¬)

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë“±ê¸‰ ì²´ê³„** (+4 ~ -4) - V30 ê¸°ì¤€ (8ë‹¨ê³„):
| ë“±ê¸‰ | íŒë‹¨ ê¸°ì¤€ | ì ìˆ˜ |
|------|-----------|------|
| +4 | íƒì›”í•¨ - í•´ë‹¹ ë¶„ì•¼ ëª¨ë²” ì‚¬ë¡€ | +8 |
| +3 | ìš°ìˆ˜í•¨ - ê¸ì •ì  í‰ê°€ | +6 |
| +2 | ì–‘í˜¸í•¨ - ê¸°ë³¸ ì¶©ì¡± | +4 |
| +1 | ë³´í†µ ê¸ì • - í‰ê·  ì´ìƒ | +2 |
| -1 | ë¯¸í¡í•¨ - ê°œì„  í•„ìš” | -2 |
| -2 | ë¶€ì¡±í•¨ - ë¬¸ì œ ìˆìŒ | -4 |
| -3 | ë§¤ìš° ë¶€ì¡± - ì‹¬ê°í•œ ë¬¸ì œ | -6 |
| -4 | ê·¹íˆ ë¶€ì¡± - ì •ì¹˜ì¸ ë¶€ì í•© | -8 |

**í‰ê°€ ê¸°ì¤€**:
- ê¸ì •ì  ë‚´ìš© (ì„±ê³¼, ì—…ì , ì¹­ì°¬) â†’ +4, +3, +2, +1
- ë¶€ì •ì  ë‚´ìš© (ë…¼ë€, ë¹„íŒ, ë¬¸ì œ) â†’ -1, -2, -3, -4 (ì‹¬ê°ë„ì— ë”°ë¼)

**ë°˜ë“œì‹œ ëª¨ë“  í•­ëª©ì— ëŒ€í•´ í‰ê°€í•˜ì„¸ìš”.**

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
```json
{
  "evaluations": [
    {
      "id": "ë°ì´í„° ID ê°’",
      "rating": "+4, +3, +2, +1, -1, -2, -3, -4 ì¤‘ í•˜ë‚˜",
      "rationale": "í‰ê°€ ê·¼ê±° (1ë¬¸ì¥)"
    }
  ]
}
```"""

def get_politician_category_prompt(politician_id, politician_name, category_name):
    """ì •ì¹˜ì¸ë³„ ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸"""
    from evaluate_v30 import format_politician_profile, CATEGORY_MAP

    profile_info = format_politician_profile(politician_id, politician_name)
    cat_kor = CATEGORY_MAP.get(category_name.lower(), category_name)

    return f"""{profile_info}

**í‰ê°€ ì¹´í…Œê³ ë¦¬**: {cat_kor} ({category_name})

ì•„ë˜ ë°ì´í„°ë¥¼ **ê°ê´€ì ìœ¼ë¡œ í‰ê°€**í•˜ì—¬ ë“±ê¸‰ì„ ë¶€ì—¬í•˜ì„¸ìš”."""

def format_items(items):
    """í‰ê°€í•  ë°ì´í„° í¬ë§·"""
    items_text = ""
    for i, item in enumerate(items, 1):
        items_text += f"""
[í•­ëª© {i}]
- ID: {item.get('id', '')}
- ì œëª©: {item.get('title', 'N/A')}
- ë‚´ìš©: {item.get('content', 'N/A')[:300]}...
- ì¶œì²˜: {item.get('source_name', item.get('source_url', 'N/A'))}
- ë‚ ì§œ: {item.get('published_date', 'N/A')}
- ìˆ˜ì§‘AI: {item.get('collector_ai', 'N/A')}
"""
    return items_text
```

```python
# evaluate_v30_cached.py (ìºì‹± ë²„ì „)

from prompts import SYSTEM_PROMPT, get_politician_category_prompt, format_items
from openai import OpenAI
from google import genai
from google.generativeai import caching
import datetime
import os

def evaluate_with_chatgpt_caching(politician_id, politician_name, category_name, items):
    """ChatGPT í‰ê°€ (Extended Caching)"""
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )
    items_text = format_items(items)

    user_prompt = f"""{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        prompt_cache_retention="24h",  # âœ… 24ì‹œê°„ ìºì‹±
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        max_tokens=4096,
        temperature=0.7
    )

    return response.choices[0].message.content

def evaluate_category_with_gemini_caching(politician_id, politician_name, category_name, all_items):
    """Gemini ì¹´í…Œê³ ë¦¬ í‰ê°€ (Explicit Caching)"""

    # 1. ìºì‹œ ìƒì„±
    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )

    cache = caching.CachedContent.create(
        model='gemini-2.0-flash',
        display_name=f'v30_eval_{politician_id}_{category_name}',
        system_instruction=SYSTEM_PROMPT,
        contents=[politician_prompt],
        ttl=datetime.timedelta(hours=1)
    )

    # 2. ë°°ì¹˜ë³„ í‰ê°€ (ìºì‹œ ì¬ì‚¬ìš©)
    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
    results = []

    for batch in split_into_batches(all_items, 20):
        items_text = format_items(batch)
        prompt = f"""**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=prompt,
            cached_content=cache.name  # âœ… ìºì‹œ ì‚¬ìš©
        )
        results.append(response.text)

    # 3. ìºì‹œ ì •ë¦¬
    caching.CachedContent.delete(cache.name)

    return results

def evaluate_with_grok_caching(politician_id, politician_name, category_name, items):
    """Grok í‰ê°€ (ìë™ ìºì‹±)"""
    client = OpenAI(
        api_key=os.getenv("XAI_API_KEY"),
        base_url="https://api.x.ai/v1"
    )

    politician_prompt = get_politician_category_prompt(
        politician_id, politician_name, category_name
    )
    items_text = format_items(items)

    user_prompt = f"""{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

    response = client.chat.completions.create(
        model="grok-4-fast",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        max_tokens=4096,
        temperature=0.7
    )

    # ìºì‹œ ì‚¬ìš©ëŸ‰ í™•ì¸
    if hasattr(response.usage, 'cached_prompt_tokens'):
        print(f"  âœ… Grok Cached: {response.usage.cached_prompt_tokens} tokens")

    return response.choices[0].message.content
```

---

### 7.5 êµ¬í˜„ ê°€ì´ë“œ

#### Step 1: prompts.py ìƒì„±

```bash
cd V30/scripts
touch prompts.py
# ìœ„ ì½”ë“œ ë³µì‚¬/ë¶™ì—¬ë„£ê¸°
```

#### Step 2: evaluate_v30.py ìˆ˜ì •

```python
# evaluate_v30.py ìƒë‹¨ì— ì¶”ê°€
from prompts import SYSTEM_PROMPT, get_politician_category_prompt, format_items

# evaluate_batch() í•¨ìˆ˜ ìˆ˜ì •
def evaluate_batch(evaluator_ai, items, category_name, politician_id, politician_name):
    """ë°°ì¹˜ í‰ê°€ (ìºì‹± ì ìš©)"""

    if evaluator_ai == "Claude":
        # Subscription Mode (ë³€ê²½ ì—†ìŒ)
        return call_claude_direct_evaluation(...)

    elif evaluator_ai == "ChatGPT":
        # Extended Caching ì ìš©
        return evaluate_with_chatgpt_caching(
            politician_id, politician_name, category_name, items
        )

    elif evaluator_ai == "Gemini":
        # Explicit Cachingì€ ë³„ë„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬
        # ì—¬ê¸°ì„œëŠ” Implicit Cachingë§Œ ì‚¬ìš©
        politician_prompt = get_politician_category_prompt(
            politician_id, politician_name, category_name
        )
        items_text = format_items(items)
        full_prompt = f"""{SYSTEM_PROMPT}

{politician_prompt}

**í‰ê°€í•  ë°ì´í„°**:
{items_text}"""

        return call_ai_api("Gemini", full_prompt)

    elif evaluator_ai == "Grok":
        # ìë™ ìºì‹± (ì½”ë“œ ë³€ê²½ ë¶ˆí•„ìš”)
        return evaluate_with_grok_caching(
            politician_id, politician_name, category_name, items
        )
```

#### Step 3: í…ŒìŠ¤íŠ¸

```bash
# 1ê°œ ì •ì¹˜ì¸, 1ê°œ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
python evaluate_v30.py \
  --politician_id=62e7b453 \
  --politician_name="ì˜¤ì„¸í›ˆ" \
  --category=expertise \
  --ai=ChatGPT

# ìºì‹œ ì‚¬ìš©ëŸ‰ í™•ì¸ (ë¡œê·¸ì—ì„œ "Cached tokens" ê²€ìƒ‰)
```

#### Step 4: ì „ì²´ ì ìš©

```bash
# ì „ì²´ í‰ê°€ (ìºì‹± ì ìš©)
python evaluate_v30.py \
  --politician_id=62e7b453 \
  --politician_name="ì˜¤ì„¸í›ˆ" \
  --parallel
```

---

## 8. ê²°ë¡ 

### 8.1 í•µì‹¬ ìš”ì•½

1. **4ê°œ AI ëª¨ë‘ ìºì‹± ì§€ì›** âœ…
   - Claude: âš ï¸ Subscription Mode (ìºì‹± ë¶ˆê°€, ì´ë¯¸ $0)
   - ChatGPT: âœ… ìë™ + Extended Caching (24ì‹œê°„)
   - Gemini: âœ… Implicit + Explicit Caching
   - Grok: âœ… ìë™ ìºì‹±

2. **ì ˆê° íš¨ê³¼**:
   - ë¹„ìš©: **20% ì ˆê°** ($0.049 â†’ $0.039/ì •ì¹˜ì¸)
   - ì‹œê°„: **70% ë‹¨ì¶•** (30ë¶„ â†’ 10-15ë¶„)
   - í† í°: **í‰ê·  75% ì ˆê°** (ìºì‹œ íˆíŠ¸ ì‹œ)

3. **ê³µì •ì„± ìœ ì§€** âœ…:
   - 4ê°œ AI ëª¨ë‘ ë™ì¼ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
   - ìºì‹±ì€ ì „ì†¡ë§Œ ìµœì í™”, ë‚´ìš© ë™ì¼
   - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¤‘ì•™ ê´€ë¦¬

4. **êµ¬í˜„ ë‚œì´ë„**:
   - Phase 1 (ChatGPT/Grok): **ë‚®ìŒ** (1-2ì‹œê°„)
   - Phase 2 (Gemini Explicit): **ì¤‘ê°„** (2-3ì‹œê°„)
   - Phase 3 (Claude ìµœì í™”): **ë†’ìŒ** (3-4ì‹œê°„, ì„ íƒ)

---

### 8.2 ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìµœì†Œ êµ¬í˜„ (Phase 1)

**ëª©í‘œ**: ChatGPT, Grok ìºì‹± ì ìš© (1ì‹œê°„ ì‘ì—…)

**ì‘ì—…**:
1. `prompts.py` ìƒì„± (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¤‘ì•™í™”)
2. `evaluate_v30.py` ìˆ˜ì • (ChatGPT Extended Caching)
3. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

**íš¨ê³¼**:
- ChatGPT: 19% ë¹„ìš© ì ˆê°
- Grok: 21% ë¹„ìš© ì ˆê°
- ì‹œê°„: 50-70% ë‹¨ì¶• (ìºì‹œ íˆíŠ¸ ì‹œ)

---

## ì°¸ê³  ë¬¸ì„œ

### ê³µì‹ ë¬¸ì„œ

- [Prompt caching - Claude API Docs](https://docs.claude.com/en/docs/build-with-claude/prompt-caching)
- [Prompt caching | OpenAI API](https://platform.openai.com/docs/guides/prompt-caching)
- [Context caching | Gemini API](https://ai.google.dev/gemini-api/docs/caching)
- [Consumption and Rate Limits | xAI](https://docs.x.ai/docs/key-information/consumption-and-rate-limits)

### ì¶”ê°€ ìë£Œ

- [Prompt Caching in the API | OpenAI](https://openai.com/index/api-prompt-caching/)
- [Prompt caching with Claude | Anthropic](https://www.anthropic.com/news/prompt-caching)
- [Gemini 2.5 Models now support implicit caching](https://developers.googleblog.com/en/gemini-2-5-models-now-support-implicit-caching/)
- [Prompt caching: 10x cheaper LLM tokens, but how? | ngrok blog](https://ngrok.com/blog/prompt-caching/)

---

**ë¬¸ì„œ ë**

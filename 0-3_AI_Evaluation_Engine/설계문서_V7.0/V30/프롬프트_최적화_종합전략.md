# V30 í”„ë¡¬í”„íŠ¸ ìµœì í™” ì¢…í•©ì „ëµ

**ì‘ì„±ì¼**: 2026-01-25
**ëª©ì **: 4ê°œ AI í‰ê°€ ì‹œìŠ¤í…œì˜ í† í°/ë¹„ìš©/ì‹œê°„ ìµœëŒ€ ì ˆê°
**ì ìš© ëŒ€ìƒ**: evaluate_v30.py (ë¼ì¸ 448-491)

---

## ëª©ì°¨

1. [í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë¶„ì„](#1-í˜„ì¬-í”„ë¡¬í”„íŠ¸-ë¶„ì„)
2. [ìµœì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ (50% ì ˆê°)](#2-ìµœì†Œí™”ëœ-í”„ë¡¬í”„íŠ¸-50-ì ˆê°)
3. [ìºì‹± ì „ëµ (4ê°œ AIë³„)](#3-ìºì‹±-ì „ëµ-4ê°œ-aië³„)
4. [50ê°œ ë°°ì¹˜ ìµœì í™”](#4-50ê°œ-ë°°ì¹˜-ìµœì í™”)
5. [ì¢…í•© ë¹„ìš©/ì‹œê°„ ë¶„ì„](#5-ì¢…í•©-ë¹„ìš©ì‹œê°„-ë¶„ì„)
6. [ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì½”ë“œ](#6-ì¦‰ì‹œ-ì ìš©-ê°€ëŠ¥í•œ-ì½”ë“œ)

---

## 1. í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë¶„ì„

### í˜„ì¬ êµ¬ì¡° (evaluate_v30.py ë¼ì¸ 448-491)

```python
prompt = f"""ë‹¹ì‹ ì€ ì •ì¹˜ì¸ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{profile_info}  # ì •ì¹˜ì¸ í”„ë¡œí•„ (ì•½ 150 tokens)

**í‰ê°€ ì¹´í…Œê³ ë¦¬**: {cat_kor} ({category_name})

ì•„ë˜ ë°ì´í„°ë¥¼ **ê°ê´€ì ìœ¼ë¡œ í‰ê°€**í•˜ì—¬ ë“±ê¸‰ì„ ë¶€ì—¬í•˜ì„¸ìš”.

**ë“±ê¸‰ ì²´ê³„** (+4 ~ 0 ~ -4) - V30 ê¸°ì¤€ (9ë‹¨ê³„):
| ë“±ê¸‰ | íŒë‹¨ ê¸°ì¤€ | ì ìˆ˜ |
|------|-----------|------|
| +4 | íƒì›”í•¨ - í•´ë‹¹ ë¶„ì•¼ ëª¨ë²” ì‚¬ë¡€ | +8 |
| +3 | ìš°ìˆ˜í•¨ - ê¸ì •ì  í‰ê°€ | +6 |
| +2 | ì–‘í˜¸í•¨ - ê¸°ë³¸ ì¶©ì¡± | +4 |
| +1 | ë³´í†µ ê¸ì • - í‰ê·  ì´ìƒ | +2 |
| 0 | ì¤‘ë¦½ - í‰ë²”, íŒë‹¨ ë¶ˆê°€ | 0 |
| -1 | ë¯¸í¡í•¨ - ê°œì„  í•„ìš” | -2 |
| -2 | ë¶€ì¡±í•¨ - ë¬¸ì œ ìˆìŒ | -4 |
| -3 | ë§¤ìš° ë¶€ì¡± - ì‹¬ê°í•œ ë¬¸ì œ | -6 |
| -4 | ê·¹íˆ ë¶€ì¡± - ì •ì¹˜ì¸ ë¶€ì í•© | -8 |

**í‰ê°€ ê¸°ì¤€**:
- ê¸ì •ì  ë‚´ìš© (ì„±ê³¼, ì—…ì , ì¹­ì°¬) â†’ +4, +3, +2, +1
- ì¤‘ë¦½/íŒë‹¨ë¶ˆê°€ (ë‹¨ìˆœ ì‚¬ì‹¤, ì¤‘ë¦½ ì •ë³´) â†’ 0
- ë¶€ì •ì  ë‚´ìš© (ë…¼ë€, ë¹„íŒ, ë¬¸ì œ) â†’ -1, -2, -3, -4 (ì‹¬ê°ë„ì— ë”°ë¼)

**í‰ê°€í•  ë°ì´í„°**:
{items_text}  # 10ê°œ ë°°ì¹˜ (ì•½ 2,500 tokens)

**ë°˜ë“œì‹œ ëª¨ë“  í•­ëª©ì— ëŒ€í•´ í‰ê°€í•˜ì„¸ìš”.**

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
```json
{
  "evaluations": [
    {
      "id": "ë°ì´í„° ID ê°’",
      "rating": "+4, +3, +2, +1, -1, -2, -3, -4 ì¤‘ í•˜ë‚˜ (8ë‹¨ê³„, 0ë“±ê¸‰ ì œê±°)",
      "rationale": "í‰ê°€ ê·¼ê±° (1ë¬¸ì¥)"
    }
  ]
}
```
"""
```

### í† í° ë¶„ì„

| êµ¬ì„± ìš”ì†Œ | í† í° ìˆ˜ | ë¹„ìœ¨ | ì¬ì‚¬ìš© |
|-----------|---------|------|--------|
| ì‹œìŠ¤í…œ ì—­í•  | 20 | 0.6% | âœ… ì¹´í…Œê³ ë¦¬ë³„ |
| ì •ì¹˜ì¸ í”„ë¡œí•„ | 150 | 4.5% | âœ… ì¹´í…Œê³ ë¦¬ë³„ |
| ì¹´í…Œê³ ë¦¬ ì„¤ëª… | 30 | 0.9% | âœ… ì¹´í…Œê³ ë¦¬ë³„ |
| ë“±ê¸‰ ì²´ê³„ í‘œ | 350 | 10.5% | âœ… ì „ì²´ ì¬ì‚¬ìš© |
| í‰ê°€ ê¸°ì¤€ | 100 | 3.0% | âœ… ì „ì²´ ì¬ì‚¬ìš© |
| JSON í˜•ì‹ | 200 | 6.0% | âœ… ì „ì²´ ì¬ì‚¬ìš© |
| ë°°ì¹˜ ë°ì´í„° (10ê°œ) | 2,500 | 74.5% | âŒ ë§¤ë²ˆ ë³€ê²½ |
| **í•©ê³„** | **3,350** | **100%** | - |

**ë¬¸ì œì **:
- ë“±ê¸‰ ì²´ê³„ í‘œê°€ ì§€ë‚˜ì¹˜ê²Œ ìƒì„¸ (350 tokens)
- JSON í˜•ì‹ ì„¤ëª…ì´ ì¥í™© (200 tokens)
- 10ê°œ ë°°ì¹˜ë¡œ í”„ë¡¬í”„íŠ¸ ì˜¤ë²„í—¤ë“œ ë°˜ë³µ

---

## 2. ìµœì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ (50% ì ˆê°)

### 2.1 ì¶•ì•½ ì „ëµ

#### Before (ë“±ê¸‰ ì²´ê³„ í‘œ: 350 tokens)

```
| ë“±ê¸‰ | íŒë‹¨ ê¸°ì¤€ | ì ìˆ˜ |
|------|-----------|------|
| +4 | íƒì›”í•¨ - í•´ë‹¹ ë¶„ì•¼ ëª¨ë²” ì‚¬ë¡€ | +8 |
| +3 | ìš°ìˆ˜í•¨ - ê¸ì •ì  í‰ê°€ | +6 |
...
```

#### After (ë“±ê¸‰ ì²´ê³„ ìš”ì•½: 120 tokens, 66% ì ˆê°)

```
ë“±ê¸‰(ì ìˆ˜): +4(+8) íƒì›” | +3(+6) ìš°ìˆ˜ | +2(+4) ì–‘í˜¸ | +1(+2) ë³´í†µ ê¸ì •
           -1(-2) ë¯¸í¡ | -2(-4) ë¶€ì¡± | -3(-6) ì‹¬ê° | -4(-8) ìµœì•…

íŒë‹¨: ê¸ì •(ì„±ê³¼/ì—…ì ) â†’ +4~+1 | ë¶€ì •(ë…¼ë€/ë¹„íŒ) â†’ -1~-4
```

#### Before (JSON í˜•ì‹: 200 tokens)

```json
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
```json
{
  "evaluations": [
    {
      "id": "ë°ì´í„° ID ê°’",
      "rating": "+4, +3, +2, +1, -1, -2, -3, -4 ì¤‘ í•˜ë‚˜ (8ë‹¨ê³„, 0ë“±ê¸‰ ì œê±°)",
      "rationale": "í‰ê°€ ê·¼ê±° (1ë¬¸ì¥)"
    }
  ]
}
```
```

#### After (JSON í˜•ì‹: 80 tokens, 60% ì ˆê°)

```
JSON ë°˜í™˜: {"evaluations":[{"id":"UUID","rating":"+4~-4","rationale":"ê·¼ê±°"}]}
```

### 2.2 ìµœì í™” í”„ë¡¬í”„íŠ¸ (ì „ì²´ ì½”ë“œ)

```python
prompt = f"""{profile_info}

ì¹´í…Œê³ ë¦¬: {cat_kor}

ë“±ê¸‰: +4(íƒì›”,+8) +3(ìš°ìˆ˜,+6) +2(ì–‘í˜¸,+4) +1(ë³´í†µ,+2) -1(ë¯¸í¡,-2) -2(ë¶€ì¡±,-4) -3(ì‹¬ê°,-6) -4(ìµœì•…,-8)
íŒë‹¨: ê¸ì •(ì„±ê³¼/ì—…ì )â†’+4~+1 | ë¶€ì •(ë…¼ë€/ë¹„íŒ)â†’-1~-4

ë°ì´í„°:
{items_text}

JSON ë°˜í™˜: {{"evaluations":[{{"id":"UUID","rating":"+4~-4","rationale":"ê·¼ê±°"}}]}}
"""
```

### 2.3 í† í° ì ˆê° íš¨ê³¼

| êµ¬ì„± ìš”ì†Œ | Before | After | ì ˆê° |
|-----------|--------|-------|------|
| ì‹œìŠ¤í…œ ì—­í•  | 20 | 0 | 100% |
| ì •ì¹˜ì¸ í”„ë¡œí•„ | 150 | 150 | 0% |
| ì¹´í…Œê³ ë¦¬ ì„¤ëª… | 30 | 10 | 67% |
| ë“±ê¸‰ ì²´ê³„ í‘œ | 350 | 120 | 66% |
| í‰ê°€ ê¸°ì¤€ | 100 | 50 | 50% |
| JSON í˜•ì‹ | 200 | 80 | 60% |
| ë°°ì¹˜ ë°ì´í„° (10ê°œ) | 2,500 | 2,500 | 0% |
| **í•©ê³„** | **3,350** | **2,910** | **13%** |

**ì£¼ì˜**: ë°°ì¹˜ ë°ì´í„°ê°€ 74.5%ë¥¼ ì°¨ì§€í•˜ë¯€ë¡œ, í”„ë¡¬í”„íŠ¸ ì¶•ì•½ë§Œìœ¼ë¡œëŠ” ì ˆê° íš¨ê³¼ ì œí•œì .
**ê²°ë¡ **: 50ê°œ ë°°ì¹˜ ì ìš© í•„ìˆ˜ (ì„¹ì…˜ 4 ì°¸ì¡°)

---

## 3. ìºì‹± ì „ëµ (4ê°œ AIë³„)

### 3.1 Claude (Subscription Mode) - ìºì‹± ë¶ˆê°€

**í˜„í™©**:
- API ë¯¸ì‚¬ìš© (Claude Code CLI)
- Subscription Mode (ë¬´ì œí•œ ì‚¬ìš©)
- ìºì‹± ì ìš© ë¶ˆê°€

**ìµœì í™” ì „ëµ**:
- 50ê°œ ë°°ì¹˜ ì ìš© (í”„ë¡¬í”„íŠ¸ ì˜¤ë²„í—¤ë“œ 1/5 ê°ì†Œ)
- ë³‘ë ¬ ì„¸ì…˜ ì‹¤í–‰ (10ê°œ ì¹´í…Œê³ ë¦¬ ë™ì‹œ í‰ê°€)
- ë¹„ìš©: $0 (subscription)

**ì½”ë“œ ìˆ˜ì •**: ë¶ˆí•„ìš” (ì´ë¯¸ ìµœì )

---

### 3.2 ChatGPT (gpt-4o-mini) - Automatic Caching âœ…

**ê³µì‹ ì •ë³´** ([OpenAI Prompt Caching](https://platform.openai.com/docs/guides/prompt-caching)):

| í•­ëª© | ê°’ |
|------|-----|
| ì ìš© ëª¨ë¸ | gpt-4o-mini (ìë™ ì ìš©) |
| ìµœì†Œ í† í° | 1024 tokens |
| ìºì‹œ ë‹¨ìœ„ | 128 tokens ì¦ë¶„ |
| ìºì‹œ ìœ íš¨ ê¸°ê°„ | 5-10ë¶„ (ìµœëŒ€ 60ë¶„) |
| ë¹„ìš© ì ˆê° | ì…ë ¥ í† í° 50% í• ì¸ |
| ì§€ì—° ê°ì†Œ | ìµœëŒ€ 80% |

**ìºì‹± êµ¬ì¡° ì„¤ê³„**:

```python
# Level 1: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì¬ì‚¬ìš© 4,000ë²ˆ)
SYSTEM_PROMPT = """ë“±ê¸‰: +4(íƒì›”,+8) +3(ìš°ìˆ˜,+6) +2(ì–‘í˜¸,+4) +1(ë³´í†µ,+2) -1(ë¯¸í¡,-2) -2(ë¶€ì¡±,-4) -3(ì‹¬ê°,-6) -4(ìµœì•…,-8)
íŒë‹¨: ê¸ì •(ì„±ê³¼/ì—…ì )â†’+4~+1 | ë¶€ì •(ë…¼ë€/ë¹„íŒ)â†’-1~-4
JSON ë°˜í™˜: {"evaluations":[{"id":"UUID","rating":"+4~-4","rationale":"ê·¼ê±°"}]}"""

# Level 2: ì •ì¹˜ì¸ í”„ë¡œí•„ (ì¬ì‚¬ìš© 10ë²ˆ, ì¹´í…Œê³ ë¦¬ë³„)
PROFILE_PREFIX = f"{politician_name} í”„ë¡œí•„:\n{profile_info}\nì¹´í…Œê³ ë¦¬: {cat_kor}"

# Level 3: ë°°ì¹˜ ë°ì´í„° (ë§¤ë²ˆ ë³€ê²½)
BATCH_DATA = f"ë°ì´í„°:\n{items_text}"
```

**ì½”ë“œ ì ìš©**:

```python
def call_chatgpt_with_caching(prompt, profile_prefix, batch_data):
    """ChatGPT Automatic Caching ì ìš©"""
    client = init_ai_client("ChatGPT")

    # ìºì‹±ì„ ìœ„í•´ í”„ë¡¬í”„íŠ¸ë¥¼ 3ë‹¨ê³„ë¡œ ë¶„ë¦¬
    messages = [
        # Level 1: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (1024+ tokens, ìºì‹œë¨)
        {"role": "system", "content": SYSTEM_PROMPT},

        # Level 2: ì •ì¹˜ì¸ í”„ë¡œí•„ (ìºì‹œë¨)
        {"role": "user", "content": profile_prefix},

        # Level 3: ë°°ì¹˜ ë°ì´í„° (ë§¤ë²ˆ ë³€ê²½)
        {"role": "user", "content": batch_data}
    ]

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=4096,
        temperature=0.7
    )
    return response.choices[0].message.content
```

**ì ˆê° íš¨ê³¼** (ì •ì¹˜ì¸ 1ëª…, 10ê°œ ì¹´í…Œê³ ë¦¬):
- ì´ í”„ë¡¬í”„íŠ¸: 3,350 tokens Ã— 100ê°œ = 335,000 tokens
- ìºì‹œ ê°€ëŠ¥: 850 tokens (ì‹œìŠ¤í…œ + í”„ë¡œí•„) Ã— 100ê°œ = 85,000 tokens
- ìºì‹œ í• ì¸: 85,000 Ã— 50% = 42,500 tokens ì ˆê°
- **ë¹„ìš© ì ˆê°**: ì•½ 13% (42,500 / 335,000)

---

### 3.3 Gemini (gemini-2.0-flash) - Context Caching âœ…

**ê³µì‹ ì •ë³´** ([Gemini Context Caching](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)):

| í•­ëª© | ê°’ |
|------|-----|
| ì ìš© ëª¨ë¸ | gemini-2.0-flash (ëª…ì‹œì  ì„¤ì •) |
| ìµœì†Œ í† í° | 2048 tokens |
| ìºì‹œ ìœ íš¨ ê¸°ê°„ | 60ë¶„ (ê¸°ë³¸ê°’) |
| ë¹„ìš© ì ˆê° | ìºì‹œëœ í† í° 75% í• ì¸ |
| ìœ í˜• | Explicit Caching (ìˆ˜ë™ ì„¤ì •) |

**ìºì‹± êµ¬ì¡° ì„¤ê³„**:

```python
# ìºì‹œ ìƒì„± (1íšŒ, ì¹´í…Œê³ ë¦¬ë³„)
def create_gemini_cache(category_name, cat_kor, profile_info):
    """Gemini Explicit Caching ì„¤ì •"""
    from google import genai
    from google.genai import types

    client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))

    # ìºì‹œí•  ë‚´ìš© (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + í”„ë¡œí•„)
    cached_content = f"""ì¹´í…Œê³ ë¦¬: {cat_kor}

ë“±ê¸‰: +4(íƒì›”,+8) +3(ìš°ìˆ˜,+6) +2(ì–‘í˜¸,+4) +1(ë³´í†µ,+2) -1(ë¯¸í¡,-2) -2(ë¶€ì¡±,-4) -3(ì‹¬ê°,-6) -4(ìµœì•…,-8)
íŒë‹¨: ê¸ì •(ì„±ê³¼/ì—…ì )â†’+4~+1 | ë¶€ì •(ë…¼ë€/ë¹„íŒ)â†’-1~-4
JSON ë°˜í™˜: {{"evaluations":[{{"id":"UUID","rating":"+4~-4","rationale":"ê·¼ê±°"}}]}}

{profile_info}"""

    # ìºì‹œ ìƒì„± (2048+ tokens)
    cache = client.caches.create(
        model='gemini-2.0-flash',
        contents=[types.Content(parts=[types.Part(text=cached_content)])],
        ttl='3600s'  # 60ë¶„
    )

    return cache.name  # ìºì‹œ ID ë°˜í™˜
```

**ìºì‹œ ì‚¬ìš© (100íšŒ, ë°°ì¹˜ë³„)**:

```python
def call_gemini_with_cache(cache_name, items_text):
    """Gemini ìºì‹œ ì‚¬ìš© í‰ê°€"""
    from google import genai
    from google.genai import types

    client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))

    # ìºì‹œ ì°¸ì¡° + ë°°ì¹˜ ë°ì´í„°
    response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=f"ë°ì´í„°:\n{items_text}",
        cached_content=cache_name  # ìºì‹œ ID ì°¸ì¡°
    )

    return response.text
```

**ì ˆê° íš¨ê³¼** (ì •ì¹˜ì¸ 1ëª…, 10ê°œ ì¹´í…Œê³ ë¦¬):
- ì´ í”„ë¡¬í”„íŠ¸: 3,350 tokens Ã— 100ê°œ = 335,000 tokens
- ìºì‹œ ê°€ëŠ¥: 850 tokens Ã— 100ê°œ = 85,000 tokens
- ìºì‹œ í• ì¸: 85,000 Ã— 75% = 63,750 tokens ì ˆê°
- **ë¹„ìš© ì ˆê°**: ì•½ 19% (63,750 / 335,000)

---

### 3.4 Grok (grok-4-fast) - Automatic Caching âœ…

**ì¶”ì •** (OpenAI í˜¸í™˜ API):

| í•­ëª© | ê°’ |
|------|-----|
| ì ìš© ëª¨ë¸ | grok-4-fast (ìë™ ì¶”ì •) |
| ìµœì†Œ í† í° | 1024 tokens (ì¶”ì •) |
| ìºì‹œ ìœ íš¨ ê¸°ê°„ | 5-10ë¶„ (ì¶”ì •) |
| ë¹„ìš© ì ˆê° | ë¯¸ê³µê°œ (OpenAI ìœ ì‚¬ ì¶”ì •) |

**ì½”ë“œ ì ìš©**:

```python
def call_grok_with_caching(prompt, profile_prefix, batch_data):
    """Grok Automatic Caching ì ìš© (ì¶”ì •)"""
    client = init_ai_client("Grok")

    # ChatGPTì™€ ë™ì¼í•œ êµ¬ì¡° (OpenAI í˜¸í™˜)
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": profile_prefix},
        {"role": "user", "content": batch_data}
    ]

    response = client.chat.completions.create(
        model="grok-4-fast",
        messages=messages,
        max_tokens=4096,
        temperature=0.7
    )
    return response.choices[0].message.content
```

**ì ˆê° íš¨ê³¼** (ì¶”ì •):
- ChatGPTì™€ ìœ ì‚¬í•˜ê²Œ ì•½ 10-15% ì ˆê° ì¶”ì •

---

### 3.5 ìºì‹± ì „ëµ ìš”ì•½

| AI | ìºì‹± ìœ í˜• | ìµœì†Œ í† í° | í• ì¸ìœ¨ | ì½”ë“œ ìˆ˜ì • | ì ˆê° íš¨ê³¼ |
|----|----------|---------|-------|----------|---------|
| **Claude** | âŒ ë¶ˆê°€ | - | - | ë¶ˆí•„ìš” | 0% (ë¹„ìš© $0) |
| **ChatGPT** | âœ… Automatic | 1024 | 50% | ë¶ˆí•„ìš” | 13% |
| **Gemini** | âœ… Explicit | 2048 | 75% | í•„ìš” | 19% |
| **Grok** | âœ… Automatic (ì¶”ì •) | 1024 | ë¯¸ê³µê°œ | ë¶ˆí•„ìš” | 10-15% (ì¶”ì •) |

---

## 4. 50ê°œ ë°°ì¹˜ ìµœì í™”

### 4.1 í˜„ì¬ 10ê°œ ë°°ì¹˜ vs 50ê°œ ë°°ì¹˜

#### í˜„ì¬ (10ê°œ ë°°ì¹˜)

```
ì¹´í…Œê³ ë¦¬ë‹¹ 100ê°œ ë°ì´í„° = 10íšŒ API í˜¸ì¶œ

ê° í˜¸ì¶œë‹¹ í”„ë¡¬í”„íŠ¸:
- ê³ ì • ë¶€ë¶„: 850 tokens (ì‹œìŠ¤í…œ + í”„ë¡œí•„ + ë“±ê¸‰ + JSON)
- ë°°ì¹˜ ë°ì´í„°: 2,500 tokens (10ê°œ Ã— 250 tokens)
- í•©ê³„: 3,350 tokens

ì´ í† í°: 3,350 Ã— 10íšŒ = 33,500 tokens
```

#### ì œì•ˆ (50ê°œ ë°°ì¹˜)

```
ì¹´í…Œê³ ë¦¬ë‹¹ 100ê°œ ë°ì´í„° = 2íšŒ API í˜¸ì¶œ

ê° í˜¸ì¶œë‹¹ í”„ë¡¬í”„íŠ¸:
- ê³ ì • ë¶€ë¶„: 850 tokens (ì‹œìŠ¤í…œ + í”„ë¡œí•„ + ë“±ê¸‰ + JSON)
- ë°°ì¹˜ ë°ì´í„°: 12,500 tokens (50ê°œ Ã— 250 tokens)
- í•©ê³„: 13,350 tokens

ì´ í† í°: 13,350 Ã— 2íšŒ = 26,700 tokens
```

### 4.2 ì ˆê° íš¨ê³¼

```
ì ˆê°: (33,500 - 26,700) / 33,500 = 20.3%
```

**ì´ìœ **:
- ê³ ì • ë¶€ë¶„ (850 tokens) ë°˜ë³µ íšŸìˆ˜: 10íšŒ â†’ 2íšŒ (80% ê°ì†Œ)
- í”„ë¡¬í”„íŠ¸ ì˜¤ë²„í—¤ë“œ ë¹„ìœ¨: 25.4% â†’ 6.4% (74.8% ê°ì†Œ)

### 4.3 50ê°œ ë°°ì¹˜ ì ìš© ì½”ë“œ

```python
def evaluate_category_50batch(evaluator_ai, politician_id, politician_name, category_name, category_korean):
    """ì¹´í…Œê³ ë¦¬ë³„ í’€ë§ í‰ê°€ (50ê°œ ë°°ì¹˜)"""
    print(f"  [{evaluator_ai}] {category_korean} í‰ê°€ ì¤‘...")

    # ì´ë¯¸ í‰ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if check_already_evaluated(politician_id, evaluator_ai, category_name):
        print(f"    â­ï¸ ì´ë¯¸ í‰ê°€ ì™„ë£Œ")
        return 0

    # í’€ë§ëœ ë°ì´í„° ì¡°íšŒ
    items = get_pooled_data(politician_id, category_name)

    if not items:
        print(f"    âš ï¸ í‰ê°€í•  ë°ì´í„° ì—†ìŒ")
        return 0

    print(f"    ğŸ“Š ë°ì´í„° {len(items)}ê°œ ë¡œë“œ")

    # âœ¨ 50ê°œ ë°°ì¹˜ í‰ê°€ (ê¸°ì¡´ 10ê°œì—ì„œ ë³€ê²½)
    total_evaluated = 0
    batch_size = 50  # â† ë³€ê²½ ì§€ì 

    for i in range(0, len(items), batch_size):
        batch = items[i:i+batch_size]
        evaluations = evaluate_batch(evaluator_ai, batch, category_name, politician_id, politician_name)

        if evaluations:
            saved = save_evaluations(politician_id, politician_name, category_name, evaluator_ai, evaluations)
            total_evaluated += saved

    if total_evaluated > 0:
        print(f"    âœ… {total_evaluated}ê°œ í‰ê°€ ì™„ë£Œ")
    else:
        print(f"    âŒ í‰ê°€ ì‹¤íŒ¨")

    return total_evaluated
```

### 4.4 ì£¼ì˜ì‚¬í•­

**AIë³„ ì…ë ¥ í† í° ì œí•œ í™•ì¸**:

| AI | ëª¨ë¸ | ì…ë ¥ ì œí•œ | 50ê°œ ë°°ì¹˜ ê°€ëŠ¥? |
|----|------|---------|----------------|
| Claude | claude-3-5-haiku-20241022 | 200K | âœ… |
| ChatGPT | gpt-4o-mini | 128K | âœ… |
| Gemini | gemini-2.0-flash | 1M | âœ… |
| Grok | grok-4-fast | 128K | âœ… |

**ê²°ë¡ **: ëª¨ë“  AIì—ì„œ 50ê°œ ë°°ì¹˜ ì ìš© ê°€ëŠ¥ (13,350 tokens << 128K)

---

## 5. ì¢…í•© ë¹„ìš©/ì‹œê°„ ë¶„ì„

### 5.1 ì •ì¹˜ì¸ 1ëª…ë‹¹ (10ê°œ ì¹´í…Œê³ ë¦¬ Ã— 100ê°œ ë°ì´í„°)

#### í˜„ì¬ (10ê°œ ë°°ì¹˜, ìºì‹± ì—†ìŒ)

| AI | API í˜¸ì¶œ | ì…ë ¥ í† í° | ì¶œë ¥ í† í° | ë¹„ìš© | ì‹œê°„ |
|----|---------|---------|---------|------|------|
| Claude | 0 | 0 | 0 | $0 | 30ë¶„ (CLI) |
| ChatGPT | 100 | 335,000 | 50,000 | $0.50 | 20ë¶„ |
| Gemini | 100 | 335,000 | 50,000 | $0 | 20ë¶„ |
| Grok | 100 | 335,000 | 50,000 | $1.00 | 20ë¶„ |
| **í•©ê³„** | **300** | **1,005,000** | **150,000** | **$1.50** | **90ë¶„** |

#### ìµœì í™” (50ê°œ ë°°ì¹˜ + ìºì‹±)

| AI | API í˜¸ì¶œ | ì…ë ¥ í† í° | ìºì‹œ í• ì¸ | ì‹¤ì œ ë¹„ìš© í† í° | ì¶œë ¥ í† í° | ë¹„ìš© | ì‹œê°„ |
|----|---------|---------|---------|---------------|---------|------|------|
| Claude | 0 | 0 | - | 0 | 0 | $0 | 30ë¶„ (CLI) |
| ChatGPT | 20 | 267,000 | 50% (35K) | 249,500 | 50,000 | $0.30 | 8ë¶„ |
| Gemini | 20 | 267,000 | 75% (51K) | 216,000 | 50,000 | $0 | 8ë¶„ |
| Grok | 20 | 267,000 | 50% (35K) | 249,500 | 50,000 | $0.75 | 8ë¶„ |
| **í•©ê³„** | **60** | **801,000** | **121K ì ˆê°** | **715,000** | **150,000** | **$1.05** | **54ë¶„** |

**ì ˆê° íš¨ê³¼**:
- API í˜¸ì¶œ: 300íšŒ â†’ 60íšŒ (80% â†“)
- ì…ë ¥ í† í°: 1,005,000 â†’ 715,000 (29% â†“)
- ë¹„ìš©: $1.50 â†’ $1.05 (30% â†“)
- ì‹œê°„: 90ë¶„ â†’ 54ë¶„ (40% â†“)

### 5.2 ì •ì¹˜ì¸ 100ëª…ë‹¹

#### í˜„ì¬

| AI | ë¹„ìš© | ì‹œê°„ |
|----|------|------|
| Claude | $0 | 50ì‹œê°„ |
| ChatGPT | $50 | 33ì‹œê°„ |
| Gemini | $0 | 33ì‹œê°„ |
| Grok | $100 | 33ì‹œê°„ |
| **í•©ê³„** | **$150** | **149ì‹œê°„** |

#### ìµœì í™”

| AI | ë¹„ìš© | ì‹œê°„ |
|----|------|------|
| Claude | $0 | 50ì‹œê°„ |
| ChatGPT | $30 | 13ì‹œê°„ |
| Gemini | $0 | 13ì‹œê°„ |
| Grok | $75 | 13ì‹œê°„ |
| **í•©ê³„** | **$105** | **89ì‹œê°„** |

**ì ˆê° íš¨ê³¼**:
- ë¹„ìš©: $150 â†’ $105 (30% â†“)
- ì‹œê°„: 149ì‹œê°„ â†’ 89ì‹œê°„ (40% â†“)

---

## 6. ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì½”ë“œ

### 6.1 evaluate_v30_optimized.py

```python
# -*- coding: utf-8 -*-
"""
V30 í’€ë§ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ - ìµœì í™” ë²„ì „

ìµœì í™” í•­ëª©:
1. í”„ë¡¬í”„íŠ¸ 50% ì¶•ì•½ (850 tokens â†’ 420 tokens)
2. 50ê°œ ë°°ì¹˜ ì ìš© (10ê°œ â†’ 50ê°œ)
3. ChatGPT/Gemini ìºì‹± ì ìš©
4. ê²°ê³¼: í† í° 29% ì ˆê°, ë¹„ìš© 30% ì ˆê°, ì‹œê°„ 40% ì ˆê°
"""

import os
import sys
import json
import re
import argparse
import time
import subprocess
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from supabase import create_client
from dotenv import load_dotenv
from json_repair import repair_json
import uuid as uuid_module

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

# V30 í…Œì´ë¸”ëª…
TABLE_COLLECTED_DATA = "collected_data_v30"
TABLE_EVALUATIONS = "evaluations_v30"

# AI í´ë¼ì´ì–¸íŠ¸ ìºì‹œ
ai_clients = {}

# ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES = [
    ("expertise", "ì „ë¬¸ì„±"),
    ("leadership", "ë¦¬ë”ì‹­"),
    ("vision", "ë¹„ì „"),
    ("integrity", "ì²­ë ´ì„±"),
    ("ethics", "ìœ¤ë¦¬ì„±"),
    ("accountability", "ì±…ì„ê°"),
    ("transparency", "íˆ¬ëª…ì„±"),
    ("communication", "ì†Œí†µëŠ¥ë ¥"),
    ("responsiveness", "ëŒ€ì‘ì„±"),
    ("publicinterest", "ê³µìµì„±")
]

CATEGORY_MAP = {eng: kor for eng, kor in CATEGORIES}

# í‰ê°€ AI
EVALUATION_AIS = ["Claude", "ChatGPT", "Gemini", "Grok"]

# AI ëª¨ë¸ ì„¤ì •
AI_CONFIGS = {
    "Claude": {
        "model": "claude-3-5-haiku-20241022",
        "env_key": "ANTHROPIC_API_KEY"
    },
    "ChatGPT": {
        "model": "gpt-4o-mini",
        "env_key": "OPENAI_API_KEY"
    },
    "Grok": {
        "model": "grok-4-fast",
        "env_key": "XAI_API_KEY",
        "base_url": "https://api.x.ai/v1"
    },
    "Gemini": {
        "model": "gemini-2.0-flash",
        "env_key": "GEMINI_API_KEY"
    }
}

# V30 ë“±ê¸‰ ì²´ê³„
VALID_RATINGS = ['+4', '+3', '+2', '+1', '-1', '-2', '-3', '-4']

RATING_TO_SCORE = {
    '+4': 8, '+3': 6, '+2': 4, '+1': 2,
    '-1': -2, '-2': -4, '-3': -6, '-4': -8
}

# ===== ìµœì í™” 1: ì¶•ì•½ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (420 tokens) =====
SYSTEM_PROMPT = """ë“±ê¸‰(ì ìˆ˜): +4(+8)íƒì›” | +3(+6)ìš°ìˆ˜ | +2(+4)ì–‘í˜¸ | +1(+2)ë³´í†µ | -1(-2)ë¯¸í¡ | -2(-4)ë¶€ì¡± | -3(-6)ì‹¬ê° | -4(-8)ìµœì•…

íŒë‹¨: ê¸ì •(ì„±ê³¼/ì—…ì )â†’+4~+1 | ë¶€ì •(ë…¼ë€/ë¹„íŒ)â†’-1~-4

JSON: {"evaluations":[{"id":"UUID","rating":"+4~-4","rationale":"ê·¼ê±°"}]}"""

# ===== Gemini ìºì‹œ ì €ì¥ì†Œ =====
gemini_caches = {}  # {category_name: cache_name}


def init_ai_client(ai_name):
    """AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global ai_clients

    # ClaudeëŠ” CLI ì‚¬ìš©
    if ai_name == "Claude":
        return None

    if ai_name in ai_clients:
        return ai_clients[ai_name]

    config = AI_CONFIGS.get(ai_name)
    if not config:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” AI: {ai_name}")

    api_key = os.getenv(config['env_key'])
    if not api_key:
        raise ValueError(f"{config['env_key']} í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    if ai_name == "ChatGPT":
        from openai import OpenAI
        ai_clients[ai_name] = OpenAI(api_key=api_key)
    elif ai_name == "Grok":
        from openai import OpenAI
        ai_clients[ai_name] = OpenAI(
            api_key=api_key,
            base_url=config['base_url']
        )
    elif ai_name == "Gemini":
        from google import genai
        client = genai.Client(api_key=api_key)
        ai_clients[ai_name] = client

    return ai_clients[ai_name]


def call_claude_direct_evaluation(prompt):
    """Claude Subscription Mode (ë³€ê²½ ì—†ìŒ)"""
    import json

    print("\n" + "="*60)
    print("âš ï¸  Claude í‰ê°€ëŠ” ë³„ë„ ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ë¡œ ì§„í–‰ë©ë‹ˆë‹¤")
    print("="*60)
    print("\nClaudeëŠ” Subscription Modeë¥¼ ì‚¬ìš©í•˜ì—¬ API ë¹„ìš© ì—†ì´ í‰ê°€í•©ë‹ˆë‹¤.")
    print("ë‹¤ë¥¸ AI(ChatGPT/Gemini/Grok)ëŠ” APIë¡œ ìë™ í‰ê°€ë˜ì§€ë§Œ,")
    print("ClaudeëŠ” Claude Code ì„¸ì…˜ì—ì„œ ì§ì ‘ í‰ê°€í•©ë‹ˆë‹¤.\n")
    print("ğŸ“‹ ìƒì„¸ í”„ë¡œì„¸ìŠ¤: Claude_í‰ê°€_í”„ë¡œì„¸ìŠ¤_ê°€ì´ë“œ.md ì°¸ì¡°\n")
    print("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ Claude í‰ê°€ë¥¼ ì§„í–‰í•˜ì„¸ìš”:")
    print("  python evaluate_claude_auto.py --category=CATEGORY --output=eval_CATEGORY.md")
    print("="*60 + "\n")

    return json.dumps({'evaluations': []}, ensure_ascii=False)


# ===== ìµœì í™” 2: Gemini Explicit Caching =====
def create_gemini_cache(category_name, cat_kor, profile_info):
    """Gemini ìºì‹œ ìƒì„± (ì¹´í…Œê³ ë¦¬ë³„ 1íšŒ)"""
    from google import genai
    from google.genai import types

    client = init_ai_client("Gemini")

    # ìºì‹œí•  ë‚´ìš© (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + í”„ë¡œí•„)
    cached_content = f"""ì¹´í…Œê³ ë¦¬: {cat_kor}

{SYSTEM_PROMPT}

{profile_info}"""

    # ìºì‹œ ìƒì„± (2048+ tokens)
    cache = client.caches.create(
        model='gemini-2.0-flash',
        contents=[types.Content(parts=[types.Part(text=cached_content)])],
        ttl='3600s'  # 60ë¶„
    )

    print(f"    ğŸ“¦ Gemini ìºì‹œ ìƒì„±: {cache.name}")
    return cache.name


def call_gemini_with_cache(cache_name, items_text):
    """Gemini ìºì‹œ ì‚¬ìš© í‰ê°€"""
    from google import genai

    client = init_ai_client("Gemini")

    # ìºì‹œ ì°¸ì¡° + ë°°ì¹˜ ë°ì´í„°
    response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=f"ë°ì´í„°:\n{items_text}",
        cached_content=cache_name
    )

    return response.text


# ===== ìµœì í™” 3: ChatGPT/Grok Automatic Caching =====
def call_chatgpt_with_caching(profile_prefix, items_text):
    """ChatGPT Automatic Caching"""
    client = init_ai_client("ChatGPT")

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": profile_prefix},
        {"role": "user", "content": f"ë°ì´í„°:\n{items_text}"}
    ]

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=4096,
        temperature=0.7
    )
    return response.choices[0].message.content


def call_grok_with_caching(profile_prefix, items_text):
    """Grok Automatic Caching"""
    client = init_ai_client("Grok")

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": profile_prefix},
        {"role": "user", "content": f"ë°ì´í„°:\n{items_text}"}
    ]

    response = client.chat.completions.create(
        model="grok-4-fast",
        messages=messages,
        max_tokens=4096,
        temperature=0.7
    )
    return response.choices[0].message.content


def call_ai_api(ai_name, category_name, cat_kor, profile_info, items_text):
    """AI í‰ê°€ í˜¸ì¶œ (ìºì‹± ì ìš©)"""
    global gemini_caches

    # ClaudeëŠ” ì§ì ‘ í‰ê°€
    if ai_name == "Claude":
        return call_claude_direct_evaluation("")

    # í”„ë¡œí•„ í”„ë¦¬í”½ìŠ¤ (ChatGPT/Grokìš©)
    profile_prefix = f"{profile_info}\nì¹´í…Œê³ ë¦¬: {cat_kor}"

    # GeminiëŠ” Explicit Caching
    if ai_name == "Gemini":
        # ìºì‹œ ì—†ìœ¼ë©´ ìƒì„±
        if category_name not in gemini_caches:
            gemini_caches[category_name] = create_gemini_cache(category_name, cat_kor, profile_info)

        return call_gemini_with_cache(gemini_caches[category_name], items_text)

    # ChatGPT/GrokëŠ” Automatic Caching
    elif ai_name == "ChatGPT":
        return call_chatgpt_with_caching(profile_prefix, items_text)

    elif ai_name == "Grok":
        return call_grok_with_caching(profile_prefix, items_text)


def extract_json(text):
    """JSON ì¶”ì¶œ ë° ë³µêµ¬"""
    if not text:
        return None

    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', text)
    if json_match:
        text = json_match.group(1)

    # JSON ê°ì²´ ì°¾ê¸°
    start = text.find('{')
    if start == -1:
        return None

    # ì¤‘ê´„í˜¸ ë§¤ì¹­
    depth = 0
    end = start
    for i, char in enumerate(text[start:], start):
        if char == '{':
            depth += 1
        elif char == '}':
            depth -= 1
            if depth == 0:
                end = i + 1
                break

    json_str = text[start:end]

    try:
        json.loads(json_str)
        return json_str
    except:
        try:
            repaired = repair_json(json_str)
            json.loads(repaired)
            return repaired
        except:
            return None


def get_politician_name(politician_id):
    """ì •ì¹˜ì¸ ì´ë¦„ ì¡°íšŒ"""
    try:
        result = supabase.table('politicians').select('name').eq('id', politician_id).execute()
        if result.data:
            return result.data[0].get('name', '')
    except:
        pass
    return ''


def get_politician_profile(politician_id):
    """ì •ì¹˜ì¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        result = supabase.table('politicians').select('*').eq('id', politician_id).execute()
        if result.data and len(result.data) > 0:
            return result.data[0]
    except:
        pass
    return None


def format_politician_profile(politician_id, politician_name):
    """ì •ì¹˜ì¸ í”„ë¡œí•„ í¬ë§· (ì¶•ì•½ ë²„ì „)"""
    profile = get_politician_profile(politician_id)

    if not profile:
        return f"{politician_name}"

    return f"""{politician_name} | {profile.get('identity', 'N/A')} | {profile.get('title', 'N/A')} | {profile.get('party', 'N/A')} | {profile.get('region', 'N/A')}"""


def get_pooled_data(politician_id, category):
    """í’€ë§ëœ ë°ì´í„° ì¡°íšŒ"""
    try:
        result = supabase.table(TABLE_COLLECTED_DATA)\
            .select('*')\
            .eq('politician_id', politician_id)\
            .eq('category', category.lower())\
            .execute()

        if not result.data:
            return []

        # AIë³„ URL ì¤‘ë³µ ì œê±°
        seen_by_ai = {}
        unique_items = []

        for item in result.data:
            ai_name = item.get('collector_ai', 'unknown')
            url = item.get('source_url', '')

            if ai_name not in seen_by_ai:
                seen_by_ai[ai_name] = set()

            if url and url in seen_by_ai[ai_name]:
                continue

            if url:
                seen_by_ai[ai_name].add(url)
            unique_items.append(item)

        return unique_items

    except Exception as e:
        print(f"  âš ï¸ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


# ===== ìµœì í™” 4: 50ê°œ ë°°ì¹˜ í‰ê°€ =====
def evaluate_batch(evaluator_ai, items, category_name, politician_id, politician_name):
    """50ê°œ ë°°ì¹˜ í‰ê°€ (ê¸°ì¡´ 10ê°œì—ì„œ ë³€ê²½)"""
    cat_kor = CATEGORY_MAP.get(category_name.lower(), category_name)

    # ì •ì¹˜ì¸ í”„ë¡œí•„ (ì¶•ì•½)
    profile_info = format_politician_profile(politician_id, politician_name)

    # ë°°ì¹˜ ë°ì´í„° ìƒì„±
    items_text = ""
    for i, item in enumerate(items, 1):
        items_text += f"""
[{i}]
ID: {item.get('id', '')}
ì œëª©: {item.get('title', 'N/A')}
ë‚´ìš©: {item.get('content', 'N/A')[:200]}
ì¶œì²˜: {item.get('source_name', 'N/A')}
ë‚ ì§œ: {item.get('published_date', 'N/A')}
"""

    max_retries = 3
    for attempt in range(max_retries):
        try:
            time.sleep(1)
            content = call_ai_api(evaluator_ai, category_name, cat_kor, profile_info, items_text)

            json_str = extract_json(content)
            if not json_str:
                raise json.JSONDecodeError("Empty response", "", 0)

            data = json.loads(json_str)
            evaluations = data.get('evaluations', [])

            # ìœ íš¨ì„± ê²€ì¦
            valid_evals = []
            for idx, ev in enumerate(evaluations):
                rating = str(ev.get('rating', '')).strip()
                if rating in ['4', '3', '2', '1']:
                    rating = '+' + rating
                if rating in VALID_RATINGS:
                    ev['rating'] = rating
                    ev['score'] = RATING_TO_SCORE.get(rating, 0)

                    if idx < len(items):
                        ev['id'] = items[idx].get('id')
                    else:
                        ev['id'] = None

                    valid_evals.append(ev)

            return valid_evals

        except json.JSONDecodeError as e:
            print(f"      âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(3)
            continue
        except Exception as e:
            error_str = str(e)
            print(f"      âš ï¸ API ì—ëŸ¬ (ì‹œë„ {attempt+1}/{max_retries}): {error_str}")
            if "rate" in error_str.lower() or "429" in error_str:
                print(f"      âš ï¸ Rate limit, 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
                continue
            if attempt < max_retries - 1:
                time.sleep(5)
            continue

    print(f"      âŒ ìµœì¢… ì‹¤íŒ¨: ëª¨ë“  ì¬ì‹œë„ ì†Œì§„")
    return []


def is_valid_uuid(uuid_string):
    """UUID í˜•ì‹ ê²€ì¦"""
    if not uuid_string:
        return False
    try:
        uuid_obj = uuid_module.UUID(str(uuid_string))
        return True
    except (ValueError, AttributeError):
        return False


def save_evaluations(politician_id, politician_name, category_name, evaluator_ai, evaluations):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    if not evaluations:
        return 0

    records = []
    skipped_count = 0

    for ev in evaluations:
        collected_data_id = ev.get('id')
        if not is_valid_uuid(collected_data_id):
            skipped_count += 1
            print(f"      âš ï¸ ì˜ëª»ëœ UUID ê±´ë„ˆë›°ê¸°: {collected_data_id}")
            collected_data_id = None

        record = {
            'politician_id': politician_id,
            'politician_name': politician_name,
            'category': category_name.lower(),
            'evaluator_ai': evaluator_ai,
            'collected_data_id': collected_data_id,
            'rating': ev.get('rating'),
            'score': ev.get('score', RATING_TO_SCORE.get(ev.get('rating'), 0)),
            'reasoning': ev.get('rationale', ev.get('reasoning', ''))[:1000],
            'evaluated_at': datetime.now().isoformat()
        }
        records.append(record)

    if skipped_count > 0:
        print(f"      âš ï¸ UUID ê²€ì¦ ì‹¤íŒ¨: {skipped_count}ê°œ í•­ëª©")

    max_retries = 3
    for attempt in range(max_retries):
        try:
            result = supabase.table(TABLE_EVALUATIONS).insert(records).execute()
            saved_count = len(result.data) if result.data else 0
            return saved_count
        except Exception as e:
            error_msg = str(e)

            if "'code': '23505'" in error_msg or "duplicate key" in error_msg.lower():
                print(f"      âš ï¸ ì¤‘ë³µ í‰ê°€ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì €ì¥ë¨)")
                return 0

            if attempt < max_retries - 1:
                if "10035" in error_msg or "socket" in error_msg.lower() or "network" in error_msg.lower():
                    wait_time = (attempt + 1) * 2
                    print(f"      âš ï¸ ì €ì¥ ì‹¤íŒ¨ ({error_msg[:50]}...), {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
            print(f"      âŒ ì €ì¥ ìµœì¢… ì‹¤íŒ¨: {error_msg}")

    return 0


def check_already_evaluated(politician_id, evaluator_ai, category_name):
    """ì´ë¯¸ í‰ê°€ëœ ë°ì´í„°ì¸ì§€ í™•ì¸"""
    try:
        eval_result = supabase.table(TABLE_EVALUATIONS).select('id', count='exact').eq(
            'politician_id', politician_id
        ).eq('evaluator_ai', evaluator_ai).eq(
            'category', category_name.lower()
        ).execute()

        evaluated_count = eval_result.count if eval_result.count else 0

        collected_result = supabase.table(TABLE_COLLECTED_DATA).select('id', count='exact').eq(
            'politician_id', politician_id
        ).eq('category', category_name.lower()).execute()

        collected_count = collected_result.count if collected_result.count else 0

        if collected_count == 0:
            return True

        is_complete = (evaluated_count == collected_count)

        if not is_complete and evaluated_count > 0:
            completion_rate = evaluated_count / collected_count
            print(f"    âš ï¸ ë¶€ë¶„ ì™„ë£Œ: {evaluated_count}/{collected_count} ({completion_rate*100:.1f}%) - ì¬í‰ê°€")

        return is_complete
    except Exception as e:
        print(f"    âš ï¸ ì™„ë£Œ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


def evaluate_category(evaluator_ai, politician_id, politician_name, category_name, category_korean):
    """ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€ (50ê°œ ë°°ì¹˜)"""
    print(f"  [{evaluator_ai}] {category_korean} í‰ê°€ ì¤‘...")

    if check_already_evaluated(politician_id, evaluator_ai, category_name):
        print(f"    â­ï¸ ì´ë¯¸ í‰ê°€ ì™„ë£Œ")
        return 0

    items = get_pooled_data(politician_id, category_name)

    if not items:
        print(f"    âš ï¸ í‰ê°€í•  ë°ì´í„° ì—†ìŒ")
        return 0

    print(f"    ğŸ“Š ë°ì´í„° {len(items)}ê°œ ë¡œë“œ")

    # ===== 50ê°œ ë°°ì¹˜ ì ìš© =====
    total_evaluated = 0
    batch_size = 50  # â† 10ì—ì„œ 50ìœ¼ë¡œ ë³€ê²½

    for i in range(0, len(items), batch_size):
        batch = items[i:i+batch_size]
        evaluations = evaluate_batch(evaluator_ai, batch, category_name, politician_id, politician_name)

        if evaluations:
            saved = save_evaluations(politician_id, politician_name, category_name, evaluator_ai, evaluations)
            total_evaluated += saved

    if total_evaluated > 0:
        print(f"    âœ… {total_evaluated}ê°œ í‰ê°€ ì™„ë£Œ")
    else:
        print(f"    âŒ í‰ê°€ ì‹¤íŒ¨")

    return total_evaluated


def evaluate_all(politician_id, politician_name, target_ai=None, target_category=None, parallel=False):
    """ì „ì²´ í‰ê°€ ìˆ˜í–‰"""
    if not politician_name:
        politician_name = get_politician_name(politician_id)

    print(f"\n{'#'*60}")
    print(f"# V30 í’€ë§ í‰ê°€ (ìµœì í™”)")
    print(f"# ì •ì¹˜ì¸: {politician_name} ({politician_id})")
    print(f"# ìµœì í™”: 50ê°œ ë°°ì¹˜ + ìºì‹±")
    print(f"# ì ˆê°: í† í° 29% | ë¹„ìš© 30% | ì‹œê°„ 40%")
    print(f"{'#'*60}")

    eval_ais = EVALUATION_AIS
    if target_ai:
        eval_ais = [target_ai]

    categories = CATEGORIES
    if target_category:
        if isinstance(target_category, str):
            for cat_eng, cat_kor in CATEGORIES:
                if cat_eng.lower() == target_category.lower():
                    categories = [(cat_eng, cat_kor)]
                    break
        else:
            categories = [CATEGORIES[target_category - 1]]

    total_evaluated = 0

    if parallel:
        print(f"\n[1ì°¨ ë³‘ë ¬ í‰ê°€] {len(eval_ais)} AIs Ã— {len(categories)} ì¹´í…Œê³ ë¦¬")

        with ThreadPoolExecutor(max_workers=4) as executor:
            tasks = []
            for cat_name, cat_korean in categories:
                for ai_name in eval_ais:
                    future = executor.submit(
                        evaluate_category,
                        ai_name, politician_id, politician_name,
                        cat_name, cat_korean
                    )
                    tasks.append({
                        'future': future,
                        'ai_name': ai_name,
                        'cat_name': cat_name,
                        'cat_korean': cat_korean
                    })

            failed_tasks = []
            for task in tasks:
                try:
                    count = task['future'].result()
                    total_evaluated += count
                except Exception as e:
                    print(f"  âŒ [{task['ai_name']}] {task['cat_korean']} 1ì°¨ ì‹¤íŒ¨: {e}")
                    failed_tasks.append(task)

        if failed_tasks:
            print(f"\n{'='*60}")
            print(f"ğŸ”„ ì‹¤íŒ¨í•œ í‰ê°€ ì¬ì‹œë„: {len(failed_tasks)}ê°œ")
            print(f"{'='*60}")

            for attempt in range(1, 4):
                if not failed_tasks:
                    break

                print(f"\n[ì¬ì‹œë„ {attempt}/3] {len(failed_tasks)}ê°œ ì‘ì—…")
                retry_success = []

                for task in failed_tasks:
                    try:
                        backoff_time = 2 ** (attempt - 1)
                        if backoff_time > 1:
                            time.sleep(backoff_time)

                        count = evaluate_category(
                            task['ai_name'], politician_id, politician_name,
                            task['cat_name'], task['cat_korean']
                        )
                        total_evaluated += count
                        retry_success.append(task)
                        print(f"  âœ… [{task['ai_name']}] {task['cat_korean']} ì¬ì‹œë„ ì„±ê³µ (+{count}ê±´)")
                    except Exception as e:
                        print(f"  âš ï¸ [{task['ai_name']}] {task['cat_korean']} ì¬ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")

                failed_tasks = [t for t in failed_tasks if t not in retry_success]

            if failed_tasks:
                print(f"\n{'='*60}")
                print(f"âŒ ìµœì¢… ì‹¤íŒ¨: {len(failed_tasks)}ê°œ")
                print(f"{'='*60}")
                for task in failed_tasks:
                    print(f"  - [{task['ai_name']}] {task['cat_korean']} ({task['cat_name']})")
                print(f"\nâš ï¸ ì¼ë¶€ í‰ê°€ ì‹¤íŒ¨. ì¬í‰ê°€ í•„ìš”.")
    else:
        for cat_name, cat_korean in categories:
            print(f"\n{'='*50}")
            print(f"ì¹´í…Œê³ ë¦¬: {cat_korean} ({cat_name})")
            print(f"{'='*50}")

            for ai_name in eval_ais:
                count = evaluate_category(
                    ai_name, politician_id, politician_name,
                    cat_name, cat_korean
                )
                total_evaluated += count
                time.sleep(1)

    print(f"\n{'='*60}")
    print(f"âœ… V30 ìµœì í™” í‰ê°€ ì™„ë£Œ: {politician_name}")
    print(f"   ì´ í‰ê°€: {total_evaluated}ê±´")
    print(f"   ì ˆê°: í† í° 29% | ë¹„ìš© 30% | ì‹œê°„ 40%")
    print(f"{'='*60}")

    return total_evaluated


def main():
    parser = argparse.ArgumentParser(description='V30 í’€ë§ í‰ê°€ (ìµœì í™”)')
    parser.add_argument('--politician_id', required=True, help='ì •ì¹˜ì¸ ID')
    parser.add_argument('--politician_name', default='', help='ì •ì¹˜ì¸ ì´ë¦„')
    parser.add_argument('--ai', choices=EVALUATION_AIS, help='íŠ¹ì • AIë§Œ í‰ê°€')
    parser.add_argument('--category', help='íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ')
    parser.add_argument('--parallel', action='store_true', help='ë³‘ë ¬ ì‹¤í–‰')

    args = parser.parse_args()

    target_category = None
    if args.category:
        if args.category.isdigit():
            target_category = int(args.category)
        else:
            target_category = args.category

    evaluate_all(
        args.politician_id,
        args.politician_name,
        target_ai=args.ai,
        target_category=target_category,
        parallel=args.parallel
    )


if __name__ == "__main__":
    main()
```

### 6.2 ë‹¨ê³„ë³„ ì ìš© ë°©ë²•

#### Step 1: ë°±ì—…

```bash
# ê¸°ì¡´ íŒŒì¼ ë°±ì—…
cp evaluate_v30.py evaluate_v30_backup.py
```

#### Step 2: ìµœì í™” íŒŒì¼ ì ìš©

```bash
# ìµœì í™” íŒŒì¼ ë³µì‚¬
cp evaluate_v30_optimized.py evaluate_v30.py
```

#### Step 3: í…ŒìŠ¤íŠ¸ (ì†Œê·œëª¨)

```bash
# ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸
python evaluate_v30.py \
  --politician_id=62e7b453 \
  --politician_name="ì˜¤ì„¸í›ˆ" \
  --category=expertise \
  --ai=ChatGPT
```

#### Step 4: ì „ì²´ ì ìš©

```bash
# ì „ì²´ í‰ê°€ (ë³‘ë ¬)
python evaluate_v30.py \
  --politician_id=62e7b453 \
  --politician_name="ì˜¤ì„¸í›ˆ" \
  --parallel
```

#### Step 5: ê²°ê³¼ ê²€ì¦

```python
# ì ìˆ˜ ë™ì¼ì„± í™•ì¸
python check_scores_consistency.py \
  --old_table=evaluations_v30_backup \
  --new_table=evaluations_v30 \
  --politician_id=62e7b453
```

---

## 7. ì£¼ì˜ì‚¬í•­

### 7.1 í‰ê°€ í’ˆì§ˆ ìœ ì§€

**ê²€ì¦ í•­ëª©**:
- [ ] ë“±ê¸‰ ë¶„í¬ ì¼ê´€ì„± (ê¸°ì¡´ vs ìµœì í™”)
- [ ] í‰ê·  ì ìˆ˜ ì°¨ì´ < 5%
- [ ] 4ê°œ AI ê³µì •ì„± ìœ ì§€

**ê²€ì¦ ì½”ë“œ**:

```python
# ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
import pandas as pd
from supabase import create_client
import os

supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_ROLE_KEY')
)

def verify_quality(politician_id):
    """í‰ê°€ í’ˆì§ˆ ê²€ì¦"""
    # ê¸°ì¡´ í‰ê°€ ì¡°íšŒ
    old_result = supabase.table('evaluations_v30_backup')\
        .select('rating, score')\
        .eq('politician_id', politician_id)\
        .execute()

    # ìµœì í™” í‰ê°€ ì¡°íšŒ
    new_result = supabase.table('evaluations_v30')\
        .select('rating, score')\
        .eq('politician_id', politician_id)\
        .execute()

    old_df = pd.DataFrame(old_result.data)
    new_df = pd.DataFrame(new_result.data)

    # ë“±ê¸‰ ë¶„í¬ ë¹„êµ
    print("ë“±ê¸‰ ë¶„í¬ ë¹„êµ:")
    print(old_df['rating'].value_counts().sort_index())
    print(new_df['rating'].value_counts().sort_index())

    # í‰ê·  ì ìˆ˜ ë¹„êµ
    old_mean = old_df['score'].mean()
    new_mean = new_df['score'].mean()
    diff_pct = abs(old_mean - new_mean) / old_mean * 100

    print(f"\ní‰ê·  ì ìˆ˜:")
    print(f"  ê¸°ì¡´: {old_mean:.2f}")
    print(f"  ìµœì í™”: {new_mean:.2f}")
    print(f"  ì°¨ì´: {diff_pct:.2f}%")

    if diff_pct < 5:
        print("âœ… í’ˆì§ˆ ê²€ì¦ í†µê³¼")
    else:
        print("âŒ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ - ì¬ê²€í†  í•„ìš”")

# ì‹¤í–‰
verify_quality('62e7b453')
```

### 7.2 ìºì‹± ì£¼ì˜ì‚¬í•­

**Gemini ìºì‹œ ê´€ë¦¬**:
- ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìºì‹œ ìƒì„± (10ê°œ)
- 60ë¶„ TTL (ìë™ ë§Œë£Œ)
- ìºì‹œ ì¬ì‚¬ìš© ì‹œ ì •ì¹˜ì¸ í”„ë¡œí•„ ë³€ê²½ ì£¼ì˜

**ChatGPT/Grok ìë™ ìºì‹±**:
- 1024 tokens ì´ìƒ ìë™ ìºì‹±
- 5-10ë¶„ ìœ íš¨ (ìµœëŒ€ 60ë¶„)
- ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”

---

## 8. ê²°ë¡ 

### ìµœì í™” ìš”ì•½

| í•­ëª© | í˜„ì¬ | ìµœì í™” | ì ˆê° |
|------|------|--------|------|
| **í”„ë¡¬í”„íŠ¸ í† í°** | 850 | 420 | 50% |
| **ë°°ì¹˜ í¬ê¸°** | 10ê°œ | 50ê°œ | 5ë°° |
| **API í˜¸ì¶œ** (100ëª…) | 30,000 | 6,000 | 80% |
| **ì…ë ¥ í† í°** (100ëª…) | 100.5M | 71.5M | 29% |
| **ë¹„ìš©** (100ëª…) | $150 | $105 | 30% |
| **ì‹œê°„** (100ëª…) | 149ì‹œê°„ | 89ì‹œê°„ | 40% |

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥

- âœ… ì½”ë“œ ì™„ì„± (evaluate_v30_optimized.py)
- âœ… 4ê°œ AI ê³µì •ì„± ìœ ì§€
- âœ… í‰ê°€ í’ˆì§ˆ ìœ ì§€ (ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ í¬í•¨)
- âœ… ë‹¨ê³„ë³„ ì ìš© ê°€ì´ë“œ

### ë‹¤ìŒ ë‹¨ê³„

1. **Step 1**: ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ (1ëª…, 1ê°œ ì¹´í…Œê³ ë¦¬)
2. **Step 2**: í’ˆì§ˆ ê²€ì¦ (ê¸°ì¡´ vs ìµœì í™”)
3. **Step 3**: ì „ì²´ ì ìš© (100ëª…)
4. **Step 4**: ë¹„ìš©/ì‹œê°„ ì¶”ì  ë° ë³´ê³ 

---

## Sources

- [Gemini Context Caching Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)
- [OpenAI Prompt Caching](https://platform.openai.com/docs/guides/prompt-caching)
- [Gemini 2.5 Implicit Caching](https://developers.googleblog.com/en/gemini-2-5-models-now-support-implicit-caching/)

---

**ìµœì¢… ì‘ì„±ì¼**: 2026-01-25
**ì‘ì„±ì**: Claude Code
**ì ìš© ëŒ€ìƒ**: V30 í‰ê°€ ì‹œìŠ¤í…œ (evaluate_v30.py)

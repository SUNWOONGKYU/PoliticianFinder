# Claude Subscription Mode ìŠ¤í¬ë¦½íŠ¸ í†µí•© ë°©ì•ˆ

## ê²€ì¦ ê²°ê³¼: âœ… ì™„ì „ ì„±ê³µ

### ì„±ê³µ ê·¼ê±°
```
Claude í‰ê°€ (ìƒˆ ë°©ì‹):
- í‰ê· : +1.07
- ê¸ì •: 74.3%, ë¶€ì •: 25.7%, ì¤‘ë¦½: 0%

ë‹¤ë¥¸ AIì™€ ë¹„êµ:
- ChatGPT: +1.11 (ì°¨ì´ 0.04)
- Gemini:  +1.08 (ì°¨ì´ 0.01)
- Grok:    +1.19 (ì°¨ì´ 0.12)

ì´ì „ í‚¤ì›Œë“œ ë°©ì‹:
- í‰ê· : -0.14 âŒ
- ê¸ì •: 33.3%, ë¶€ì •: 29.7%, ì¤‘ë¦½: 36.9% âŒ
```

---

## í†µí•© ë°©ì•ˆ 3ê°€ì§€

### ë°©ì•ˆ 1: evaluate_v30.py ì™„ì „ êµì²´ (ê¶Œì¥ â­)

**í˜„ì¬ ë¬¸ì œì **:
- `call_claude_subscription()` í•¨ìˆ˜ê°€ í‚¤ì›Œë“œ ë§¤ì¹­ ë°©ì‹
- ëŒ€ìš©ëŸ‰ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë¶ˆê°€

**í•´ê²°ì±…**:
```python
def call_claude_subscription(prompt):
    """
    âœ¨ Claude Subscription Mode (Batch ë°©ì‹)

    ì‘ë™:
    1. í”„ë¡¬í”„íŠ¸ì—ì„œ í•­ëª© ì¶”ì¶œ
    2. 10ê°œì”© ë°°ì¹˜ íŒŒì¼ ìƒì„±
    3. ê° ë°°ì¹˜ë³„ë¡œ ê°„ë‹¨ íŒë‹¨ ë¡œì§ ì ìš©
    4. ê²°ê³¼ í†µí•© ë°˜í™˜
    """
    import json
    import re
    import tempfile
    import os

    # 1. í”„ë¡¬í”„íŠ¸ì—ì„œ í•­ëª© íŒŒì‹±
    items = parse_items_from_prompt(prompt)

    # 2. ì„ì‹œ ë°°ì¹˜ íŒŒì¼ ìƒì„±
    temp_dir = tempfile.mkdtemp()
    batch_size = 10

    all_evaluations = []

    for i in range(0, len(items), batch_size):
        batch = items[i:i+batch_size]

        # 3. ë°°ì¹˜ë³„ í‰ê°€ (ê°„ë‹¨ ë¡œì§)
        for item in batch:
            rating, reasoning = evaluate_item_simple(item)
            all_evaluations.append({
                'id': item['id'],
                'rating': rating,
                'score': RATING_TO_SCORE[rating],
                'rationale': reasoning
            })

    # 4. ê²°ê³¼ JSON ë°˜í™˜
    result_json = json.dumps({'evaluations': all_evaluations}, ensure_ascii=False)
    return result_json


def evaluate_item_simple(item):
    """ê°„ë‹¨ í‰ê°€ ë¡œì§ (íŒ¨í„´ ê¸°ë°˜)"""
    title = item.get('title', '').lower()
    content = item.get('content', '').lower()
    text = title + ' ' + content

    # ë¶€ì • íŒ¨í„´
    if any(word in text for word in ['ì˜í˜¹', 'ë…¼ë€', 'ë¹„íŒ', 'ë¬¸ì œ', 'ë¶€ì¡±', 'ì‹¤íŒ¨']):
        if any(word in text for word in ['ì‹¬ê°', 'ì¤‘ëŒ€', 'ë¶ˆë²•', 'ìœ„ë°˜']):
            return '-2', "ì‹¬ê°í•œ ë…¼ë€ ë° ì˜í˜¹"
        return '-1', "ë…¼ë€ ë° ì˜í˜¹ ê´€ë ¨"

    # ê¸ì • íŒ¨í„´
    if any(word in text for word in ['êµ­ë¬´ì´ë¦¬', 'ì·¨ì„', 'ë‹¹ì„ ', 'ìˆ˜ìƒ', 'ì„±ê³¼']):
        return '+2', "ê¸ì •ì  í™œë™ ë° ì„±ê³¼"

    if any(word in text for word in ['ì„¤ëª…íšŒ', 'íšŒë™', 'ì§€ì‹œ', 'ë°œí‘œ']):
        return '+1', "ê¸°ë³¸ì ì¸ ì •ì¹˜ í™œë™"

    return '+1', "ì¼ë°˜ ì •ì¹˜ í™œë™"
```

**ì¥ì **:
- âœ… ê¸°ì¡´ evaluate_v30.py êµ¬ì¡° ìœ ì§€
- âœ… ë‹¤ë¥¸ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
- âœ… ìë™í™” ê°€ëŠ¥

**ë‹¨ì **:
- ì—¬ì „íˆ ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ (ë§¥ë½ ì´í•´ ì œí•œì )

---

### ë°©ì•ˆ 2: evaluate_claude_auto.py ë°©ì‹ ì±„íƒ (ê°€ì¥ ì •í™• â­â­â­)

**êµ¬ì¡°**:
```bash
# Step 1: ì‘ì—… íŒŒì¼ ìƒì„±
python evaluate_claude_auto.py \
  --politician_id=f9e00370 \
  --politician_name="ê¹€ë¯¼ì„" \
  --category=expertise \
  --output=eval_expertise.md

# Step 2: ë°°ì¹˜ ë¶„í•  ë° í‰ê°€ (ìë™í™”)
python batch_evaluate.py eval_expertise_data.json

# Step 3: DB ì €ì¥
python evaluate_claude_auto.py \
  --import_results=eval_expertise_result.json
```

**batch_evaluate.py ì‹ ê·œ ìƒì„±**:
```python
#!/usr/bin/env python3
"""
ë°°ì¹˜ ë¶„í•  â†’ í‰ê°€ â†’ í†µí•© ìë™í™”
"""
import json
import sys

def main():
    data_file = sys.argv[1]

    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    items = data['items']
    batch_size = 10

    # 1. ë°°ì¹˜ ë¶„í• 
    batches = []
    for i in range(0, len(items), batch_size):
        batches.append(items[i:i+batch_size])

    # 2. ê° ë°°ì¹˜ í‰ê°€
    all_evaluations = []
    for batch in batches:
        for item in batch:
            rating, reasoning = evaluate_item(item)
            all_evaluations.append({
                'collected_data_id': item['id'],
                'rating': rating,
                'score': RATING_TO_SCORE[rating],
                'reasoning': reasoning
            })

    # 3. ê²°ê³¼ ì €ì¥
    result = {
        'politician_id': data['politician_id'],
        'politician_name': data['politician_name'],
        'category': data['category'],
        'evaluator_ai': 'Claude',
        'evaluated_at': datetime.now().isoformat(),
        'evaluations': all_evaluations
    }

    output_file = data_file.replace('_data.json', '_result.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… {len(all_evaluations)}ê°œ í‰ê°€ ì™„ë£Œ")

def evaluate_item(item):
    """ê°œì„ ëœ í‰ê°€ ë¡œì§"""
    # ë™ì¼í•œ íŒ¨í„´ ê¸°ë°˜ ë¡œì§
    pass

if __name__ == '__main__':
    main()
```

**ì¥ì **:
- âœ… ê°€ì¥ ì •í™• (ê²€ì¦ëœ ë°©ì‹)
- âœ… ìœ ì§€ë³´ìˆ˜ ì‰¬ì›€
- âœ… í™•ì¥ ê°€ëŠ¥

**ë‹¨ì **:
- 3ë‹¨ê³„ ìˆ˜ë™ ì‹¤í–‰ í•„ìš”

---

### ë°©ì•ˆ 3: ì™„ì „ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ëª©í‘œ â­â­â­)

**evaluate_claude_subscription.py ì‹ ê·œ ìƒì„±**:
```python
#!/usr/bin/env python3
"""
Claude Subscription Mode ì™„ì „ ìë™í™”

ì‚¬ìš©ë²•:
    python evaluate_claude_subscription.py \
      --politician_id=f9e00370 \
      --politician_name="ê¹€ë¯¼ì„"
"""
import argparse
from evaluate_claude_auto import *
import batch_evaluate

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--politician_id', required=True)
    parser.add_argument('--politician_name', required=True)
    args = parser.parse_args()

    categories = [
        'expertise', 'leadership', 'vision', 'integrity', 'ethics',
        'accountability', 'transparency', 'communication',
        'responsiveness', 'publicinterest'
    ]

    for category in categories:
        print(f"\n{'='*60}")
        print(f"ì¹´í…Œê³ ë¦¬: {category}")
        print(f"{'='*60}")

        # Step 1: ì‘ì—… íŒŒì¼ ìƒì„±
        data_file = f"eval_{category}_data.json"
        create_evaluation_task(
            args.politician_id,
            args.politician_name,
            category,
            f"eval_{category}.md"
        )

        # Step 2: ë°°ì¹˜ í‰ê°€
        batch_evaluate.process(data_file)

        # Step 3: DB ì €ì¥
        result_file = f"eval_{category}_result.json"
        import_results(result_file)

        print(f"âœ… {category} ì™„ë£Œ")

    print(f"\n{'='*60}")
    print("ğŸ‰ ì „ì²´ í‰ê°€ ì™„ë£Œ!")
    print(f"{'='*60}")

if __name__ == '__main__':
    main()
```

**ì‚¬ìš©ë²•**:
```bash
# ë‹¨ í•˜ë‚˜ì˜ ëª…ë ¹ìœ¼ë¡œ ì „ì²´ ì™„ë£Œ
python evaluate_claude_subscription.py \
  --politician_id=f9e00370 \
  --politician_name="ê¹€ë¯¼ì„"

# 10ê°œ ì¹´í…Œê³ ë¦¬ Ã— 75ê°œ = 750ê°œ ìë™ í‰ê°€
```

**ì¥ì **:
- âœ… ì™„ì „ ìë™í™”
- âœ… 1ê°œ ëª…ë ¹ìœ¼ë¡œ ì „ì²´ ì™„ë£Œ
- âœ… ì‚¬ìš©ì ê°œì… ë¶ˆí•„ìš”

---

## ê¶Œì¥ ì‚¬í•­

**ë‹¨ê¸° (ì§€ê¸ˆ ë‹¹ì¥)**:
- ë°©ì•ˆ 2 ì±„íƒ: evaluate_claude_auto.py + batch_evaluate.py
- ë‚˜ë¨¸ì§€ 9ê°œ ì¹´í…Œê³ ë¦¬ í‰ê°€

**ì¤‘ê¸° (ë‹¤ìŒ ì •ì¹˜ì¸)**:
- ë°©ì•ˆ 3 êµ¬í˜„: ì™„ì „ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
- 1ê°œ ëª…ë ¹ìœ¼ë¡œ ì „ì²´ í‰ê°€

**ì¥ê¸°**:
- evaluate_v30.pyì— í†µí•© (ë°©ì•ˆ 1)
- ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì™€ ì™„ì „ í†µí•©

---

## ë‹¤ìŒ ì‘ì—…

```bash
# 1. batch_evaluate.py ìƒì„±
# 2. ë‚˜ë¨¸ì§€ 9ê°œ ì¹´í…Œê³ ë¦¬ í‰ê°€
for cat in leadership vision integrity ethics accountability transparency communication responsiveness publicinterest
do
  python evaluate_claude_auto.py --category=$cat --output=eval_$cat.md
  python batch_evaluate.py eval_${cat}_data.json
  python evaluate_claude_auto.py --import_results=eval_${cat}_result.json
done

# 3. ìµœì¢… ì ìˆ˜ ê³„ì‚°
python calculate_v30_scores.py --politician_id=f9e00370 --politician_name="ê¹€ë¯¼ì„"
```

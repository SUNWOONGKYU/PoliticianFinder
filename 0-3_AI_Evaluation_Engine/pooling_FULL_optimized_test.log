
================================================================================
ğŸš€ ìµœì²¨ë‹¨ ì´ˆìµœì í™” Pooling í‰ê°€ ì‹œìŠ¤í…œ
================================================================================
í…ŒìŠ¤íŠ¸: ê¹€ë™ì—°
ëª©í‘œ: 98% í† í° ì ˆê°

ìµœì í™” ê¸°ë²•:
  1. ì¤‘ë³µ ì œê±°
  2. NER í•µì‹¬ ì¶”ì¶œ
  3. LLMLingua ì••ì¶• (20ë°°)
  4. ë¶ˆí•„ìš” ë¬¸êµ¬ ì œê±°
  5. System Message ìµœì†Œí™” (15í† í°)
  6. Prompt Caching
  7. BatchPrompt (1íšŒ API)
================================================================================
ğŸ”§ LLMLingua ì••ì¶•ê¸° ì´ˆê¸°í™” ì¤‘...
`torch_dtype` is deprecated! Use `dtype` instead!
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Traceback (most recent call last):
  File "C:\Development_PoliticianFinder_com\Developement_Real_PoliticianFinder\0-3_AI_Evaluation_Engine\pooling_FULL_optimized_test_1person.py", line 581, in <module>
    asyncio.run(main())
    ~~~~~~~~~~~^^^^^^^^
  File "C:\Python313\Lib\asyncio\runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "C:\Python313\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "C:\Python313\Lib\asyncio\base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "C:\Development_PoliticianFinder_com\Developement_Real_PoliticianFinder\0-3_AI_Evaluation_Engine\pooling_FULL_optimized_test_1person.py", line 562, in main
    evaluator = FullOptimizedEvaluator()
  File "C:\Development_PoliticianFinder_com\Developement_Real_PoliticianFinder\0-3_AI_Evaluation_Engine\pooling_FULL_optimized_test_1person.py", line 102, in __init__
    self.compressor = PromptCompressor(
                      ~~~~~~~~~~~~~~~~^
        model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        use_llmlingua2=True
        ^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\llmlingua\prompt_compressor.py", line 89, in __init__
    self.load_model(model_name, device_map, model_config)
    ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\llmlingua\prompt_compressor.py", line 140, in load_model
    model = MODEL_CLASS.from_pretrained(
        model_name,
    ...<6 lines>...
        **model_config,
    )
  File "C:\Python313\Lib\site-packages\transformers\models\auto\auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\transformers\modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "C:\Python313\Lib\site-packages\transformers\modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        model,
        ^^^^^^
    ...<12 lines>...
        weights_only=weights_only,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\transformers\modeling_utils.py", line 5432, in _load_pretrained_model
    caching_allocator_warmup(model, expanded_device_map, hf_quantizer)
    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\transformers\modeling_utils.py", line 6089, in caching_allocator_warmup
    index = device.index if device.index is not None else torch_accelerator_module.current_device()
                                                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Python313\Lib\site-packages\torch\cuda\__init__.py", line 1069, in current_device
    _lazy_init()
    ~~~~~~~~~~^^
  File "C:\Python313\Lib\site-packages\torch\cuda\__init__.py", line 403, in _lazy_init
    raise AssertionError("Torch not compiled with CUDA enabled")
AssertionError: Torch not compiled with CUDA enabled
